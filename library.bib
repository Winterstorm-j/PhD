@book{Orengo,
   author = {Christine Orengo and David Jones and Janet Thornton},
   title = {Bioinformatics: genes, proteins and computers},
}
@inbook{Frenkel2017,
   author = {D. Frenkel and B. Smit and B. Smit},
   issue = {2001},
   journal = {Understanding molecular simulation : from algorithms to applications.},
   pages = {84-104},
   title = {Chapter 4. Molecular Dynamics Simulations},
   year = {2017},
}
@article{Park2015,
   author = {Sanghyun Park},
   issue = {September},
   title = {Stretching Deca-alanine},
   year = {2015},
}
@article{Leimkuhler2015,
   author = {Ben Leimkuhler and Charles Matthews},
   doi = {10.1007/978-3-319-16375-8},
   isbn = {978-3-319-16374-1},
   issn = {1520-5207},
   issue = {2001},
   pmid = {20455590},
   title = {Molecular Dynamics},
   volume = {39},
   url = {http://link.springer.com/10.1007/978-3-319-16375-8},
   year = {2015},
}
@article{Schneider,
   author = {Ralf Schneider and Amit Raj Sharma and Abha Rai},
   doi = {10.1007/978-3-540-74686-7_1},
   journal = {Computational Many-Particle Physics},
   pages = {3-40},
   title = {Introduction to Molecular Dynamics},
   url = {http://link.springer.com/10.1007/978-3-540-74686-7_1},
}
@article{So2017,
   author = {S So and S Amos},
   pages = {0-2},
   title = {Page 1 of 4},
   year = {2017},
}
@article{Welch2018,
   author = {David Welch},
   issue = {August},
   pages = {1-2},
   title = {Bioinf 703 Networks lab},
   year = {2018},
}
@article{Merico2009,
   abstract = {Networks in biology can appear complex and difficult to decipher. We illustrate how to interpret biological networks with the help of frequently used visualization and analysis patterns.},
   author = {Daniele Merico and David Gfeller and Gary D. Bader},
   doi = {10.1038/nbt.1567},
   isbn = {1546-1696 (Electronic)\r1087-0156 (Linking)},
   issn = {10870156},
   issue = {10},
   journal = {Nature Biotechnology},
   pages = {921-924},
   pmid = {19816451},
   publisher = {Nature Publishing Group},
   title = {How to visually interpret biological data using networks},
   volume = {27},
   url = {http://dx.doi.org/10.1038/nbt.1567},
   year = {2009},
}
@article{Taipale2018,
   abstract = {Genomic analyses have revealed that free‐living biological organisms carry between 107 and 1011 bits of information in their genomes. In large organisms with relatively small population sizes, such as humans, only in the order of 1% of the genomic information is shaped by the environment via natural selection. A much larger amount of information than this is routinely being generated by biomedical researchers, and the rapidly accumulating data is often interpreted to mean that biological systems are extremely complex. However, as the genome is finite in length, it cannot define precisely optimal values for the quantitative parameters of the experimentally identified molecular phenotypes. Furthermore, because the genomic sequences orchestrate a biochemical system that is much more information‐rich than the genome, the vast majority of the measured molecular phenotypes must represent “molecular spandrels”, that is phenotypes that are not independent of each other, and instead co‐determined by the same genomic sequences. These considerations are important in interpreting the results of individual experiments. In addition, they indicate that full understanding of biological systems requires a genome‐centric model that does not abstract away the information contained in the genome, and instead explicitly maps all phenotypic data back to specific genomic sequences.},
   author = {Jussi Taipale},
   doi = {10.15252/embj.201696114},
   issn = {0261-4189},
   issue = {10},
   journal = {The EMBO Journal},
   pages = {e96114},
   pmid = {29669861},
   title = {Informational limits of biological organisms},
   volume = {37},
   url = {http://emboj.embopress.org/lookup/doi/10.15252/embj.201696114},
   year = {2018},
}
@article{Singer2004,
   abstract = {Exonization of Alu retroposons awakens public opinion, particularly when causing genetic diseases. However, often neglected, alternative "Alu-exons" also carry the potential to greatly enhance genetic diversity by increasing the transcriptome of primates chiefly via alternative splicing. Here, we report a 5′ exon generated from one of the two alternative transcripts in human tumor necrosis factor receptor gene type 2 (p75TNFR) that contains an ancient Alu-SINE, which provides an alternative N-terminal protein-coding domain. We follow the primate evolution over the past 63 million years to reconstruct the key events that gave rise to a novel receptor isoform. The Alu integration and start codon formation occurred between 58 and 40 million years ago (MYA) in the common ancestor of anthropoid primates. Yet a functional gene product could not be generated until a novel splice site and an open reading frame were introduced between 40 and 25 MYA on the catarrhine lineage (Old World monkeys including apes). © 2004 Elsevier Ltd.},
   author = {Silke S. Singer and Daniela N. Männel and Thomas Hehlgans and Jürgen Brosius and Jürgen Schmitz},
   doi = {10.1016/j.jmb.2004.06.070},
   isbn = {0022-2836 (Print)\n0022-2836 (Linking)},
   issn = {00222836},
   issue = {4},
   journal = {Journal of Molecular Biology},
   keywords = {Alu exonization,differentially spliced transcripts,gene evolution,primates,tumor necrosis factor receptor gene},
   pages = {883-886},
   pmid = {15328599},
   title = {From "junk" to gene: Curriculum vitae of a primate receptor isoform gene},
   volume = {341},
   year = {2004},
}
@article{Sep2013,
   author = {Babbage Sep and Large Hadron Collider},
   journal = {The Economist},
   pages = {5-9},
   title = {Tape rescues big data},
   url = {https://www.economist.com/blogs/babbage/2013/09/information-storage},
   year = {2013},
}
@article{Seitz2001,
   abstract = {We report the identification of a novel p75TNF receptor isoform termed icp75TNFR, which is generated by the use of an alternative transcriptional start site within the p75TNFR gene and characterized by regulated intracellular expression. The icp75TNFR protein has an apparent molecular mass of approximately 50 kDa and is recognized by antibodies generated against the transmembrane form of p75TNFR. The icp75TNFR binds the tumor necrosis factor(TNF) and mediates intracellular signaling. Overexpression of the icp75TNFR cDNA results in TNF-induced activation of NFkappaB in a TNF receptor-associated factor 2 (TRAF2)-dependent manner. Thus, our results provide an example for intracellular cytokine receptor activation.},
   author = {Carola Seitz and Peter Müller and René C. Krieg and Daniela N. Männel and Thomas Hehlgans},
   doi = {10.1074/jbc.M101336200},
   isbn = {0021-9258 (Print)\r0021-9258 (Linking)},
   issn = {00219258},
   issue = {22},
   journal = {Journal of Biological Chemistry},
   pages = {19390-19395},
   pmid = {11279196},
   title = {A Novel p75TNF Receptor Isoform Mediating NFκB Activation},
   volume = {276},
   year = {2001},
}
@article{Poole2018,
   author = {Ant Poole},
   pages = {1-2},
   title = {Evaluating a published paper (7%)},
   year = {2018},
}
@article{Zimmermann2008,
   abstract = {The extraction of genetic information from preserved tissue samples or museum specimens is a fundamental component of many fields of research, including the Barcode of Life initiative, forensic investigations, biological studies using scat sample analysis, and cancer research utilizing formaldehyde-fixed, paraffin-embedded tissue. Efforts to obtain genetic information from these sources are often hampered by an inability to amplify the desired DNA as a consequence of DNA damage.Previous studies have described techniques for improved DNA extraction from such samples or focused on the effect of damaging agents - such as light, oxygen or formaldehyde - on free nucleotides.We present ongoing work to characterize lesions in DNA samples extracted from preserved specimens. The extracted DNA is digested to single nucleosides with a combination of DNase I, Snake Venom Phosphodiesterase, and Antarctic Phosphatase and then analyzed by HPLC-ESI-TOF-MS.We present data for moth specimens that were preserved dried and pinned with no additional preservative and for frog tissue samples that were preserved in either ethanol, or formaldehyde, or fixed in formaldehyde and then preserved in ethanol. These preservation methods represent the most common methods of preserving animal specimens in museum collections. We observe changes in the nucleoside content of these samples over time, especially a loss of deoxyguanosine. We characterize the fragmentation state of the DNA and aim to identify abundant nucleoside lesions. Finally, simple models are introduced to describe the DNA fragmentation based on nicks and double-strand breaks.},
   author = {Juergen Zimmermann and Mehrdad Hajibabaei and David C. Blackburn and James Hanken and Elizabeth Cantin and Janos Posfai and Thomas C. Evans},
   doi = {10.1186/1742-9994-5-18},
   isbn = {1742-9994},
   issn = {17429994},
   journal = {Frontiers in Zoology},
   pages = {1-13},
   pmid = {18947416},
   title = {DNA damage in preserved specimens and tissue samples: A molecular assessment},
   volume = {5},
   year = {2008},
}
@inproceedings{Kazansky2016,
   abstract = {Securely storing large amounts of information over relatively short timescales of 100 years, comparable to the span of the human memory, is a challenging problem. Conventional optical data storage technology used in CDs and DVDs has reached capacities of hundreds of gigabits per square inch, but its lifetime is limited to a decade. DNA based data storage can hold hundreds of terabytes per gram, but the durability is limited. The major challenge is the lack of appropriate combination of storage technology and medium possessing the advantages of both high capacity and long lifetime. The recording and retrieval of the digital data with a nearly unlimited lifetime was implemented by femtosecond laser nanostructuring of fused quartz. The storage allows unprecedented properties including hundreds of terabytes per disc data capacity, thermal stability up to 1000 °C, and virtually unlimited lifetime at room temperature opening a new era of eternal data archiving.},
   author = {Peter Kazansky and Ausra Cerkauskaite and Martynas Beresna and Rokas Drevinskas and Aabid Patel and Jingyu Zhang and Mindaugas Gecevicius},
   city = {San Francisco, California},
   doi = {10.1117/2.1201603.006365},
   isbn = {9781628419719},
   issn = {18182259},
   issue = {March 2016},
   journal = {The International Society of Optics and Photonics - LASE},
   keywords = {form birefringence,material processing,optical data multiplexing,ultrafast phenomena},
   title = {Eternal 5D data storage via ultrafast-laser writing in glass},
   url = {http://www.spie.org/x117492.xml},
   year = {2016},
}
@article{TabatabaeiYazdi2015,
   abstract = {We describe the first DNA-based storage architecture that enables random access to data blocks and rewriting of information stored at arbitrary locations within the blocks. The newly developed architecture overcomes drawbacks of existing read-only methods that require decoding the whole file in order to read one data fragment. Our system is based on new constrained coding techniques and accompanying DNA editing methods that ensure data reliability, specificity and sensitivity of access, and at the same time provide exceptionally high data storage capacity. As a proof of concept, we encoded parts of the Wikipedia pages of six universities in the USA, and selected and edited parts of the text written in DNA corresponding to three of these schools. The results suggest that DNA is a versatile media suitable for both ultrahigh density archival and rewritable storage applications.},
   author = {S. M.Hossein Tabatabaei Yazdi and Yongbo Yuan and Jian Ma and Huimin Zhao and Olgica Milenkovic},
   doi = {10.1038/srep14138},
   isbn = {2045-2322 (Electronic)\r2045-2322 (Linking)},
   issn = {20452322},
   journal = {Nature Scientific Reports},
   pages = {1-10},
   pmid = {26382652},
   publisher = {Nature Publishing Group},
   title = {A Rewritable, Random-Access DNA-Based Storage System},
   volume = {5},
   url = {http://dx.doi.org/10.1038/srep14138},
   year = {2015},
}
@article{Lynch2006,
   abstract = {The genomes of unicellular species, particularly prokaryotes, are greatly reduced in size and simplified in terms of gene structure relative to those of multicellular eukaryotes. Arguments proposed to explain this disparity include selection for metabolic efficiency and elevated rates of deletion in microbes, but the evidence in support of these hypotheses is at best equivocal. An alternative explanation based on fundamental population-genetic principles is proposed here. By increasing the mutational target sizes of associated genes, most forms of nonfunctional DNA are opposed by weak selection. Free-living microbial species have elevated effective population sizes, and the consequent reduction in the power of random genetic drift appears to be sufficient to enable natural selection to inhibit the accumulation of excess DNA. This hypothesis provides a potentially unifying explanation for the continuity in genomic scaling from prokaryotes to multicellular eukaryotes, the divergent patterns of mitochondrial evolution in animals and land plants, and various aspects of genomic modification in microbial endosymbionts},
   author = {Michael Lynch},
   doi = {10.1146/annurev.micro.60.080805.142300},
   isbn = {0066-4227 (Print)},
   issn = {0066-4227},
   issue = {1},
   journal = {Annual Review of Microbiology},
   keywords = {genome evolution,genomic streamlining,mutation,prokaryotes,random genetic drift,recombination},
   pages = {327-349},
   pmid = {16824010},
   title = {Streamlining and Simplification of Microbial Genome Architecture},
   volume = {60},
   url = {http://www.annualreviews.org/doi/10.1146/annurev.micro.60.080805.142300},
   year = {2006},
}
@article{Corley2016,
   abstract = {Many genes carry information for making proteins. To make a protein, a working copy of the information stored in DNA is first copied into a molecule of messenger RNA. These RNA messages are then interpreted by the ribosome, the molecular machine that makes proteins. Many messages are produced from each gene, and each message can be read multiple times. Thus, it should follow that the number of messages produced dictates the number of proteins made. However, this is not the case and the number of proteins produced cannot be completely predicted from knowing the number of messenger RNAs. Cells control how much of a given protein they produce through interactions between the messenger RNAs and other regulatory RNAs. The regulatory RNAs bind directly to a message and impede protein production. Because there are millions of RNAs in a cell, these interactions have evolved to be highly specific. Nevertheless, it seems inevitable that messenger RNAs would encounter other RNAs too, which could short-circuit gene regulation and lead to less protein being produced. Umu et al. have now asked if such short-circuit events are selected against during evolution. Computational tools were used to predict the strength of binding between the RNAs found in the dominant forms of microbial life on Earth: the bacteria and the archaea. This approach revealed that the majority of messenger RNAs bind more weakly to the most common RNA molecules found in cells than would be expected by chance. Weakened binding should prevent the RNA molecules from becoming tangled with each other and ensure that protein levels are not perturbed by unintended interactions between highly expressed messages and other RNAs. To test this hypothesis further, Umu et al. generated versions of the gene for a green fluorescent protein that differed only in how well their messenger RNAs could avoid interacting with the most abundant RNAs in E. coli cells. Those messengers that were designed to avoid interacting with other RNAs yielded far more protein than those that were not. The findings show that taking this kind of avoidance into account can improve predictions about how much protein will be produced and should therefore make it easier to control protein production in experimental systems. Finally, the messenger RNAs of some bacteria do not show such clear avoidance. However, these bacteria have a more complex internal cell structure. This finding hints at an alternative means for avoiding short-circuiting events that could be used by more complicated cells, such of those of animals and plants, which also contain much larger numbers of RNAs.},
   author = {M Corley and A Laederach},
   doi = {10.7554/eLife.13479},
   isbn = {1044071060},
   issn = {2050084X},
   issue = {September},
   journal = {eLife},
   pages = {3-5},
   pmid = {27642845},
   title = {Selecting against accidental RNA interactions},
   volume = {5},
   year = {2016},
}
@article{Gillings2016,
   abstract = {Evolution has transformed life through key innovations in information storage and replication, including RNA, DNA, multicellularity, and culture and language. We argue that the carbon-based biosphere has generated a cognitive system (humans) capable of creating technology that will result in a comparable evolutionary transition. Digital information has reached a similar magnitude to information in the biosphere. It increases exponentially, exhibits high-fidelity replication, evolves through differential fitness, is expressed through artificial intelligence (AI), and has facility for virtually limitless recombination. Like previous evolutionary transitions, the potential symbiosis between biological and digital information will reach a critical point where these codes could compete via natural selection. Alternatively, this fusion could create a higher-level superorganism employing a low-conflict division of labor in performing informational tasks. Digital information is accumulating at an exponential rate and could exceed the quantity of DNA-based information. There are biological and social implications arising from our growing fusion with the digital world.The parallels between evolution in the biological and digital worlds need to be explored.},
   author = {Michael R. Gillings and Martin Hilbert and Darrell J. Kemp},
   doi = {10.1016/j.tree.2015.12.013},
   isbn = {1872-8383 (Electronic)\r0169-5347 (Linking)},
   issn = {01695347},
   issue = {3},
   journal = {Trends in Ecology and Evolution},
   keywords = {Artificial intelligence,Big data,Digital,Evolutionary transition,Information,Moore's law,Replicator,Singularity,Synthetic biology},
   pages = {180-189},
   pmid = {26777788},
   publisher = {Elsevier Ltd},
   title = {Information in the Biosphere: Biological and Digital Worlds},
   volume = {31},
   url = {http://dx.doi.org/10.1016/j.tree.2015.12.013},
   year = {2016},
}
@article{Lehman2018,
   abstract = {Biological evolution provides a creative fount of complex and subtle adaptations, often surprising the scientists who discover them. However, because evolution is an algorithmic process that transcends the substrate in which it occurs, evolution's creativity is not limited to nature. Indeed, many researchers in the field of digital evolution have observed their evolving algorithms and organisms subverting their intentions, exposing unrecognized bugs in their code, producing unexpected adaptations, or exhibiting outcomes uncannily convergent with ones in nature. Such stories routinely reveal creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.},
   author = {Joel Lehman and Jeff Clune and Dusan Misevic and Christoph Adami and Lee Altenberg and Julie Beaulieu and Peter J. Bentley and Samuel Bernard and Guillaume Beslon and David M. Bryson and Patryk Chrabaszcz and Nick Cheney and Antoine Cully and Stephane Doncieux and Fred C. Dyer and Kai Olav Ellefsen and Robert Feldt and Stephan Fischer and Stephanie Forrest and Antoine Frénoy and Christian Gagné and Leni Le Goff and Laura M. Grabowski and Babak Hodjat and Frank Hutter and Laurent Keller and Carole Knibbe and Peter Krcah and Richard E. Lenski and Hod Lipson and Robert MacCurdy and Carlos Maestre and Risto Miikkulainen and Sara Mitri and David E. Moriarty and Jean-Baptiste Mouret and Anh Nguyen and Charles Ofria and Marc Parizeau and David Parsons and Robert T. Pennock and William F. Punch and Thomas S. Ray and Marc Schoenauer and Eric Shulte and Karl Sims and Kenneth O. Stanley and François Taddei and Danesh Tarapore and Simon Thibault and Westley Weimer and Richard Watson and Jason Yosinski},
   pages = {1-31},
   title = {The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities},
   url = {http://arxiv.org/abs/1803.03453},
   year = {2018},
}
@article{Shipman2017,
   abstract = {DNA is an excellent medium for data archival. Recent efforts have illustrated the potential for information storage in DNA using synthesized oligonucleotides assembled in vitro 1-6. A relatively unexplored avenue of information storage in DNA is the ability to write information into the genome of a living cell by the addition of nucleotides over time. Using the Cas1-Cas2 integrase, the CRISPR-Cas microbial immune system stores the nucleotide content of invading viruses to confer adaptive immunity 7. Harnessed, this system has the potential to write arbitrary information into the genome 8. Here, we use the CRISPR-Cas system to encode images and a short movie into the genomes of a population of living bacteria. In doing so, we push the technical limits of this information storage system and optimize strategies to minimize those limitations. We additionally uncover underlying principles of the CRISPR-Cas adaptation system, including sequence determinants of spacer acquisition relevant for understanding both the basic biology of bacterial adaptation as well as its technological applications. This work demonstrates that this system can capture and stably store practical amounts of real data within the genomes of populations of living cells. By combining the principles of information storage in DNA with DNA capture systems capable of functioning in living cells, one can create living organisms that capture, store, and},
   author = {Seth L. Shipman and Jeff Nivala and Jeffrey D. Macklis and George M. Church},
   doi = {10.1038/nature23017},
   isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
   issn = {14764687},
   issue = {7663},
   journal = {Nature},
   pages = {345-349},
   pmid = {28700573},
   publisher = {Nature Publishing Group},
   title = {CRISPR-Cas encoding of a digital movie into the genomes of a population of living bacteria},
   volume = {547},
   url = {http://dx.doi.org/10.1038/nature23017},
   year = {2017},
}
@article{Shapiro2013,
   abstract = {The genome has traditionally been treated as a Read-Only Memory (ROM) subject to change by copying errors and accidents. In this review, I propose that we need to change that perspective and understand the genome as an intricately formatted Read-Write (RW) data storage system constantly subject to cellular modifications and inscriptions. Cells operate under changing conditions and are continually modifying themselves by genome inscriptions. These inscriptions occur over three distinct time-scales (cell reproduction, multicellular development and evolutionary change) and involve a variety of different processes at each time scale (forming nucleoprotein complexes, epigenetic formatting and changes in DNA sequence structure). Research dating back to the 1930s has shown that genetic change is the result of cell-mediated processes, not simply accidents or damage to the DNA. This cell-active view of genome change applies to all scales of DNA sequence variation, from point mutations to large-scale genome rearrangements and whole genome duplications (WGDs). This conceptual change to active cell inscriptions controlling RW genome functions has profound implications for all areas of the life sciences. © 2013 Elsevier B.V.},
   author = {James A. Shapiro},
   doi = {10.1016/j.plrev.2013.07.001},
   isbn = {1571-0645},
   issn = {15710645},
   issue = {3},
   journal = {Physics of Life Reviews},
   keywords = {Epigenetics,Genome inscriptions,Mobile genetic elements (MGEs),Natural genetic engineering (NGE)},
   pages = {287-323},
   pmid = {23876611},
   publisher = {Elsevier B.V.},
   title = {How life changes itself: The Read-Write (RW) genome},
   volume = {10},
   url = {http://dx.doi.org/10.1016/j.plrev.2013.07.001},
   year = {2013},
}
@article{Lee2005,
   author = {Hoong-chien Lee},
   isbn = {0769521940},
   issue = {3},
   keywords = {genomics,molecular evolution,shannon information,statistical analysis},
   pages = {587-608},
   title = {Shannon information in complete genomes},
   volume = {3},
   year = {2005},
}
@article{Rubio2013,
   abstract = {OBJECTIVE: The objective was to use a dual quantitative and qualitative approach to analyze the dental DNA degradation produced by the passage of time since tooth death under controlled environmental conditions.\n\nMATERIALS AND METHODS: Sixty human teeth were stored at room temperature for 0, 1, 3, 6, 12 or 18 months post-extraction. DNA quantification was determined by real-time quantitative PCR using a Quantifiler(TM) kit. DNA quality was assessed by the allelic dropout ratio between the smallest and largest loci obtained after STR genotyping and using an AmpFlSTR® Identifiler™ PCR kit. We also evaluated differences of DNA concentration related to gender and tooth position.\n\nRESULTS: DNA concentration significantly reduced in 1 month post-extraction, stabilized between 1-12 months post-extraction, but decreased again at 18 months post-extraction. Interestingly, a significant reduction of the allelic dropout ratio (DNA quality) was only detected at 18 months post-extraction.\n\nCONCLUSIONS: Stability of dental DNA decreased over time, differently affecting the amount and quality of the DNA in a time-dependent process over the first 18 months post-extraction. These results have a potential use in post-mortem intervals in human teeth in controlled environmental conditions.},
   author = {Leticia Rubio and Ignacio Santos and Maria Jesus Gaitan and Stella Martin De- Las Heras},
   doi = {10.3109/00016357.2012.700068},
   isbn = {10.3109/00016357.2012.700068},
   issn = {00016357},
   issue = {3-4},
   journal = {Acta Odontologica Scandinavica},
   keywords = {DNA degradation,Forensic dentistry,Post-extraction interval,Time since death,Tooth},
   pages = {638-643},
   pmid = {22783923},
   title = {Time-dependent changes in DNA stability in decomposing teeth over 18 months},
   volume = {71},
   year = {2013},
}
@article{Yachie2007,
   abstract = {The practical realization of DNA data storage is a major scientific goal. Here we introduce a simple, flexible, and robust data storage and retrieval method based on sequence alignment of the genomic DNA of living organisms. Duplicated data encoded by different oligonucleotide sequences was inserted redundantly into multiple loci of the Bacillus subtilis genome. Multiple alignment of the bit data sequences decoded by B. subtilis genome sequences enabled the retrieval of stable and compact data without the need for template DNA, parity checks, or error-correcting algorithms. Combined with the computational simulation of data retrieval from mutated message DNA, a practical use of this alignment-based method is discussed.},
   author = {Nozomu Yachie and Kazuhide Sekiyama and Junichi Sugahara and Yoshiaki Ohashi and Masaru Tomita},
   doi = {10.1021/bp060261y},
   isbn = {8756-7938 (Print)\n1520-6033 (Linking)},
   issn = {87567938},
   issue = {2},
   journal = {Biotechnology Progress},
   pages = {501-505},
   pmid = {17253725},
   title = {Alignment-based approach for durable data storage into living organisms},
   volume = {23},
   year = {2007},
}
@article{Bishop2017,
   author = {Bryan Bishop and Nathan Mccorkle and Victor Zhirnov},
   pages = {1-39},
   title = {Technology Working Group Meeting on future DNA synthesis technologies},
   url = {https://www.src.org/program/grc/semisynbio/semisynbio-consortium-roadmap/6043-full-report-dna-based-storage-final-twg1-4-18.pdf},
   year = {2017},
}
@article{Tabor2003,
   abstract = {An automaton built with DNA enzymes plays tic-tac-toe against human players.},
   author = {Jeffrey J. Tabor and Andrew D. Ellington},
   doi = {10.1038/nbt0903-1013},
   isbn = {1087-0156},
   issn = {10870156},
   issue = {9},
   journal = {Nature Biotechnology},
   pages = {1013-1015},
   pmid = {12949563},
   title = {Playing to win at DNA computation},
   volume = {21},
   year = {2003},
}
@inproceedings{Markowitz2016,
   author = {David Markowitz},
   city = {Arlington, VA},
   journal = {SRC/IARPA Workshop on DNA-based Massive Information Storage},
   publisher = {The Intelligence Advanced Research Projects Activity and Semiconductor Research Corporation},
   title = {Workshop Results},
   year = {2016},
}
@article{Organick2017,
   abstract = {Current storage technologies can no longer keep pace with exponentially growing amounts of data. Synthetic DNA offers an attractive alternative due to its potential information density of ~ 1018B/mm3, 107 times denser than magnetic tape, and potential durability of thousands of years. Recent advances in DNA data storage have highlighted technical challenges, in particular, coding and random access, but have stored only modest amounts of data in synthetic DNA. This paper demonstrates an end-to-end approach toward the viability of DNA data storage with large-scale random access. We encoded and stored 35 distinct files, totaling 200MB of data, in more than 13 million DNA oligonucleotides (about 2 billion nucleotides in total) and fully recovered the data with no bit errors, representing an advance of almost an order of magnitude compared to prior work. Our data curation focused on technologically advanced data types and historical relevance, including the Universal Declaration of Human Rights in over 100 languages, a high-definition music video of the band OK Go, and a CropTrust database of the seeds stored in the Svalbard Global Seed Vault. We developed a random access methodology based on selective amplification, for which we designed and validated a large library of primers, and successfully retrieved arbitrarily chosen items from a subset of our pool containing 10.3 million DNA sequences. Moreover, we developed a novel coding scheme that dramatically reduces the physical redundancy (sequencing read coverage) required for error-free decoding to a median of 5x, while maintaining levels of logical redundancy comparable to the best prior codes. We further stress-tested our coding approach by successfully decoding a file using the more error-prone nanopore-based sequencing. We provide a detailed analysis of errors in the process of writing, storing, and reading data from synthetic DNA at a large scale, which helps characterize DNA as a storage medium and justify our coding approach. Thus, we have demonstrated a significant improvement in data volume, random access, and encoding/decoding schemes that contribute to a whole-system vision for DNA data storage.},
   author = {Lee Organick and Siena Dumas Ang and Yuan-Jyue Chen and Randolph Lopez and Sergey Yekhanin and Konstantin Makarychev and Miklos Z. Racz and Govinda Kamath and Parikshit Gopalan and Bichlien Nguyen and Christopher Takahashi and Sharon Newman and Hsing-Yeh Parker and Cyrus Rashtchian and Kendall Stewart and Gagan Gupta and Robert Carlson and John Mulligan and Douglas Carmean and Georg Seelig and Luis Ceze and Karin Strauss},
   doi = {10.1101/114553},
   isbn = {9781510856738},
   journal = {bioRxiv},
   pages = {114553},
   title = {Scaling up DNA data storage and random access retrieval},
   url = {https://www.biorxiv.org/content/early/2017/03/07/114553},
   year = {2017},
}
@article{Olalde2014,
   abstract = {Ancient genomic sequences have started to reveal the origin and the demographic impact of farmers from the Neolithic period spreading into Europe. The adoption of farming, stock breeding and sedentary societies during the Neolithic may have resulted in adaptive changes in genes associated with immunity and diet. However, the limited data available from earlier hunter-gatherers preclude an understanding of the selective processes associated with this crucial transition to agriculture in recent human evolution. Here we sequence an approximately 7,000-year-old Mesolithic skeleton discovered at the La Braña-Arintero site in León, Spain, to retrieve a complete pre-agricultural European human genome. Analysis of this genome in the context of other ancient samples suggests the existence of a common ancient genomic signature across western and central Eurasia from the Upper Paleolithic to the Mesolithic. The La Braña individual carries ancestral alleles in several skin pigmentation genes, suggesting that the light skin of modern Europeans was not yet ubiquitous in Mesolithic times. Moreover, we provide evidence that a significant number of derived, putatively adaptive variants associated with pathogen resistance in modern Europeans were already present in this hunter-gatherer.},
   author = {Iñigo Olalde and Morten E. Allentoft and Federico Sánchez-Quinto and Gabriel Santpere and Charleston W K Chiang and Michael DeGiorgio and Javier Prado-Martinez and Juan Antonio Rodríguez and Simon Rasmussen and Javier Quilez and Oscar Ramírez and Urko M. Marigorta and Marcos Fernández-Callejo and María Encina Prada and Julio Manuel Vidal Encinas and Rasmus Nielsen and Mihai G. Netea and John Novembre and Richard A. Sturm and Pardis Sabeti and Tomàs Marquès-Bonet and Arcadi Navarro and Eske Willerslev and Carles Lalueza-Fox},
   doi = {10.1038/nature12960},
   isbn = {1476-4687 (Electronic)\r0028-0836 (Linking)},
   issn = {14764687},
   issue = {7491},
   journal = {Nature},
   pages = {225-228},
   pmid = {24463515},
   title = {Derived immune and ancestral pigmentation alleles in a 7,000-year-old Mesolithic European},
   volume = {507},
   year = {2014},
}
@article{Porter2013,
   abstract = {The article discusses education business models, focusing on massive open online courses (MOOCs) as of August 2013 and arguing that MOOCs can become profitable. Potential sources of revenue such as state education subsidies, student tuition, employment recruiting services, syndication, and sponsors are discussed. Companies such as the recruitment company Udacity are mentioned.},
   author = {Leo Porter and Mark Guzdial and Charlie McDowell and Beth Simon},
   doi = {10.1145/2492007},
   isbn = {0001-0782},
   issn = {00010782},
   issue = {8},
   journal = {Communications of the ACM},
   keywords = {BUSINESS models,BUSINESS revenue,EMPLOYEE recruitment,MASSIVE open online courses,TUITION},
   pages = {25-28},
   pmid = {89594063},
   title = {Success in Introductory Programming: What Works?},
   volume = {56},
   url = {10.1145/2492007.2492017%5Cnhttps://proxy.tamuc.edu:2048/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=89594060&site=eds-live},
   year = {2013},
}
@article{Minsky2004,
   abstract = {Nucleic acids are characterized by a vast structural variability. Secondary structural conformations include the main polymorphs A, B, and Z, cruciforms, intrinsic curvature, and multistranded motifs. DNA secondary motifs are stabilized and regulated by the primary base sequence, contextual effects, environmental factors, as well as by high-order DNA packaging modes. The high-order modes are, in turn, affected by secondary structures and by the environment. This review is concerned with the flow of structural information among the hierarchical structural levels of DNA molecules, the intricate interplay between the various factors that affect these levels, and the regulation and physiological significance of DNA high-order structures.},
   author = {Abraham Minsky},
   doi = {10.1146/annurev.biophys.33.110502.133328},
   isbn = {%(},
   issn = {1056-8700},
   issue = {1},
   journal = {Annual Review of Biophysics and Biomolecular Structure},
   keywords = {and z,are characterized by a,b,cruci-,dna condensation,dna microheterogeneity,dna repair,electrostatic collapse,ion correlation,restricted diffusion,s abstract nucleic acids,secondary structural conformations include,the main polymorphs a,vast structural variability},
   pages = {317-342},
   pmid = {15139816},
   title = {Information Content and Complexity in the High-Order Organization of DNA},
   volume = {33},
   url = {http://www.annualreviews.org/doi/10.1146/annurev.biophys.33.110502.133328},
   year = {2004},
}
@inproceedings{Rashtchian2017,
   abstract = {Storing data in synthetic DNA offers the possibility of improving information density and durability by several orders of magnitude compared to current storage technologies. However, DNA data storage requires a computationally intensive process to retrieve the data. In particular, a crucial step in the data retrieval pipeline involves clustering billions of strings with respect to edit distance. Datasets in this domain have many notable properties, such as containing a very large number of small clusters that are well-separated in the edit distance metric space. In this regime, existing algorithms are unsuitable because of either their long running time or low accuracy. To address this issue, we present a novel distributed algorithm for approximately computing the underlying clusters. Our algorithm converges efficiently on any dataset that satisfies certain separability properties, such as those coming from DNA data storage systems. We also prove that, under these assumptions, our algorithm is robust to outliers and high levels of noise. We provide empirical justification of the accuracy, scalability, and convergence of our algorithm on real and synthetic data. Compared to the state-of-the-art algorithm for clustering DNA sequences, our algorithm simultaneously achieves higher accuracy and a 1000x speedup on three real datasets.},
   author = {Cyrus Rashtchian and Konstantin Makarychev and Miklos Z. Racz and Djordje Jevdjic and Sergey Yekhanin and Siena Dumas Ang and Karin Strauss and Luis Ceze},
   issn = {10495258},
   issue = {Nips},
   journal = {Conf. Neural Information Processing Systems},
   pages = {1-12},
   title = {Clustering Billions of Reads for DNA Data Storage},
   year = {2017},
}
@article{Panda2018,
   author = {Darshan Panda and Kutubuddin Ali Molla and Mirza Jainul Baig and Alaka Swain and Deeptirekha Behera and Manaswini Dash},
   doi = {10.1007/s13205-018-1246-7},
   isbn = {0123456789},
   issn = {21905738},
   issue = {5},
   journal = {3 Biotech},
   keywords = {DNA hard drive,DNA steganography,DNA storage,Data crunch,Data longevity,Digital data,Silicone pollution},
   pages = {1-9},
   publisher = {Springer Berlin Heidelberg},
   title = {DNA as a digital information storage device: hope or hype?},
   volume = {8},
   url = {https://doi.org/10.1007/s13205-018-1246-7},
   year = {2018},
}
@article{Orlando2013,
   abstract = {The rich fossil record of equids has made them a model for evolutionary processes. Here we present a 1.12-times coverage draft genome from a horse bone recovered from permafrost dated to approximately 560-780 thousand years before present (kyr BP). Our data represent the oldest full genome sequence determined so far by almost an order of magnitude. For comparison, we sequenced the genome of a Late Pleistocene horse (43 kyr BP), and modern genomes of five domestic horse breeds (Equus ferus caballus), a Przewalski's horse (E. f. przewalskii) and a donkey (E. asinus). Our analyses suggest that the Equus lineage giving rise to all contemporary horses, zebras and donkeys originated 4.0-4.5 million years before present (Myr BP), twice the conventionally accepted time to the most recent common ancestor of the genus Equus. We also find that horse population size fluctuated multiple times over the past 2 Myr, particularly during periods of severe climatic changes. We estimate that the Przewalski's and domestic horse populations diverged 38-72 kyr BP, and find no evidence of recent admixture between the domestic horse breeds and the Przewalski's horse investigated. This supports the contention that Przewalski's horses represent the last surviving wild horse population. We find similar levels of genetic variation among Przewalski's and domestic populations, indicating that the former are genetically viable and worthy of conservation efforts. We also find evidence for continuous selection on the immune system and olfaction throughout horse evolution. Finally, we identify 29 genomic regions among horse breeds that deviate from neutrality and show low levels of genetic variation compared to the Przewalski's horse. Such regions could correspond to loci selected early during domestication.},
   author = {Ludovic Orlando and Aurélien Ginolhac and Guojie Zhang and Duane Froese and Anders Albrechtsen and Mathias Stiller and Mikkel Schubert and Enrico Cappellini and Bent Petersen and Ida Moltke and Philip L.F. Johnson and Matteo Fumagalli and Julia T. Vilstrup and Maanasa Raghavan and Thorfinn S. Korneliussen and Anna Sapfo Malaspinas and Josef Vogt and Damian Szklarczyk and Christian D. Kelstrup and Jakob Vinther and Andrei Dolocan and Jesper Stenderup and Amhed M.V. Velazquez and James Cahill and Morten Rasmussen and Xiaoli Wang and Jiumeng Min and Grant D. Zazula and Andaine Seguin-Orlando and Cecilie Mortensen and Kim Magnussen and John F. Thompson and Jacobo Weinstock and Kristian Gregersen and Knut H. Røed and Véra Eisenmann and Carl J. Rubin and Donald C. Miller and Douglas F. Antczak and Mads F. Bertelsen and Søren Brunak and Khaled A.S. Al-Rasheid and Oliver Ryder and Leif Andersson and John Mundy and Anders Krogh and M. Thomas P. Gilbert and Kurt Kjær and Thomas Sicheritz-Ponten and Lars Juhl Jensen and Jesper V. Olsen and Michael Hofreiter and Rasmus Nielsen and Beth Shapiro and Jun Wang and Eske Willerslev},
   doi = {10.1038/nature12323},
   isbn = {1476-4687 (Electronic)\n0028-0836 (Linking)},
   issn = {00280836},
   issue = {7456},
   journal = {Nature},
   pages = {74-78},
   pmid = {23803765},
   title = {Recalibrating equus evolution using the genome sequence of an early Middle Pleistocene horse},
   volume = {499},
   year = {2013},
}
@article{Organick2018,
   abstract = {200 MB of digital data is stored in DNA, randomly accessed and recovered using an error-free approach.},
   author = {Lee Organick and Siena Dumas Ang and Yuan-Jyue Chen and Randolph Lopez and Sergey Yekhanin and Konstantin Makarychev and Miklos Z Racz and Govinda Kamath and Parikshit Gopalan and Bichlien Nguyen and Christopher N Takahashi and Sharon Newman and Hsing-Yeh Parker and Cyrus Rashtchian and Kendall Stewart and Gagan Gupta and Robert Carlson and John Mulligan and Douglas Carmean and Georg Seelig and Luis Ceze and Karin Strauss},
   doi = {10.1038/nbt.4079},
   isbn = {1364298980840},
   issn = {1087-0156},
   issue = {3},
   journal = {Nature Biotechnology},
   pmid = {29457795},
   title = {Random access in large-scale DNA data storage},
   volume = {36},
   url = {http://www.nature.com/doifinder/10.1038/nbt.4079},
   year = {2018},
}
@article{Kalff2016,
   abstract = {The advent of devices based on single dopants, such as the single atom transistor, the single spin magnetometer and the single atom memory, motivates the quest for strategies that permit to control matter with atomic precision. Manipulation of individual atoms by means of low-temperature scanning tunnelling microscopy provides ways to store data in atoms, encoded either into their charge state, magnetization state or lattice position. A defining challenge at this stage is the controlled integration of these individual functional atoms into extended, scalable atomic circuits. Here we present a robust digital atomic scale memory of up to 1 kilobyte (8,000 bits) using an array of individual surface vacancies in a chlorine terminated Cu(100) surface. The memory can be read and rewritten automatically by means of atomic scale markers, and offers an areal density of 502 Terabits per square inch, outperforming state-of-the-art hard disk drives by three orders of magnitude. Furthermore, the chlorine vacancies are found to be stable at temperatures up to 77 K, offering prospects for expanding large-scale atomic assembly towards ambient conditions.},
   author = {F. E. Kalff and M. P. Rebergen and E. Fahrenfort and J. Girovsky and R. Toskovic and J. L. Lado and J. Fernández-Rossier and A. F. Otte},
   doi = {10.1038/nnano.2016.131},
   isbn = {1748-3395},
   issn = {17483395},
   issue = {11},
   journal = {Nature Nanotechnology},
   pages = {926-929},
   pmid = {27428273},
   publisher = {Nature Publishing Group},
   title = {A kilobyte rewritable atomic memory},
   volume = {11},
   url = {http://dx.doi.org/10.1038/nnano.2016.131},
   year = {2016},
}
@article{Kaiser2008,
   abstract = {In the past years, many publications about identification and sex-determination of dry human bones by means of DNA analysis have been published. However, few studies exist that investigate the potential use of DNA technique to determine the postmortem interval (PMI). In the present study we analyzed the rate of increasingly smaller fragments of chromosomal DNA and PMI. We examined DNA degradation in human bones with postmortem intervals ranging between 1 and more than 200 years that had been kept under comparable conditions concerning weather and soil. Following bone separation into the three different zones of interest of inner/middle/outer segments the quantity of total DNA was determined in each region. Subsequently, the degree of DNA fragmentation was estimated by searching for PCR products of defined size (150, 507 and 763 bp) with primers of the human-specific multicopy β-actin-gene. Concerning DNA quantity we detected a significant correlation between the zone of interest and the amount of DNA. However, there was no correlation between the amount of DNA and PMI. In contrast to this, analyzing DNA using PCR showed a significant inverse correlation between fragment length and PMI. Thus, postmortem DNA degradation into increasingly smaller fragments reveals a time-dependent process. It has the potential to be used as a predictor of PMI in human bone findings, provided that environmental conditions are known. © 2007 Elsevier Ireland Ltd. All rights reserved.},
   author = {Christina Kaiser and Beatrice Bachmeier and Claudius Conrad and Andreas Nerlich and Hansjürgen Bratzke and Wolfgang Eisenmenger and Oliver Peschel},
   doi = {10.1016/j.forsciint.2007.10.005},
   isbn = {1872-6283 (Electronic)\n0379-0738 (Linking)},
   issn = {03790738},
   issue = {1},
   journal = {Forensic Science International},
   keywords = {Bone,DNA degradation,Forensic,Postmortem interval (PMI),Time since death},
   pages = {32-36},
   pmid = {18063334},
   title = {Molecular study of time dependent changes in DNA stability in soil buried skeletal residues},
   volume = {177},
   year = {2008},
}
@article{Jerome2002,
   author = {Keith R Jerome and Meei-li Huang and Anna Wald and Stacy Selke and Lawrence Corey},
   doi = {10.1128/JCM.40.7.2609},
   issn = {0095-1137},
   issue = {7},
   journal = {Journal of clinical microbiology},
   pages = {2609-2611},
   title = {Quantitative Stability of DNA after Extended Storage of Clinical Specimens as Determined by Real-Time PCR Quantitative Stability of DNA after Extended Storage of Clinical Specimens as Determined by Real-Time PCR},
   volume = {40},
   year = {2002},
}
@article{Houlihan2017,
   abstract = {Nucleic acids are a distinct form of sequence-defined biopolymer. What sets them apart from other biopolymers such as polypeptides or polysaccharides is their unique capacity to encode, store, and propagate genetic information (molecular heredity). In nature, just two closely related nucleic acids, DNA and RNA, function as repositories and carriers of genetic information. They therefore are the molecular embodiment of biological information. This naturally leads to questions regarding the degree of variation from this seemingly ideal "Goldilocks" chemistry that would still be compatible with the fundamental property of molecular heredity. To address this question, chemists have created a panoply of synthetic nucleic acids comprising unnatural sugar ring congeners, backbone linkages, and nucleobases in order to establish the molecular parameters for encoding genetic information and its emergence at the origin of life. A deeper analysis of the potential of these synthetic genetic polymers for molecular heredity requires a means of replication and a determination of the fidelity of information transfer. While non-enzymatic synthesis is an increasingly powerful method, it currently remains restricted to short polymers. Here we discuss efforts toward establishing enzymatic synthesis, replication, and evolution of synthetic genetic polymers through the engineering of polymerase enzymes found in nature. To endow natural polymerases with the ability to efficiently utilize non-cognate nucleotide substrates, novel strategies for the screening and directed evolution of polymerase function have been realized. High throughput plate-based screens, phage display, and water-in-oil emulsion technology based methods have yielded a number of engineered polymerases, some of which can synthesize and reverse transcribe synthetic genetic polymers with good efficiency and fidelity. The inception of such polymerases demonstrates that, at a basic level at least, molecular heredity is not restricted to the natural nucleic acids DNA and RNA, but may be found in a large (if finite) number of synthetic genetic polymers. And it has opened up these novel sequence spaces for investigation. Although largely unexplored, first tentative forays have yielded ligands (aptamers) against a range of targets and several catalysts elaborated in a range of different chemistries. Finally, taking the lead from established DNA designs, simple polyhedron nanostructures have been described. We anticipate that further progress in this area will expand the range of synthetic genetic polymers that can be synthesized, replicated, and evolved providing access to a rich sequence, structure, and phenotypic space. "Synthetic genetics", that is, the exploration of these spaces, will illuminate the chemical parameter range for en- and decoding information, 3D folding, and catalysis and yield novel ligands, catalysts, and nanostructures and devices for applications in biotechnology and medicine.},
   author = {Gillian Houlihan and Sebastian Arangundy-Franklin and Philipp Holliger},
   doi = {10.1021/acs.accounts.7b00056},
   isbn = {1520-4898 (Electronic)0001-4842 (Linking)},
   issn = {15204898},
   issue = {4},
   journal = {Accounts of Chemical Research},
   pages = {1079-1087},
   pmid = {28383245},
   title = {Exploring the Chemistry of Genetic Information Storage and Propagation through Polymerase Engineering},
   volume = {50},
   year = {2017},
}
@inproceedings{Mansuripur2004,
   abstract = {Digital information can be encoded in the building-block sequence of macromolecules, such as RNA and single-stranded DNA. Methods of "writing" and "reading" macromolecular strands are currently available, but they are slow and expensive. In an ideal molecular data storage system, routine operations such as write, read, erase, store, and transfer must be done reliably and at high speed within an integrated chip. As a first step toward demonstrating the feasibility of this concept, we report preliminary results of DNA readout experiments conducted in miniaturized chambers that are scalable to even smaller dimensions. We show that translocation of a single-stranded DNA molecule (consisting of 50 adenosine bases followed by 100 cytosine bases) through an ion-channel yields a characteristic signal that is attributable to the 2-segment structure of the molecule. We also examine the dependence of the rate and speed of molecular translocation on the adjustable parameters of the experiment.},
   author = {M Mansuripur and P Khulbe},
   city = {Monterey, California},
   doi = {Doi 10.1117/12.562434},
   isbn = {0277-786X},
   issn = {0277786X},
   journal = {The International Society of Optics and Photonics - Optical Data Storage Topical Meeting},
   title = {Macro-molecular data storage with petabyte/cm(3) density, highly parallel read/write operations, and genuine 3D storage capability},
   year = {2004},
}
@article{Lapique2018,
   abstract = {14–21 . Recent research has shown that digital information storage in DNA, imple-mented using deep sequencing and conventional software, can approach the maximum Shannon information capacity 22 of two bits per nucleotide 23 . In nature, DNA is used to store genetic programs, but the information content of the encod-ing rarely approaches this maximum 24 . We hypothesize that the biological function of a genetic program can be preserved while reducing the length of its DNA encoding and increasing the information content per nucleotide. Here we support this hypothesis by describing an experimental procedure for com-pressing a genetic program and its subsequent autonomous decompression and execution in human cells. As a test-bed we choose an RNAi cell classifier circuit 25 that comprises redun-dant DNA sequences and is therefore amenable for compres-sion, as are many other complex gene circuits 15,18,26–28 . In one example, we implement a compressed encoding of a ten-gene four-input AND gate circuit using only four genetic constructs. The compression principles applied to gene circuits can enable fitting complex genetic programs into DNA delivery vehicles with limited cargo capacity, and storing compressed and bio-logically inert programs in vivo for on-demand activation. The RNAi classifier 25},
   author = {Nicolas Lapique and Yaakov Benenson},
   doi = {10.1038/s41565-017-0004-z},
   isbn = {0000000000000},
   issn = {17483395},
   issue = {4},
   journal = {Nature Nanotechnology},
   pages = {309-315},
   pmid = {26928661},
   publisher = {Springer US},
   title = {Genetic programs can be compressed and autonomously decompressed in live cells},
   volume = {13},
   url = {http://dx.doi.org/10.1038/s41565-017-0004-z},
   year = {2018},
}
@article{Kosuri2014,
   abstract = {For over 60 years, the synthetic production of new DNA sequences has helped researchers understand and engineer biology. Here we summarize methods and caveats for the de novo synthesis of DNA, with particular emphasis on recent technologies that allow for large-scale and low-cost production. In addition, we discuss emerging applications enabled by large-scale de novo DNA constructs, as well as the challenges and opportunities that lie ahead.},
   author = {Sriram Kosuri and George M. Church},
   doi = {10.1038/nmeth.2918},
   isbn = {doi:10.1038/nmeth.2918},
   issn = {15487105},
   issue = {5},
   journal = {Nature Methods},
   pages = {499-507},
   pmid = {24781323},
   title = {Large-scale de novo DNA synthesis: Technologies and applications},
   volume = {11},
   year = {2014},
}
@article{Kashiwamura2005,
   abstract = {DNA is an attractive memory unit because of its immense information density. Here, we describe a memory model made of DNA, called Nested Primer Molecular Memory (NPMM). NPMM consists of many DNA strands, and each DNA strand consists of two areas: a data area and a data address area. When the address of target data is specified, only the target data can be extracted from NPMM. In this paper, we evaluate the validity of the basic operations of NPMM and then discuss the feasibility of scaled-up NPMM through some laboratory experiments. In the latter, we deal with scaled-up NPMM simulated by the Concentration Scaling method. © 2004 Elsevier Ireland Ltd. All rights reserved.},
   author = {Satoshi Kashiwamura and Masahito Yamamoto and Atsushi Kameda and Toshikazu Shiba and Azuma Ohuchi},
   doi = {10.1016/j.biosystems.2004.10.007},
   isbn = {0303-2647 (Print)\r0303-2647 (Linking)},
   issn = {03032647},
   issue = {1},
   journal = {BioSystems},
   keywords = {Concentration Scaling method,DNA computing,DNA memory,High accuracy,NPMM,Nested PCR},
   pages = {99-112},
   pmid = {15740839},
   title = {Potential for enlarging DNA memory: The validity of experimental operations of scaled-up nested primer molecular memory},
   volume = {80},
   year = {2005},
}
@article{Goldman2013,
   abstract = {Digital production, transmission and storage have revolutionized how we access and use information but have also made archiving an increasingly complex task that requires active, continuing maintenance of digital media. This challenge has focused some interest on DNA as an attractive target for information storage because of its capacity for high-density information encoding, longevity under easily achieved conditions and proven track record as an information bearer. Previous DNA-based information storage approaches have encoded only trivial amounts of information or were not amenable to scaling-up, and used no robust error-correction and lacked examination of their cost-efficiency for large-scale information archival. Here we describe a scalable method that can reliably store more information than has been handled before. We encoded computer files totalling 739 kilobytes of hard-disk storage and with an estimated Shannon information of 5.2 × 10(6) bits into a DNA code, synthesized this DNA, sequenced it and reconstructed the original files with 100% accuracy. Theoretical analysis indicates that our DNA-based storage scheme could be scaled far beyond current global information volumes and offers a realistic technology for large-scale, long-term and infrequently accessed digital archiving. In fact, current trends in technological advances are reducing DNA synthesis costs at a pace that should make our scheme cost-effective for sub-50-year archiving within a decade.},
   author = {Nick Goldman and Paul Bertone and Siyuan Chen and Christophe Dessimoz and Emily M. Leproust and Botond Sipos and Ewan Birney},
   doi = {10.1038/nature11875},
   isbn = {0028-0836},
   issn = {00280836},
   issue = {7435},
   journal = {Nature},
   pages = {77-80},
   pmid = {23354052},
   publisher = {Nature Publishing Group},
   title = {Towards practical, high-capacity, low-maintenance information storage in synthesized DNA},
   volume = {494},
   url = {http://dx.doi.org/10.1038/nature11875},
   year = {2013},
}
@article{Gibson2010,
   abstract = {We report the design, synthesis, and assembly of the 1.08–mega–base pair Mycoplasma mycoides JCVI-syn1.0 genome starting from digitized genome sequence information and its transplantation into a M. capricolum recipient cell to create new M. mycoides cells that are controlled only by the synthetic chromosome. The only DNA in the cells is the designed synthetic DNA sequence, including " watermark " sequences and other designed gene deletions and polymorphisms, and mutations acquired during the building process. The new cells have expected phenotypic properties and are capable of continuous self-replication. I n 1977, Sanger and colleagues determined the complete genetic sequence of phage ϕX174 (1), the first DNA genome to be completely sequenced. Eighteen years later, in 1995, our team was able to read the first complete genetic sequence of a self-replicating bacterium, Haemophilus influenzae (2). Reading the genetic sequence of a wide range of species has increased exponentially from these early studies. The ability to rapidly digitize genomic information has increased by more than eight orders of mag-nitude over the past 25 years (3). Efforts to un-derstand all this new genomic information have spawned numerous new computational and experimental paradigms, yet our genomic knowl-edge remains very limited. No single cellular system has all of its genes understood in terms of their biological roles. Even in simple bacterial cells, do the chromosomes contain the entire ge-netic repertoire? If so, can a complete genetic sys-tem be reproduced by chemical synthesis starting with only the digitized DNA sequence contained in a computer? Our interest in synthesis of large DNA mol-ecules and chromosomes grew out of our efforts over the past 15 years to build a minimal cell that contains only essential genes. This work was inaugurated in 1995 when we sequenced the genome of Mycoplasma genitalium, a bacterium with the smallest complement of genes of any known organism capable of independent growth in the laboratory. More than 100 of the 485 protein-coding genes of M. genitalium are dispensable when disrupted one at a time (4–6). We developed a strategy for assembling viral-sized pieces to produce large DNA molecules that enabled us to assemble a synthetic M. genitalium genome in four stages from chemically synthe-sized DNA cassettes averaging about 6 kb in size. This was accomplished through a combination of in vitro enzymatic methods and in vivo recombi-nation in Saccharomyces cerevisiae. The whole synthetic genome [582,970 base pairs (bp)] was stably grown as a yeast centromeric plasmid (YCp) (7). Several hurdles were overcome in transplanting and expressing a chemically synthesized chromo-some in a recipient cell. We needed to improve methods for extracting intact chromosomes from yeast. We also needed to learn how to transplant these genomes into a recipient bacterial cell to establish a cell controlled only by a synthetic ge-nome. Because M. genitalium has an extremely slow growth rate, we turned to two faster-growing mycoplasma species, M. mycoides subspecies capri (GM12) as donor, and M. capricolum sub-species capricolum (CK) as recipient. To establish conditions and procedures for transplanting the synthetic genome out of yeast, we developed methods for cloning entire bacterial chromosomes as centromeric plasmids in yeast, including a native M. mycoides genome (8, 9). However, initial attempts to extract the M. mycoides genome from yeast and transplant it into M. capricolum failed. We discovered that the donor and recipient mycoplasmas share a com-mon restriction system. The donor genome was methylated in the native M. mycoides cells and was therefore protected against restriction during the transplantation from a native donor cell (10). However, the bacterial genomes grown in yeast are unmethylated and so are not protected from the single restriction system of the recipient cell. We overcame this restriction barrier by methylat-ing the donor DNA with purified methylases or},
   author = {Daniel G. Gibson and John I. Glass and Carole Lartigue and Vladimir N. Noskov and Ray Yuan Chuang and Mikkel A. Algire and Gwynedd A. Benders and Michael G. Montague and Li Ma and Monzia M. Moodie and Chuck Merryman and Sanjay Vashee and Radha Krishnakumar and Nacyra Assad-Garcia and Cynthia Andrews-Pfannkoch and Evgeniya A. Denisova and Lei Young and Zhi Ng Qi and Thomas H. Segall-Shapiro and Christopher H. Calvey and Prashanth P. Parmar and Clyde A. Hutchison and Hamilton O. Smith and J. Craig Venter},
   doi = {10.1126/science.1190719},
   isbn = {1095-9203 (Electronic)\n0036-8075 (Linking)},
   issn = {00368075},
   issue = {5987},
   journal = {Science},
   pages = {52-56},
   pmid = {20488990},
   title = {Creation of a bacterial cell controlled by a chemically synthesized genome},
   volume = {329},
   year = {2010},
}
@article{Erlich2017,
   author = {Yaniv Erlich and Dina Zielinski},
   doi = {10.1126/science.aaj2038},
   issue = {March},
   pages = {950-954},
   title = {Supplementary Materials for DNA Fountain enables a robust and efficient storage architecture},
   volume = {954},
   year = {2017},
}
@article{Mearian,
   author = {Lucus Mearian},
   journal = {Computerworld},
   pages = {4},
   title = {DNA Could Be Used for Data Storage},
}
@article{Hesketh2018,
   abstract = {Close collaboration between specialists from diverse backgrounds and working in different scientific domains is an effective strategy to overcome challenges in areas that interface between biology, chemistry, physics and engineering. Communication in such collaborations can itself be challenging.  Even when projects are successfully concluded, resulting publications - necessarily multi-authored - have the potential to be disjointed. Few, both in the field and outside, may be able to fully understand the work as a whole. This needs to be addressed to facilitate efficient working, peer review, accessibility and impact to larger audiences. We are an interdisciplinary team working in a nascent scientific area, the repurposing of DNA as a storage medium for digital information. In this note, we highlight some of the difficulties that arise from such collaborations and outline our efforts to improve communication through a glossary and a controlled vocabulary and accessibility via short plain-language summaries. We hope to stimulate early discussion within this emerging field of how our community might improve the description and presentation of our work to facilitate clear communication within and between research groups and increase accessibility to those not familiar with our respective fields - be it molecular biology, computer science, information theory or others that might become relevant in future. To enable an open and inclusive discussion we have created a glossary and controlled vocabulary as a cloud-based shared document and we invite other scientists to critique our suggestions and contribute their own ideas.},
   author = {Emily E. Hesketh and Jossy Sayir and Nick Goldman},
   doi = {10.12688/f1000research.13482.1},
   issn = {2046-1402},
   issue = {May},
   journal = {F1000Research},
   keywords = {communication,controlled vocabulary,digital information storage in,dna,dna-storage,glossary,interdisciplinary collaboration,short plain-language summaries,synthetic biology},
   pages = {39},
   pmid = {29707196},
   title = {Improving communication for interdisciplinary teams working on storage of digital information in DNA},
   volume = {7},
   url = {https://f1000research.com/articles/7-39/v1},
   year = {2018},
}
@article{Hameed2011,
   author = {Kashif Hameed},
   issue = {1},
   journal = {International Journal of Emerging Science},
   title = {DNA Computation Based Approach for Enhanced Computing Power},
   volume = {1},
   year = {2011},
}
@article{Grass2015,
   abstract = {Information, such as text printed on paper or images projected onto microfilm, can survive for over 500 years. However, the storage of digital information for time frames exceeding 50 years is challenging. Here we show that digital information can be stored on DNA and recovered without errors for considerably longer time frames. To allow for the perfect recovery of the information, we encapsulate the DNA in an inorganic matrix, and employ error-correcting codes to correct storage-related errors. Specifically, we translated 83 kB of information to 4991 DNA segments, each 158 nucleotides long, which were encapsulated in silica. Accelerated aging experiments were performed to measure DNA decay kinetics, which show that data can be archived on DNA for millennia under a wide range of conditions. The original information could be recovered error free, even after treating the DNA in silica at 70 °C for one week. This is thermally equivalent to storing information on DNA in central Europe for 2000 years.},
   author = {Robert N. Grass and Reinhard Heckel and Michela Puddu and Daniela Paunescu and Wendelin J. Stark},
   doi = {10.1002/anie.201411378},
   isbn = {2012290248},
   issn = {15213773},
   issue = {8},
   journal = {Angewandte Chemie - International Edition},
   keywords = {DNA,Fossils,Information storage,Long-term memory,Sol-gel processes},
   pages = {2552-2555},
   pmid = {25650567},
   title = {Robust chemical preservation of digital information on DNA in silica with error-correcting codes},
   volume = {54},
   year = {2015},
}
@article{Benenson2003,
   abstract = {The unique properties of DNA make it a fundamental building block in the fields of supramolecular chemistry, nanotechnology, nano-circuits, molecular switches, molecular devices, and molecular computing. In our recently introduced autonomous molecular automaton, DNA molecules serve as input, output, and software, and the hardware consists of DNA restriction and ligation enzymes using ATP as fuel. In addition to information, DNA stores energy, available on hybridization of complementary strands or hydrolysis of its phosphodiester backbone. Here we show that a single DNA molecule can provide both the input data and all of the necessary fuel for a molecular automaton. Each computational step of the automaton consists of a reversible software molecule input molecule hybridization followed by an irreversible software-directed cleavage of the input molecule, which drives the computation forward by increasing entropy and releasing heat. The cleavage uses a hitherto unknown capability of the restriction enzyme FokI, which serves as the hardware, to operate on a noncovalent software input hybrid. In the previous automaton, software input ligation consumed one software molecule and two ATP molecules per step. As ligation is not performed in this automaton, a fixed amount of software and hardware molecules can, in principle, process any input molecule of any length without external energy supply. Our experiments demonstrate 3 x 10(12) automata per microl performing 6.6 x 10(10) transitions per second per microl with transition fidelity of 99.9%, dissipating about 5 x 10(-9) W microl as heat at ambient temperature.},
   author = {Y. Benenson and R. Adar and T. Paz-Elizur and Z. Livneh and E. Shapiro},
   doi = {10.1073/pnas.0535624100},
   isbn = {0027-8424},
   issn = {0027-8424},
   issue = {5},
   journal = {Proceedings of the National Academy of Sciences},
   pages = {2191-2196},
   pmid = {12601148},
   title = {DNA molecule provides a computing machine with both data and fuel},
   volume = {100},
   url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0535624100},
   year = {2003},
}
@article{Heckel2018,
   abstract = {New research sets a world record in the volume of data stored in and retrieved from DNA.},
   author = {Reinhard Heckel},
   doi = {10.1038/nbt.4093},
   issn = {15461696},
   issue = {3},
   journal = {Nature Biotechnology},
   pages = {236-237},
   pmid = {29509740},
   publisher = {Nature Publishing Group},
   title = {An archive written in DNA},
   volume = {36},
   url = {http://dx.doi.org/10.1038/nbt.4093},
   year = {2018},
}
@article{Baum1995,
   author = {Eric B Baum},
   issue = {5210},
   journal = {Science},
   pages = {583-585},
   title = {Building an Associative Memory Vastly Larger Than the Brain Author ( s ): Eric B . Baum Published by : American Association for the Advancement of Science Stable URL : https://www.jstor.org/stable/2886656 digitize , preserve and extend access to Science B},
   volume = {268},
   year = {1995},
}
@article{Bancroft2001,
   author = {Carter Bancroft and Timothy Bowler and Brian Bloom and Catherine Taylor Clelland},
   doi = {10.1038/020493a0},
   isbn = {9780470114735},
   issn = {0028-0836},
   issue = {5536},
   journal = {Science},
   pages = {1763-1765},
   pmid = {8539599},
   title = {Long-Term Storage of Information in DNA},
   volume = {293},
   year = {2001},
}
@article{Boeke2016,
   author = {Jef D. Boeke and George M. Church and Andrew Hessel and Nancy J Kelley},
   pages = {1-22},
   title = {Genome Project-write: A Grand Challenge Using Synthesis, Gene Editing and Other Technologies to Understand, Engineer and Test Living Systems},
   year = {2016},
}
@misc{TwistBioscience,
   author = {Twist Bioscience},
   pages = {1-5},
   title = {DNA-Based Digital Storage (White Paper)},
}
@article{Extance2002,
   author = {Andy Extance},
   pages = {3-6},
   title = {Digital DNA},
   year = {2002},
}
@article{Church2012,
   author = {George M. Church and Yuan Gao and Sriram Kosuri},
   doi = {10.1007/sl0869-007-9037-x},
   isbn = {0506006905},
   issue = {6102},
   journal = {Science},
   pages = {1628},
   title = {Next-Generation Digital Information Storage in DNA},
   volume = {337},
   year = {2012},
}
@article{Bornholt2016,
   author = {J Bornholt},
   doi = {10.1145/2872362.2872397},
   isbn = {9781450340915},
   issn = {0163-5980},
   issue = {2},
   journal = {ACM SIGOPS Operating Systems Review},
   keywords = {archival storage,dna,molecular computing},
   pages = {637},
   title = {A DNA-based archival storage system},
   volume = {50},
   year = {2016},
}
@article{Alaeddini2010,
   abstract = {Forensic DNA identification techniques are principally based on determination of the size or sequence of desired PCR products. The fragmentation of DNA templates or the structural modifications that can occur during the decomposition process can impact the outcomes of the analytical procedures. This study reviews the pathways involved in cell death and DNA decomposition and the subsequent difficulties these present in DNA analysis of degraded samples. © 2009 Elsevier Ireland Ltd.},
   author = {Reza Alaeddini and Simon J. Walsh and Ali Abbas},
   doi = {10.1016/j.fsigen.2009.09.007},
   isbn = {1872-4973},
   issn = {18724973},
   issue = {3},
   journal = {Forensic Science International: Genetics},
   keywords = {Cell death,DNA degradation,Deoxyribonuclease,Forensics,PCR},
   pages = {148-157},
   pmid = {20215026},
   publisher = {Elsevier Ireland Ltd},
   title = {Forensic implications of genetic analyses from degraded DNA-A review},
   volume = {4},
   url = {http://dx.doi.org/10.1016/j.fsigen.2009.09.007},
   year = {2010},
}
@article{Adler2011,
   abstract = {The recovery of genetic material from preserved hard skeletal remains is an essential part of ancient DNA, archaeological and forensic research. However, there is little understanding about the relative concentrations of DNA within different tissues, the impact of sampling methods on extracted DNA, or the role of environmentally-determined degradation rates on DNA survival in specimens. We examine these issues by characterizing the mitochondrial DNA (mtDNA) content of different hard and soft tissues in 42 ancient human and bovid specimens at a range of fragment lengths (77-235 bp) using real-time PCR. Remarkably, the standard drill speeds used to sample skeletal material (c. 1000 RPM) were found to decrease mtDNA yields up to 30 times (by 3.1 × 105mtDNA copies on average) compared to pulverization in a bone mill. This dramatic negative impact appears to relate to heat damage, and disappeared at very low drill speeds (e.g. 100 RPM). Consequently, many ancient DNA and forensic studies may have obtained false negative results, especially from important specimens which are commonly sampled with drills to minimize signs of damage. The mtDNA content of tooth cementum was found to be five times higher than the commonly used dentine (141 bp, p = 0.01), making the cementum-rich root tip the best sample for ancient human material. Lastly, mtDNA was found to display a consistent pattern of exponential fragmentation across many depositional environments, with different rates for geographic areas and tissue types, improving the ability to predict and understand DNA survival in preserved specimens. © 2010.},
   author = {C. J. Adler and W. Haak and D. Donlon and A. Cooper},
   doi = {10.1016/j.jas.2010.11.010},
   isbn = {0305-4403},
   issn = {10959238},
   issue = {5},
   journal = {Journal of Archaeological Science},
   keywords = {Ancient human mtDNA,Cementum,DNA fragmentation,Dentine,Drilling,Pulverizing},
   pages = {956-964},
   title = {Survival and recovery of DNA from ancient teeth and bones},
   volume = {38},
   year = {2011},
}
@article{Akin2011,
   abstract = {DNA Computing is a rapidly-developing interdisciplinary area which could benefit from more experimental results to solve practical problems with the current biological tools. In this study, we have integrated microelectronics and molecular biology techniques for the storage of information and basic arithmetic operations via DNA. Using 16 different complementary sequences of DNA, we stored 4 bits of information on an electronic microarray and read the data via the fluorescent signal strength coming from the microarray pads. We also showed the possibility of addition and subtraction of quantities of fluorescently tagged DNA determined via their fluorescent signal strength. We conclude that the hybrid technology we employed, based on a matured Si-CMOS platform, has the potential to strengthen the pursuit of DNA Computation as well as finding its own niche applications. Copyright © 2011 American Scientific Publishers.},
   author = {H E Akin and D A O Karabay and J R Kyle and A P Mills Jr. and C S Ozkan and M Ozkan},
   doi = {10.1166/jnn.2011.38844717},
   issn = {15334899},
   issue = {6},
   journal = {Journal of Nanoscience and Nanotechnology},
   keywords = {Computing,DNA,Fluorescence,Hybridization,Massive Parallelism,Microarray,Permeation Layer},
   pages = {4717-4723},
   title = {Electronic microarrays in DNA computing},
   volume = {11},
   url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-80051757205&partnerID=40&md5=12fe94548220735a574d28cb4a4f006b},
   year = {2011},
}
@article{Alonso2004,
   abstract = {We explore different designs to estimate both nuclear and mitochondrial human DNA (mtDNA) content based on the detection of the 5′ nuclease activity of the Taq DNA polymerase using fluorogenic probes and a real-time quantitative PCR detection system. Human mtDNA quantification was accomplished by monitoring the real-time progress of the PCR-amplification of two different fragment sizes (113 and 287bp) within the hypervariable region I (HV1) of the mtDNA control region, using two fluorogenic probes to specifically determine the mtDNA copy of each fragment size category. This mtDNA real-time PCR design has been used to assess the mtDNA preservation (copy number and degradation state) of DNA samples retrieved from 500 to 1500 years old human remains that showed low copy number and highly degraded mtDNA. The quantification of nuclear DNA was achieved by real-time PCR of a segment of the X-Y homologous amelogenin (AMG) gene that allowed the simultaneous estimation of a Y-specific fragment (AMGY: 112bp) and a X-specific fragment (AMGX: 106bp) making possible not only haploid or diploid DNA quantitation but also sex determination. The AMG real-time PCR design has been used to quantify a set of 57 DNA samples from 4-5 years old forensic bone remains with improved sensitivity compared with the slot-blot hybridization method. The potential utility of this technology to improve the quality of some PCR-based forensic and ancient DNA studies (microsatellite typing and mtDNA sequencing) is discussed. © 2003 Elsevier Ireland Ltd. All rights reserved.},
   author = {Antonio Alonso and Pablo Martín and Cristina Albarrán and Pilar García and Oscar García and Lourdes Fernández De Simón and Julia García-Hirschfeld and Manuel Sancho and Concepción De La Rúa and Jose Fernández-Piqueras},
   doi = {10.1016/j.forsciint.2003.10.008},
   isbn = {0379-0738 (Print)\n0379-0738 (Linking)},
   issn = {03790738},
   issue = {2-3},
   journal = {Forensic Science International},
   keywords = {Amelogenin gene,Ancient DNA,Forensic genetics,Mitochondrial DNA control region,Real-time PCR},
   pages = {141-149},
   pmid = {15040907},
   title = {Real-time PCR designs to estimate nuclear and mitochondrial DNA copy number in forensic and ancient DNA studies},
   volume = {139},
   year = {2004},
}
@article{DeAlmeida2013,
   abstract = {Long-term stored DNA can be sometimes the only source of genetic material of an organism that does not exist anymore, but a research interest still persists. However, there is a lack of information about useful methods to improve quality from such type of material. In this study, we compared four different protocols using DNA samples collected in 1998. Fresh DNA was also tested aiming to check the differences between these two material types. Sixteen samples of each DNA type treated with phenol-chloroform with PEG 5.0%, silica-gel membrane spin column, PEG 7.5%, and glass-fiber matrix spin column were submitted to spectrophotometer measurements, electrophoresis, PCR, and RFLP-PCR to assess the best method concerning yield, quality, and purity. Based on the results, purification with PEG 7.5% was considered the best method to treat aged DNA samples. In addition to the efficiency, this protocol has low cost. Analyzing the data, we also conclude that long-term stored DNA may be considered a reliable and potential resource for future molecular studies.},
   author = {Máira Pedroso De Almeida and Carlos Souza do Nascimento and Iuri Viotti Périssé and Marcio de Souza Duarte and Renata Veroneze and Simone E. Facioni Guimarães},
   doi = {10.1002/elps.201300245},
   issn = {01730835},
   issue = {20-21},
   journal = {Electrophoresis},
   keywords = {DNA purification,Long-term storage,Polymerase chain reaction},
   pages = {3039-3045},
   pmid = {23893799},
   title = {Treatment of long-term stored DNA-Comparison between different methods to obtain high-quality material},
   volume = {34},
   year = {2013},
}
@article{Ball2016,
   author = {Philip Ball and Philip Ball},
   keywords = {biophysics,chemical biology,complexity,computational biology,statistical physics},
   title = {The problems of biological information Opinion piece Subject Areas : Author for correspondence :},
   year = {2016},
}
@article{Bulik-Sullivan2015,
   abstract = {Identifying genetic correlations between complex traits and diseases can provide useful etiological insights and help prioritize likely causal relationships. The major challenges preventing estimation of genetic correlation from genome-wide association study (GWAS) data with current methods are the lack of availability of individual-level genotype data and widespread sample overlap among meta-analyses. We circumvent these difficulties by introducing a technique-cross-trait LD Score regression-for estimating genetic correlation that requires only GWAS summary statistics and is not biased by sample overlap. We use this method to estimate 276 genetic correlations among 24 traits. The results include genetic correlations between anorexia nervosa and schizophrenia, anorexia and obesity, and educational attainment and several diseases. These results highlight the power of genome-wide analyses, as there currently are no significantly associated SNPs for anorexia nervosa and only three for educational attainment.},
   author = {Brendan Bulik-Sullivan and Hilary K. Finucane and Verneri Anttila and Alexander Gusev and Felix R. Day and Po Ru Loh and Laramie Duncan and John R.B. Perry and Nick Patterson and Elise B. Robinson and Mark J. Daly and Alkes L. Price and Benjamin M. Neale},
   doi = {10.1038/ng.3406},
   isbn = {1061-4036},
   issn = {15461718},
   issue = {11},
   journal = {Nature Genetics},
   pages = {1236-1241},
   pmid = {26414676},
   publisher = {Nature Publishing Group},
   title = {An atlas of genetic correlations across human diseases and traits},
   volume = {47},
   url = {http://dx.doi.org/10.1038/ng.3406},
   year = {2015},
}
@article{Boyle2017,
   abstract = {A central goal of genetics is to understand the links between genetic variation and disease. Intuitively, one might expect disease-causing variants to cluster into key pathways that drive disease etiology. But for complex traits, association signals tend to be spread across most of the genome—including near many genes without an obvious connection to disease. We propose that gene regulatory networks are sufficiently interconnected such that all genes expressed in disease-relevant cells are liable to affect the functions of core disease-related genes and that most heritability can be explained by effects on genes outside core pathways. We refer to this hypothesis as an “omnigenic” model.},
   author = {Evan A. Boyle and Yang I. Li and Jonathan K. Pritchard},
   doi = {10.1016/j.cell.2017.05.038},
   isbn = {0092-8674, 1097-4172},
   issn = {10974172},
   issue = {7},
   journal = {Cell},
   pages = {1177-1186},
   pmid = {28622505},
   publisher = {Elsevier},
   title = {An Expanded View of Complex Traits: From Polygenic to Omnigenic},
   volume = {169},
   url = {http://dx.doi.org/10.1016/j.cell.2017.05.038},
   year = {2017},
}
@article{Leslie2015,
   abstract = {Fine-scale genetic variation between human populations is interesting as a signature of historical demographic events and because of its potential for confounding disease studies. We use haplotype-based statistical methods to analyse genome-wide single nucleotide polymorphism (SNP) data from a carefully chosen geographically diverse sample of 2,039 individuals from the United Kingdom. This reveals a rich and detailed pattern of genetic differentiation with remarkable concordance between genetic clusters and geography. The regional genetic differentiation and differing patterns of shared ancestry with 6,209 individuals from across Europe carry clear signals of historical demographic events. We estimate the genetic contribution to southeastern England from Anglo-Saxon migrations to be under half, and identify the regions not carrying genetic material from these migrations. We suggest significant pre-Roman but post-Mesolithic movement into southeastern England from continental Europe, and show that in non-Saxon parts of the United Kingdom, there exist genetically differentiated subgroups rather than a general ‘Celtic’ population.},
   author = {Stephen Leslie and Bruce Winney and Garrett Hellenthal and Dan Davison and Abdelhamid Boumertit and Tammy Day and Katarzyna Hutnik and Ellen C. Royrvik and Barry Cunliffe and Daniel J. Lawson and Daniel Falush and Colin Freeman and Matti Pirinen and Simon Myers and Mark Robinson and Peter Donnelly and Walter Bodmer},
   doi = {10.1038/nature14230},
   isbn = {0000017892},
   issn = {14764687},
   issue = {7543},
   journal = {Nature},
   pages = {309-314},
   pmid = {25788095},
   title = {The fine-scale genetic structure of the British population},
   volume = {519},
   year = {2015},
}
@article{Gibson2012,
   abstract = {Genome-wide association studies have greatly improved our understanding of the genetic basis of disease risk. The fact that they tend not to identify more than a fraction of the specific causal loci has led to divergence of opinion over whether most of the variance is hidden as numerous rare variants of large effect or as common variants of very small effect. Here I review 20 arguments for and against each of these models of the genetic basis of complex traits and conclude that both classes of effect can be readily reconciled.},
   author = {Greg Gibson},
   doi = {10.1038/nrg3118},
   isbn = {1471-0064 (Electronic)\r1471-0056 (Linking)},
   issn = {14710056},
   issue = {2},
   journal = {Nature Reviews Genetics},
   pages = {135-145},
   pmid = {22251874},
   publisher = {Nature Publishing Group},
   title = {Rare and common variants: Twenty arguments},
   volume = {13},
   url = {http://dx.doi.org/10.1038/nrg3118},
   year = {2012},
}
@article{Malaspinas2016,
   abstract = {The population history of Aboriginal Australians remains largely uncharacterized. Here we generate high-coverage genomes for 83 Aboriginal Australians (speakers of Pama–Nyungan languages) and 25 Papuans from the New Guinea Highlands. We find that Papuan and Aboriginal Australian ancestors diversified 25–40 thousand years ago (kya), suggesting pre-Holocene population structure in the ancient continent of Sahul (Australia, New Guinea and Tasmania). However, all of the studied Aboriginal Australians descend from a single founding population that differentiated ~10–32 kya. We infer a population expansion in northeast Australia during the Holocene epoch (past 10,000 years) associated with limited gene flow from this region to the rest of Australia, consistent with the spread of the Pama–Nyungan languages. We estimate that Aboriginal Australians and Papuans diverged from Eurasians 51–72 kya, following a single out-of-Africa dispersal, and subsequently admixed with archaic populations. Finally, we report evidence of selection in Aboriginal Australians potentially associated with living in the desert.},
   author = {Anna Sapfo Malaspinas and Michael C. Westaway and Craig Muller and Vitor C. Sousa and Oscar Lao and Isabel Alves and Anders Bergström and Georgios Athanasiadis and Jade Y. Cheng and Jacob E. Crawford and Tim H. Heupink and Enrico MacHoldt and Stephan Peischl and Simon Rasmussen and Stephan Schiffels and Sankar Subramanian and Joanne L. Wright and Anders Albrechtsen and Chiara Barbieri and Isabelle Dupanloup and Anders Eriksson and Ashot Margaryan and Ida Moltke and Irina Pugach and Thorfinn S. Korneliussen and Ivan P. Levkivskyi and J. Víctor Moreno-Mayar and Shengyu Ni and Fernando Racimo and Martin Sikora and Yali Xue and Farhang A. Aghakhanian and Nicolas Brucato and Søren Brunak and Paula F. Campos and Warren Clark and Sturla Ellingvåg and Gudjugudju Fourmile and Pascale Gerbault and Darren Injie and George Koki and Matthew Leavesley and Betty Logan and Aubrey Lynch and Elizabeth A. Matisoo-Smith and Peter J. McAllister and Alexander J. Mentzer and Mait Metspalu and Andrea B. Migliano and Les Murgha and Maude E. Phipps and William Pomat and Doc Reynolds and Francois Xavier Ricaut and Peter Siba and Mark G. Thomas and Thomas Wales and Colleen Ma Run Wall and Stephen J. Oppenheimer and Chris Tyler-Smith and Richard Durbin and Joe Dortch and Andrea Manica and Mikkel H. Schierup and Robert A. Foley and Marta Mirazón Lahr and Claire Bowern and Jeffrey D. Wall and Thomas Mailund and Mark Stoneking and Rasmus Nielsen and Manjinder S. Sandhu and Laurent Excoffier and David M. Lambert and Eske Willerslev},
   doi = {10.1038/nature18299},
   isbn = {0008-5472 (Print)\r0008-5472 (Linking)},
   issn = {14764687},
   issue = {7624},
   journal = {Nature},
   pages = {207-214},
   pmid = {11507039},
   publisher = {Nature Publishing Group},
   title = {A genomic history of Aboriginal Australia},
   volume = {538},
   url = {http://dx.doi.org/10.1038/nature18299},
   year = {2016},
}
@article{Falush2016,
   abstract = {Genetic clustering algorithms, implemented in popular programs such as STRUCTURE and ADMIXTURE, have been used extensively in the characterisation of individuals and populations based on genetic data. A successful example is reconstruction of the genetic history of African Americans who are a product of recent admixture between highly differentiated populations. Histories can also be reconstructed using the same procedure for groups which do not have admixture in their recent history, where recent genetic drift is strong or that deviate in other ways from the underlying inference model. Unfortunately, such histories can be misleading. We have implemented an approach (available at www.paintmychromsomes.com) to assessing the goodness of fit of the model using the ancestry 'palettes' estimated by CHROMOPAINTER and apply it to both simulated and real examples. Combining these complementary analyses with additional methods that are designed to test specific hypothesis allows a richer and more robust analysis of recent demographic history based on genetic data.},
   author = {Daniel Falush and Lucy van Dorp and Daniel J. Lawson},
   doi = {10.1101/066431},
   isbn = {4146701805257},
   issn = {2041-1723},
   journal = {bioRxiv},
   pages = {066431},
   title = {A tutorial on how (not) to over-interpret STRUCTURE/ADMIXTURE bar plots},
   year = {2016},
}
@article{Uyeda2018,
   abstract = {As a result of the process of descent with modification, closely related species tend to be similar to one another in a myriad different ways. In statistical terms, this means that traits measured on one species will not be independent of traits measured on others. Since their introduction in the 1980s, phylogenetic comparative methods (PCMs) have been framed as a solution to this problem. In this paper, we argue that this way of thinking about PCMs is deeply misleading. Not only has this sowed widespread confusion in the literature about what PCMs are doing but has led us to develop methods that are susceptible to the very thing we sought to build defenses against \---| unreplicated evolutionary events. Through three Case Studies, we demonstrate that the susceptibility to singular events indeed a recurring problem in comparative biology that links several seemingly unrelated controversies. In each Case Study we propose a potential solution to the problem. While the details of our proposed solutions differ, they share a common theme: unifying hypothesis testing with data-driven approaches (which we term "phylogenetic natural history") to disentangle the impact of singular evolutionary events from that of the factors we are investigating. More broadly, we argue that our field has, at times, been sloppy when weighing evidence in support of causal hypotheses. We suggest that one way to refine our inferences is to re-imagine phylogenies as probabilistic graphical models; adopting this way of thinking will help clarify precisely what we are testing and what evidence supports our claims.},
   author = {Josef C Uyeda and Rosana Zenil-Ferguson and Matthew W Pennell},
   doi = {10.1093/sysbio/syy031},
   issn = {1063-5157},
   journal = {Systematic Biology},
   keywords = {causality,graphical models,macroevolution,phylogenetic natural},
   title = {Rethinking phylogenetic comparative methods},
   url = {https://academic.oup.com/sysbio/advance-article/doi/10.1093/sysbio/syy031/4985805},
   year = {2018},
}
@article{Burnham2011,
   abstract = {We briefly outline the information-theoretic (I-T) approaches to valid inference including a review of some simple methods for making formal inference from all the hypotheses in the model set (multimodel inference). The I-T approaches can replace the usual t tests and ANOVA tables that are so inferentially limited, but still commonly used. The I-T methods are easy to compute and understand and provide formal measures of the strength of evidence for both the null and alternative hypotheses, given the data. We give an example to highlight the importance of deriving alternative hypotheses and representing these as probability models. Fifteen technical issues are addressed to clarify various points that have appeared incorrectly in the recent literature. We offer several remarks regarding the future of empirical science and data analysis under an I-T framework.},
   author = {Kenneth P. Burnham and David R. Anderson and Kathryn P. Huyvaert},
   doi = {10.1007/s00265-010-1029-6},
   isbn = {0340-5443},
   issn = {03405443},
   issue = {1},
   journal = {Behavioral Ecology and Sociobiology},
   keywords = {AIC,Evidence,Kullback-Leibler information,Model averaging,Model likelihoods,Model probabilities,Model selection,Multimodel inference},
   pages = {23-35},
   pmid = {3527},
   title = {AIC model selection and multimodel inference in behavioral ecology: Some background, observations, and comparisons},
   volume = {65},
   year = {2011},
}
@article{Burnham2014,
   abstract = {Early statistical methods focused on pre-data probability statements (i.e., data as random variables) such as P values; these are not really inferences nor are P values evidential. Statistical science clung to these principles throughout much of the 20th century as a wide variety of methods were developed for special cases. Looking back, it is clear that the underlying paradigm (i.e., testing and P values) was weak. As Kuhn (1970) suggests, new paradigms have taken the place of earlier ones: this is a goal of good science. New methods have been developed and older methods extended and these allow proper measures of strength of evidence and multimodel inference. It is time to move forward with sound theory and practice for the difficult practical problems that lie ahead. Given data the useful foundation shifts to post-data probability statements such as model probabilities (Akaike weights) or related quantities such as odds ratios and likelihood intervals. These new methods allow formal inference from multiple models in the a prior set. These quantities are properly evidential. The past century was aimed at finding the "best" model and making inferences from it. The goal in the 21st century is to base inference on all the models weighted by their model probabilities (model averaging). Estimates of precision can include model selection uncertainty leading to variances conditional on the model set. The 21st century will be about the quantification of information, proper measures of evidence, and multi-model inference. Nelder (1999:261) concludes, "The most important task before us in developing statistical science is to demolish the P-value culture, which has taken root to a frightening extent in many areas of both pure and applied science and technology".},
   author = {K P Burnham and David R. Anderson},
   doi = {10.1890/13-1066.1},
   isbn = {0012-9658},
   issn = {0012-9658},
   issue = {3},
   journal = {Ecology},
   keywords = {Data Interpretation,Models,Research Design,Statistical},
   pages = {627-30},
   pmid = {24804444},
   title = {P values are only an index to evidence: 20th- vs. 21st-century statistical science.},
   volume = {95},
   url = {http://www.ncbi.nlm.nih.gov/pubmed/24804444},
   year = {2014},
}
@article{Anderson2002,
   author = {David R. Anderson and Kenneth P. Burnham},
   issue = {3},
   journal = {Journal of Wildlife Management},
   keywords = {akaike,analysis guidelines,any methodology can be,in the sci-,information,information-theoretic methods,kullback-leibler,misused,model selection uncertainty,modeling,p-values,s information criterion,support for null hypothesis,testing and associat-},
   pages = {912-918},
   title = {Avoiding Pitfalls When Using Information-Theoretic Methods Author ( s ): David R . Anderson and Kenneth P . Burnham Published by : Wiley on behalf of the Wildlife Society Stable URL : http://www.jstor.org/stable/3803155},
   volume = {66},
   year = {2002},
}
@book{Anderson2008,
   author = {D.R. Anderson},
   isbn = {9780387740737},
   publisher = {Springer},
   title = {Model Based Inference in the
Life Sciences: A Primer on Evidence},
   year = {2008},
}
@article{Rabosky2010,
   abstract = {Molecular phylogenies contain information about the tempo and mode of species diversification through time. Because extinction leaves a characteristic signature in the shape of molecular phylogenetic trees, many studies have used data from extant taxa only to infer extinction rates. This is a promising approach for the large number of taxa for which extinction rates cannot be estimated from the fossil record. Here, I explore the consequences of violating a common assumption made by studies of extinction from phylogenetic data. I show that when diversification rates vary among lineages, simple estimators based on the birth-death process are unable to recover true extinction rates. This is problematic for phylogenetic trees with complete taxon sampling as well as for the simpler case of clades with known age and species richness. Given the ubiquity of variation in diversification rates among lineages and clades, these results suggest that extinction rates should not be estimated in the absence of fossil data.},
   author = {Daniel L. Rabosky},
   doi = {10.1111/j.1558-5646.2009.00926.x},
   isbn = {0014-3820},
   issn = {00143820},
   issue = {6},
   journal = {Evolution},
   keywords = {Adaptive radiation,Extinction,Macroevolution,Phylogenetics},
   pages = {1816-1824},
   pmid = {20030708},
   title = {Extinction rates should not be estimated from molecular phylogenies},
   volume = {64},
   year = {2010},
}
@article{Quental2010,
   abstract = {Over the last two decades, new tools in the analysis of molecular phylogenies have enabled study of the diversification dynamics of living clades in the absence of information about extinct lineages. However, computer simulations and the fossil record show that the inability to access extinct lineages severely limits the inferences that can be drawn from molecular phylogenies. It appears that molecular phylogenies can tell us only when there have been changes in diversification rates, but are blind to the true diversity trajectories and rates of origination and extinction that have led to the species that are alive today. We need to embrace the fossil record if we want to fully understand the diversity dynamics of the living biota. © 2010 Elsevier Ltd.},
   author = {Tiago B. Quental and Charles R. Marshall},
   doi = {10.1016/j.tree.2010.05.002},
   isbn = {0169-5347},
   issn = {01695347},
   issue = {8},
   journal = {Trends in Ecology and Evolution},
   pages = {435-441},
   pmid = {20646780},
   publisher = {Elsevier Ltd},
   title = {Diversity dynamics: Molecular phylogenies need the fossil record},
   volume = {25},
   url = {http://dx.doi.org/10.1016/j.tree.2010.05.002},
   year = {2010},
}
@article{Marshall2017,
   abstract = {Five laws derived from fossil data describe the relationships between species extinction and longevity, species richness, origination rates, extinction rates and diversification. These laws are crucial to the study of evolution and ecology.},
   author = {Charles R. Marshall},
   doi = {10.1038/s41559-017-0165},
   issn = {2397334X},
   issue = {6},
   journal = {Nature Ecology and Evolution},
   pages = {1-6},
   pmid = {28812640},
   publisher = {Macmillan Publishers Limited},
   title = {Five palaeobiological laws needed to understand the evolution of the living biota},
   volume = {1},
   url = {http://dx.doi.org/10.1038/s41559-017-0165},
   year = {2017},
}
@article{Maddison2015,
   author = {Wayne P Maddison and Peter E Midford and Sarah P Otto},
   doi = {10.1080/10635150701607033},
   issue = {5},
   pages = {701-710},
   title = {Society of Systematic Biologists Estimating a Binary Character ' s Effect on Speciation and Extinction Estimating a Binary Character ' s Effect on Speciation},
   volume = {56},
   year = {2015},
}
@article{Matzke2015,
   author = {Nicholas J. Matzke},
   journal = {Science},
   pages = {131-139},
   title = {FAQ on “The Evolution of Antievolution Legislation After Kitzmiller v. Dover”},
   volume = {139},
   year = {2015},
}
@book{Harmon,
   author = {Luke Harmon},
   title = {Phylogenetic Comparative Methods},
   url = {https://lukejharmon.github.io/pcm/},
}
@inbook{Franklin2001,
   author = {Alan B Franklin and Tanya M Shenk and David R Anderson and Kenneth P Burnham},
   city = {Washington, D.C.},
   editor = {Tanya M. Shenk and Alan B. Franklin},
   journal = {Modeling in natural resource management: Development, interpretation, and application},
   pages = {75-90},
   publisher = {Island Press},
   title = {Statistical Model Selection: An Alternative to Null Hypothesis Testing},
   year = {2001},
}
@article{Gavryushkina2017,
   abstract = {The total-evidence approach to divergence-time dating uses molecular and morphological data from extant and fossil species to infer phylogenetic relationships, species divergence times, and macroevolutionary parameters in a single coherent framework. Current model-based implementations of this approach lack an appropriate model for the tree describing the diversification and fossilization process and can produce estimates that lead to erroneous conclusions. We address this shortcoming by providing a total-evidence method implemented in a Bayesian framework. This approach uses a mechanistic tree prior to describe the underlying diversification process that generated the tree of extant and fossil taxa. Previous attempts to apply the total-evidence approach have used tree priors that do not account for the possibility that fossil samples may be direct ancestors of other samples. The fossilized birth-death (FBD) process explicitly models the diversification, fossilization, and sampling processes and naturally allows for sampled ancestors. This model was recently applied to estimate divergence times based on molecular data and fossil occurrence dates. We incorporate the FBD model and a model of morphological trait evolution into a Bayesian total-evidence approach to dating species phylogenies. We apply this method to extant and fossil penguins and show that the modern penguins radiated much more recently than has been previously estimated, with the basal divergence in the crown clade occurring at ~12.7 Ma and most splits leading to extant species occurring in the last 2 million years. Our results demonstrate that including stem-fossil diversity can greatly improve the estimates of the divergence times of crown taxa. The method is available in BEAST2 (v. 2.4) www.beast2.org with packages SA (v. at least 1.1.4) and morph-models (v. at least 1.0.4).},
   author = {Alexandra Gavryushkina and Tracy A. Heath and Daniel T. Ksepka and Tanja Stadler and David Welch and Alexei J. Drummond},
   doi = {10.1093/sysbio/syw060},
   isbn = {1063-5157},
   issn = {1076836X},
   issue = {1},
   journal = {Systematic Biology},
   keywords = {Birth-death process,Calibration,Divergence times,MCMC,Phylogenetics},
   pages = {57-73},
   pmid = {27558632},
   title = {Bayesian total-evidence dating reveals the recent crown radiation of penguins},
   volume = {66},
   year = {2017},
}
@article{Pagel1993,
   abstract = {I present a new statistical method for analysing the relationship between two discrete characters that are measured across a group of hierarchically evolved species or populations. The method assesses whether a pattern of association across the group is evidence for correlated evolutionary change in the two characters. The method takes into account information on the lengths of the branches of phylogenetic trees, develops estimates of the rates of change of the discrete characters, and tests the hypothesis of correlated evolution without relying upon reconstructions of the ancestral character states. A likelihood ratio test statistic is used to discriminate between two models that are fitted to the data: one allowing only for independent evolution of the two characters, the other allowing for correlated evolution. Tests of specific directional hypotheses can also be made. The method is illustrated with an application to the Hominoidea.},
   author = {Mark Pagel},
   doi = {10.1098/rspb.1994.0006},
   isbn = {09628452},
   issn = {14712970},
   issue = {1342},
   journal = {Proceedings of the Royal Society: Biological Sciences},
   keywords = {comparative method,phylogeny},
   pages = {37-45},
   pmid = {1695},
   title = {Detecting correlated evolution on phylogenies: a general method for the comparative analysis of discrete characters},
   volume = {255},
   url = {http://rspb.royalsocietypublishing.org/},
   year = {1993},
}
@article{Beaulieu2015,
   abstract = {Hundreds of studies have been dedicated to estimating speciation and extinction from phylogenies of extant species. While it has long been known that estimates of extinction rates using trees of extant organisms are often uncertain, an influential paper by Rabosky (2010) suggested that when birth rates vary continuously across the tree estimates of the extinction fraction (i.e., extinction rate/speciation rate) will appear strongly bimodal, with a peak suggesting no extinction and a peak implying speciation and extinction rates are approaching equality. On the basis of these results, and the realistic nature of this form of rate variation, it is now generally assumed by many practitioners that extinction cannot be understood from molecular phylogenies alone. Here we reevaluated and extended the analyses of Rabosky (2010) and come to the opposite conclusion – namely, that it is possible to estimate extinction from molecular phylogenies, even with model violations due to heritable variation in diversification rate. Note that while it may be tempting to interpret our study as advocating the application of simple birth-death models, our goal here is to show how a particular model violation does not necessitate the abandonment of an entire field: use prudent caution, but do not abandon all hope. Key},
   author = {Jeremy M. Beaulieu and Brian C. O'Meara},
   doi = {10.1111/evo.12614},
   isbn = {0014-3820},
   issn = {15585646},
   issue = {4},
   journal = {Evolution},
   keywords = {Birth-death,Diversification,Extinction,Molecular phylogenies,Speciation},
   pages = {1036-1043},
   pmid = {25639334},
   title = {Extinction can be estimated from moderately sized molecular phylogenies},
   volume = {69},
   year = {2015},
}
@article{Nee2006,
   abstract = {Birth-death models, and their subsets—the pure birth and pure death models—have a long history of use for informing thinking about macroevolutionary patterns. Here we illustrate with examples the wide range of questions they have been used to address, including estimating and comparing rates of diversification of clades, investigating the “shapes” of clades, and some rather surprising uses such as estimating speciation rates from data that are not resolved below the level of the genus. The raw data for inference can be the fossil record or the molecular phylogeny of a clade, and we explore the similarites and differences in the behavior of the birth-death models when applied to these different forms of data.},
   author = {Sean Nee},
   doi = {10.1146/annurev.ecolsys.37.091305.110035},
   isbn = {1543-592X},
   issn = {1543-592X},
   issue = {1},
   journal = {Annual Review of Ecology, Evolution, and Systematics},
   keywords = {fossil record,paleobiology,phylogenetics},
   pages = {1-17},
   title = {Birth-Death Models in Macroevolution},
   volume = {37},
   url = {http://www.annualreviews.org/doi/10.1146/annurev.ecolsys.37.091305.110035},
   year = {2006},
}
@article{Akaike1974,
   author = {Hirotuhu Akaike},
   isbn = {9781139095112},
   issue = {6},
   journal = {IEEE Transactions on Automatic Control},
   pages = {716-723},
   title = {A new look at the statistical model identification},
   volume = {AC-19},
   year = {1974},
}
@article{Matzke2016,
   abstract = {NJM, 0000-0002-8698-7656 Tip-dating methods are becoming popular alternatives to traditional node cali-bration approaches for building time-scaled phylogenetic trees, but questions remain about their application to empirical datasets. We compared the per-formance of the most popular methods against a dated tree of fossil Canidae derived from previously published monographs. Using a canid morphology dataset, we performed tip-dating using BEAST v. 2.1.3 and MRBAYES v. 3.2.5. We find that for key nodes (Canis, approx. 3.2 Ma, Caninae approx. 11.7 Ma) a non-mechanistic model using a uniform tree prior produces estimates that are unrealistically old (27.5, 38.9 Ma). Mechanistic models (incorporating line-age birth, death and sampling rates) estimate ages that are closely in line with prior research. We provide a discussion of these two families of models (mechanistic versus non-mechanistic) and their applicability to fossil datasets.},
   author = {Nicholas J. Matzke and April Wright},
   doi = {10.1098/rsbl.2016.0328},
   issn = {1744957X},
   issue = {8},
   journal = {Biology Letters},
   keywords = {BEASTMASTER,Canidae,MRBAYES,Tip-dating,Total evidence dating,Uniform tree prior},
   pmid = {27512133},
   title = {Inferring node dates from tip dates in fossil Canidae: The importance of tree priors},
   volume = {12},
   year = {2016},
}
@inbook{Drummond2014k,
   author = {Alexei J. Drummond and Andrew Rambaut},
   isbn = {9781139095112},
   journal = {Bayesian Evolutionary Analysis with BEAST},
   pages = {195-206},
   title = {14.1 Basic patterns},
   year = {2014},
}
@article{Joshi2014,
   author = {Girish P. Joshi},
   doi = {10.1097/PRS.0000000000000677},
   isbn = {0000000000000},
   issn = {0032-1052},
   journal = {Plastic and Reconstructive Surgery},
   pages = {94S-100S},
   pmid = {15141531},
   title = {Putting It All Together},
   volume = {134},
   url = {http://content.wkhealth.com/linkback/openurl?sid=WKPTLP:landingpage&an=00006534-201410002-00015},
   year = {2014},
}
@article{Freyman2017,
   abstract = {A major goal of evolutionary biology is to identify key evolutionary transitions that correspond with shifts in speciation and extinction rates. Stochastic character mapping has become the primary method used to infer the timing, nature, and number of character state transitions along the branches of a phylogeny. The method is widely employed for standard substitution models of character evolution. However, current approaches cannot be used for models that specifically test the association of character state transitions with shifts in diversification rates such as state-dependent speciation and extinction (SSE) models. Here we introduce a new stochastic character mapping algorithm that overcomes these limitations, and apply it to study mating system evolution over a densely sampled fossil-calibrated phylogeny of the plant family Onagraceae. Utilizing a hidden state SSE model we tested the association of the loss of self-incompatibility with shifts in diversification rates. Confirming long standing theory, we found that self-compatible lineages have higher extinction rates and lower net diversification rates compared to self-incompatible lineages. Further, our mapped character histories show that the loss of self-incompatibility is followed by a short-term spike in speciation rates, which declines after a time lag of several million years resulting in negative net diversification. Lineages that have long been self-compatible such as Fuchsia and Clarkia are in a previously unrecognized and ongoing evolutionary decline. Our results demonstrate that stochastic character mapping of SSE models is a powerful tool for examining the timing and nature of both character state transitions and shifts in diversification rates over the phylogeny.},
   author = {William Freyman and Sebastian Hoehna},
   doi = {10.1101/210484},
   issue = {2002},
   journal = {bioRxiv},
   pages = {210484},
   title = {Stochastic character mapping of state-dependent diversification reveals the tempo of evolutionary decline in self-compatible lineages},
   url = {https://www.biorxiv.org/content/early/2018/06/22/210484%0Ahttps://www.biorxiv.org/content/early/2017/10/28/210484},
   year = {2017},
}
@inbook{Drummond2014d,
   author = {Alexei J. Drummond and A. Rambaut},
   isbn = {9781139095112},
   journal = {Bayesian Evolutionary Analysis with BEAST},
   pages = {184-194},
   title = {BEAST XML},
   year = {2014},
}
@article{Felsenstein1985,
   author = {Joseph Felsenstein},
   issue = {1},
   journal = {The American Naturalist},
   pages = {1-15},
   title = {Phylogenies and the Comparative Method},
   volume = {125},
   url = {http://www.jstor.org/stable/2461605},
   year = {1985},
}
@inbook{Drummond2018,
   author = {Alexei J. Drummond and Remco R. Bouckaert},
   doi = {https://doi.org/10.1017/CBO9781139095112},
   isbn = {9781139095112},
   issue = {5},
   journal = {International Journal of Radiation Oncology*Biology*Physics},
   pages = {1406-1407},
   title = {Index of Authors},
   volume = {100},
   year = {2018},
}
@inbook{Drummond2014a,
   author = {Alexei J. Drummond and A. Rambaut},
   doi = {10.1099/00221287-16-33-1-51},
   isbn = {9780511487279},
   issn = {1350-0872},
   journal = {Bayesian Evolutionary Analysis with BEAST},
   pages = {300-304},
   title = {Index of subjects},
   year = {2014},
}
@inbook{Drummond2014g,
   author = {Alexei J. Drummond and A. Rambaut},
   doi = {10.1017/CBO9781139095112.009},
   isbn = {9781139095112},
   journal = {Bayesian Evolutionary Analysis with BEAST},
   pages = {116-126},
   title = {Estimating species trees from multilocus data},
   year = {2014},
}
@article{Yan2012,
   author = {Min Yan},
   isbn = {1424404495},
   journal = {Time},
   pages = {6-11},
   title = {Advanced Analysis},
   year = {2012},
}
@article{Drummond2001,
   author = {Alexei J. Drummond and Andrew Rambaut},
   isbn = {9780511819049},
   issue = {May 2017},
   pages = {564-591},
   title = {Bayesian evolutionary analysis by sampling trees THEORY},
   year = {2001},
}
@article{Williams2008a,
   author = {Phil Williams},
   isbn = {9781139095112},
   pages = {9-10},
   title = {Part III},
   year = {2008},
}
@inbook{Drummond2014,
   author = {Alexei J. Drummond and A. Rambaut},
   doi = {10.1017/CBO9781139095112.012},
   isbn = {9781139095112},
   journal = {Bayesian Evolutionary Analysis with BEAST},
   pages = {154-166},
   title = {Exploring phylogenetic tree space},
   year = {2014},
}
@article{The2003,
   author = {T The},
   isbn = {9781139095112},
   pages = {2003-2003},
   title = {Setting Up and Running a Simulation},
   year = {2003},
}
@inbook{Drummond2014f,
   author = {Alexei J. Drummond and A. Rambaut},
   doi = {10.1017/CBO9781139095112.011},
   isbn = {9781139095112},
   journal = {Bayesian Evolutionary Analysis with BEAST},
   pages = {139-153},
   title = {Posterior analysis and post-processing},
   year = {2014},
}
@inbook{Drummond2014h,
   author = {Alexei J. Drummond and A. Rambaut},
   doi = {10.1017/CBO9781139095112.013},
   isbn = {9781139095112},
   journal = {Bayesian Evolutionary Analysis with BEAST},
   pages = {169-183},
   title = {Getting started with BEAST 2},
   year = {2014},
}
@inbook{Drummond2014b,
   author = {Alexei J. Drummond and A. Rambaut},
   doi = {10.1017/CBO9781139095112.003},
   isbn = {9781139095112},
   journal = {Bayesian Evolutionary Analysis with BEAST},
   pages = {21-43},
   title = {Evolutionary trees},
   url = {http://ebooks.cambridge.org/ref/id/CBO9781139095112A016},
   year = {2014},
}
@inbook{Drummond2014m,
   author = {Alexei J. Drummond and Andrew Rambaut and Marc Suchard},
   isbn = {9781139095112},
   journal = {Bayesian Evolutionary Analysis with BEAST},
   pages = {3-20},
   title = {Introduction},
   year = {2014},
}
@inbook{Centre2017,
   author = {A Drummond and Remco R. Bouckaert},
   isbn = {9781139095112},
   journal = {Bayesian Evolutionary Analysis with BEAST},
   pages = {9-10},
   title = {Part I - Theory},
   year = {2015},
}
@inbook{Drummond2014e,
   author = {Alexei J. Drummond and A. Rambaut},
   isbn = {9781139095112},
   journal = {Bayesian Evolutionary Analysis with BEAST},
   pages = {128-129},
   title = {Summary of most significant capabilities of BEAST 2},
   year = {2014},
}
@article{Publications2018,
   author = {Sage Publications and Proquest Ebook Central},
   isbn = {9781139095112},
   pages = {9-10},
   title = {Part Ii},
   year = {2018},
}
@inbook{Drummond2014i,
   author = {Alexei J. Drummond and A. Rambaut},
   doi = {10.1017/CBO9781139095112.006},
   isbn = {9781139095112},
   journal = {Bayesian Evolutionary Analysis with BEAST},
   pages = {68-76},
   title = {Structured trees and phylogeography},
   year = {2014},
}
@article{Rogers2014,
   author = {A.R. Rogers},
   doi = {10.1016/B978-0-08-095975-7.01206-7},
   isbn = {9780080959757},
   journal = {Treatise on Geochemistry},
   pages = {55-61},
   title = {The Molecular Clock},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780080959757012067},
   year = {2014},
}
@inbook{Drummond2014c,
   author = {Alexei J. Drummond and A. Rambaut},
   isbn = {9781139095112},
   issue = {3},
   journal = {Bayesian Evolutionary Analysis with BEAST},
   pages = {2008},
   title = {Chapter 3 Substitutions and site models},
   year = {2014},
}
@article{Das2015,
   author = {Shagnik Das},
   pages = {7},
   title = {A brief note on estimates of binomial coefficients},
   url = {http://page.mi.fu-berlin.de/shagnik/notes/binomials.pdf},
   year = {2015},
}
@misc{Drummond2017,
   author = {Alexei J. Drummond},
   journal = {BIOINF 702 Lecture},
   pages = {1-23},
   title = {Overview of lectures today},
   year = {2017},
}
@article{Grassly1997,
   author = {Nicholas C Grassly and Jun Adachi and Andrew Rambaut},
   issue = {5},
   pages = {559-560},
   title = {Phylogenetic Trees},
   volume = {13},
   year = {1997},
}
@inbook{Drummond2014l,
   author = {Alexei J. Drummond and Andrew Rambaut},
   isbn = {9781139095112},
   journal = {Bayesian Evolutionary Analysis with BEAST},
   title = {Acknowledgements},
   year = {2014},
}
@inbook{Drummond2014j,
   author = {Alexei J. Drummond and Andrew Rambaut},
   isbn = {0521801834},
   journal = {Bayesian Evolutionary Analysis with BEAST},
   title = {Contents},
   year = {2014},
}
@article{Drummond2018,
   author = {Alexei J. Drummond},
   issue = {March},
   title = {Developing Darwin ’ s computer Modeling evolution from the fossil record to infectious disease},
   year = {2018},
}
@article{Drummond2013,
   author = {Alexei J. Drummond and Remco R. Bouckaert},
   isbn = {9781107019652},
   issue = {Mcmc},
   keywords = {Bayesian statistics,Tutorials},
   pages = {1-242},
   title = {Bayesian evolutionary analysis with BEAST 2.0},
   url = {papers3://publication/uuid/3823A21D-9EE4-446A-BF3C-436D972825DB},
   year = {2013},
}
@inbook{Drummond2015,
   author = {Alexei J. Drummond and Remco R. Bouckaert},
   editor = {Alexei J. Drummond and Remco R. Bouckaert},
   isbn = {9781139095112},
   journal = {Bayesian Evolutionary Analysis with BEAST},
   title = {Preface},
   year = {2015},
}
@article{Waldorp2013,
   author = {Lourens Waldorp},
   pages = {1-4},
   title = {Assignment 4},
   year = {2013},
}
@misc{Ogawa,
   author = {Tetsuji Ogawa},
   pages = {1-18},
   title = {Hidden Markov Models},
   url = {file:///Users/masatomiyahara/Downloads/Lec-PRML13-HMM.pdf},
}
@article{Drummond2018a,
   author = {Alexei J. Drummond},
   issue = {April},
   pages = {15-17},
   title = {BIOINF 702 Assignment 2},
   year = {2018},
}
@article{Stirling2018,
   author = {Use Stirling},
   issue = {March},
   pages = {2-3},
   title = {Bioinf 702, 2018},
   year = {2018},
}
@article{Ze,
   author = {Kxkyize Q K Y Ze},
   isbn = {4444444444444},
   pages = {4-5},
   title = {Multiple Sequence Alignment (MSA)},
}
@article{Heled2010,
   abstract = {Until recently, it has been common practice for a phylogenetic analysis to use a single gene sequence from a single individual organism as a proxy for an entire species. With technological advances, it is now becoming more common to collect data sets containing multiple gene loci and multiple individuals per species. These data sets often reveal the need to directly model intraspecies polymorphism and incomplete lineage sorting in phylogenetic estimation procedures. For a single species, coalescent theory is widely used in contemporary population genetics to model intraspecific gene trees. Here, we present a Bayesian Markov chain Monte Carlo method for the multispecies coalescent. Our method coestimates multiple gene trees embedded in a shared species tree along with the effective population size of both extant and ancestral species. The inference is made possible by multilocus data from multiple individuals per species. Using a multiindividual data set and a series of simulations of rapid species radiations, we demonstrate the efficacy of our new method. These simulations give some insight into the behavior of the method as a function of sampled individuals, sampled loci, and sequence length. Finally, we compare our new method to both an existing method (BEST 2.2) with similar goals and the supermatrix (concatenation) method. We demonstrate that both BEST and our method have much better estimation accuracy for species tree topology than concatenation, and our method outperforms BEST in divergence time and population size estimation.},
   author = {Joseph Heled and Alexei J. Drummond},
   doi = {10.1093/molbev/msp274},
   isbn = {0737-4038},
   issn = {07374038},
   issue = {3},
   journal = {Molecular Biology and Evolution},
   keywords = {Bayesian inference,Censored coalescent,Gene trees,Molecular systematics,Multispecies coalescent,Species trees},
   pages = {570-580},
   pmid = {19906793},
   title = {Bayesian Inference of Species Trees from Multilocus Data},
   volume = {27},
   year = {2010},
}
@article{Drummond2005,
   abstract = {We introduce the Bayesian skyline plot, a new method for estimating past population dynamics through time from a sample of molecular sequences without dependence on a prespecified parametric model of demographic history. We describe a Markov chain Monte Carlo sampling procedure that efficiently samples a variant of the generalized skyline plot, given sequence data, and combines these plots to generate a posterior distribution of effective population size through time. We apply the Bayesian skyline plot to simulated data sets and show that it correctly reconstructs demographic history under canonical scenarios. Finally, we compare the Bayesian skyline plot model to previous coalescent approaches by analyzing two real data sets (hepatitis C virus in Egypt and mitochondrial DNA of Beringian bison) that have been previously investigated using alternative coalescent methods. In the bison analysis, we detect a severe but previously unrecognized bottleneck, estimated to have occurred 10,000 radiocarbon years ago, which coincides with both the earliest undisputed record of large numbers of humans in Alaska and the megafaunal extinctions in North America at the beginning of the Holocene.},
   author = {Alexei J. Drummond and A. Rambaut and B. Shapiro and O. G. Pybus},
   doi = {10.1093/molbev/msi103},
   isbn = {0737-4038},
   issn = {07374038},
   issue = {5},
   journal = {Molecular Biology and Evolution},
   keywords = {Coalescent inference,Demographic model selection,Holocene,Markov chain Monte Carlo,Megafaunal extinctions,Skyline plot},
   pages = {1185-1192},
   pmid = {15703244},
   title = {Bayesian coalescent inference of past population dynamics from molecular sequences},
   volume = {22},
   year = {2005},
}
@article{Vaughan2014,
   abstract = {MOTIVATION: Population structure significantly affects evolutionary dynamics. Such structure may be due to spatial segregation, but may also reflect any other gene-flow-limiting aspect of a model. In combination with the structured coalescent, this fact can be used to inform phylogenetic tree reconstruction, as well as to infer parameters such as migration rates and subpopulation sizes from annotated sequence data. However, conducting Bayesian inference under the structured coalescent is impeded by the difficulty of constructing Markov Chain Monte Carlo (MCMC) sampling algorithms (samplers) capable of efficiently exploring the state space.\n\nRESULTS: In this article, we present a new MCMC sampler capable of sampling from posterior distributions over structured trees: timed phylogenetic trees in which lineages are associated with the distinct subpopulation in which they lie. The sampler includes a set of MCMC proposal functions that offer significant mixing improvements over a previously published method. Furthermore, its implementation as a BEAST 2 package ensures maximum flexibility with respect to model and prior specification. We demonstrate the usefulness of this new sampler by using it to infer migration rates and effective population sizes of H3N2 influenza between New Zealand, New York and Hong Kong from publicly available hemagglutinin (HA) gene sequences under the structured coalescent.\n\nAVAILABILITY AND IMPLEMENTATION: The sampler has been implemented as a publicly available BEAST 2 package that is distributed under version 3 of the GNU General Public License at http://compevol.github.io/MultiTypeTree.},
   author = {Timothy G. Vaughan and Denise Kühnert and Alex Popinga and David Welch and Alexei J. Drummond},
   doi = {10.1093/bioinformatics/btu201},
   isbn = {1367-4811 (Electronic)\r1367-4803 (Linking)},
   issn = {14602059},
   issue = {16},
   journal = {Bioinformatics},
   pages = {2272-2279},
   pmid = {24753484},
   title = {Efficient Bayesian inference under the structured coalescent},
   volume = {30},
   year = {2014},
}
@article{Bouckaert2017,
   abstract = {Reconstructing phylogenies through Bayesian methods has many benefits, which include providing a mathematically sound framework, providing realistic estimates of uncertainty and being able to incorporate different sources of information based on formal principles. Bayesian phylogenetic analyses are popular for interpreting nucleotide sequence data, however for such studies one needs to specify a site model and associated substitution model. Often, the parameters of the site model is of no interest and an ad-hoc or additional likelihood based analysis is used to select a single site model.},
   author = {Remco R. Bouckaert and Alexei J. Drummond},
   doi = {10.1186/s12862-017-0890-6},
   isbn = {1471-2148},
   issn = {14712148},
   issue = {1},
   journal = {BMC Evolutionary Biology},
   keywords = {Model averaging,Model comparison,Model selection,ModelTest,Phylogenetic model averaging,Phylogenetic model comparison,Site model,Statistical phylogenetics,Substitution model},
   pages = {1-11},
   pmid = {28166715},
   publisher = {BMC Evolutionary Biology},
   title = {bModelTest: Bayesian phylogenetic site model averaging and model comparison},
   volume = {17},
   year = {2017},
}
@article{Slatkin1989a,
   abstract = {A method for estimating the average level of gene flow among populations is introduced. The method provides an estimate of Nm, where N is the size of each local population in an island model and m is the migration rate. This method depends on knowing the phylogeny of the nonrecombining segments of DNA that are sampled. Given the phylogeny, the geographic location from which each sample is drawn is treated as multistate character with one state for each geographic location. A parsimony criterion applied to the evolution of this character on the phylogeny provides the minimum number of migration events consistent with the phylogeny. Extensive simulations show that the distribution of this minimum number is a simple function of Nm. Assuming the phylogeny is accurately estimated, this method provides an estimate of Nm that is as nearly as accurate as estimates obtained using FST and other statistics when Nm is moderate. Two examples of the use of this method with mitochondrial DNA data are presented.},
   author = {M. Slatkin and W. P. Maddison},
   doi = {D - NLM: PMC1203833 EDAT- 1989/11/01 MHDA- 1989/11/01 00:01 CRDT- 1989/11/01 00:00 PST - ppublish},
   isbn = {0016-6731 (Print)\r0016-6731 (Linking)},
   issn = {00166731},
   issue = {3},
   journal = {Genetics},
   pages = {603-613},
   pmid = {2599370},
   title = {A cladistic measure of gene flow inferred from the phylogenies of alleles},
   volume = {123},
   year = {1989},
}
@article{Pearse2004,
   abstract = {Both the ability to generate DNA data and the variety of analytical methods for conservation genetics are expanding at an ever-increasing pace. Analytical approaches are now possible that were unthinkable even five years ago due to limitations in computational power or the availability of DNA data, and this has vastly expanded the accuracy and types of information that may be gained from population genetic data. Here we provide a guide to recently developed methods for population genetic analysis, including identi- fication of population structure, quantification of gene flow, and inference of demographic history. We cover both allele-frequency and sequence-based approaches, with a special focus on methods relevant to conservation genetic applications. Although classical population genetic approaches such as FST (and its derivatives) have carried the field thus far, newer, more powerful, methods can infer much more from the data, rely on fewer assumptions, and are appropriate for conservation genetic management when precise estimates are needed. Background},
   author = {Devon E Pearse and Keith a Crandall},
   doi = {10.1007/s10592-003-1863-4},
   isbn = {1566-0621},
   issn = {1566-0621},
   journal = {Conservation Genetics},
   pages = {585-602},
   pmid = {141},
   title = {Beyond F ST : Analysis of population genetic data for conservation},
   volume = {5},
   year = {2004},
}
@article{Lyrholm1999,
   author = {Thomas Lyrholm and Olof Leimar and Bo Johanneson and Ulf Gyllensten and Thomas Lyrholm and Olof Leimar and Bo Johanneson and Ulf Gyllensten},
   doi = {10.1098/rspb.1999.0644},
   issue = {October 1998},
   keywords = {1991,1996 a,associating,best 1979,containing aproxi-,dispersal,gyllensten 1998,in apparently matrilineally related,lyrholm,microsatellites,mtdna,population structure,richard et al,social bonds,social organization,sperm whale,units,whitehead et al},
   title = {Sex − biased dispersal in sperm whales : contrasting mitochondrial and nuclear genetic structure of global populations Sex-biased dispersal in sperm whales : contrasting mitochondrial and nuclear genetic structure of global populations},
   year = {1999},
}
@misc{Laverya,
   abstract = {Instruction of Calculating Fst},
   author = {Shane Lavery},
   keywords = {doc -,g,stats2},
   title = {Shane’s Simple Guide to F-statistics},
}
@article{Slatkin1989,
   abstract = {The method of coalescents is used to find the probability that none of the ancestors of alleles sampled from a population are immigrants. If that is the case for samples from two or more populations, then there would be concordance between the phylogenies of those alleles and the geographic locations from which they are drawn. This type of concordance has been found in several studies of mitochondrial DNA from natural populations. It is shown that if the number of sequences sampled from each population is reasonably large (10 or more), then this type of concordance suggests that the average number of individuals migrating between populations is likely to be relatively small (Nm less than 1) but the possibility of occasional migrants cannot be excluded. The method is applied to the data of E. Bermingham and J. C. Avise on mtDNA from the bowfin, Amia calva.},
   author = {M. Slatkin},
   isbn = {0016-6731 (Print)\r0016-6731 (Linking)},
   issn = {00166731},
   issue = {3},
   journal = {Genetics},
   pages = {609-612},
   pmid = {2714639},
   title = {Detecting small amounts of gene flow from phylogenies of alleles},
   volume = {121},
   year = {1989},
}
@article{Platt2018,
   author = {John R Platt},
   issue = {5952},
   journal = {Science},
   pages = {535-538},
   title = {American Association for the Advancement of Science},
   volume = {326},
   year = {2018},
}
@article{Jamieson2006,
   author = {Ig Jamieson and Gp Wallis and Jv Briskie},
   doi = {10.1111/j.1523-1739.2006.00282.x},
   issue = {1},
   journal = {Conservation Biology},
   keywords = {black robin,genetic purging,kakapo,takahe},
   pages = {38-47},
   title = {Inbreeding and endangered species management: is New Zealand out of step with the rest of the world?},
   volume = {20},
   url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1523-1739.2005.00282.x/full},
   year = {2006},
}
@article{Bromham2003,
   abstract = {The discovery of the molecular clock--a relatively constant rate of molecular evolution--provided an insight into the mechanisms of molecular evolution, and created one of the most useful new tools in biology. The unexpected constancy of rate was explained by assuming that most changes to genes are effectively neutral. Theory predicts several sources of variation in the rate of molecular evolution. However, even an approximate clock allows time estimates of events in evolutionary history, which provides a method for testing a wide range of biological hypotheses ranging from the origins of the animal kingdom to the emergence of new viral epidemics.},
   author = {Lindell Bromham and David Penny},
   doi = {10.1038/nrg1020},
   isbn = {1471-0056},
   issn = {14710056},
   issue = {3},
   journal = {Nature Reviews Genetics},
   pages = {216-224},
   pmid = {12610526},
   title = {The modern molecular clock},
   volume = {4},
   year = {2003},
}
@article{Hey2003,
   abstract = {Natural populations, including those of humans, have complex geographies and histories. Studying how they evolve is difficult, but it is possible with population-based DNA sequence data. However, the study of structured populations is divided by two distinct schools of thought and analysis. The phylogeographic approach is fundamentally graphical and begins with a gene-tree estimate. By contrast, the more traditional approach of using summary statistics is fundamentally mathematical. Both approaches have limitations, but there is promise in newer probabilistic methods that offer the flexibility and data exploitation of the phylogeographic approach in an explicitly model-based mathematical framework.},
   author = {Jody Hey and Carlos A. Machado},
   doi = {10.1038/nrg1112},
   isbn = {1471-0056 (Print)\n1471-0056 (Linking)},
   issn = {14710056},
   issue = {7},
   journal = {Nature Reviews Genetics},
   pages = {535-543},
   pmid = {12838345},
   title = {The study of structured populations - New hope for a difficult and divided science},
   volume = {4},
   year = {2003},
}
@article{Baker1993,
   author = {C S Baker and A Perry and J L Bannister and M T Weinrich and R B Abernethy and J Calambokidis and J Lien and R H Lambertsen and J Urban Ramirez and O Vasquez and P J Clapham and A Alling and S J O Brien},
   title = {Abundant Mitochondrial DNA Variation and World-Wide Population Structure in Humpback Whales Palumbi Proceedings of the National Academy of Sciences of the United States of America , Vol . 90 , No .},
   year = {1993},
}
@article{Tallmon2004,
   abstract = {A series of important new theoretical, experimental and observational studies demonstrate that just a few immigrants can have positive immediate impacts on the evolutionary trajectory of local populations. In many cases, a low level of immigration into small populations has produced fitness benefits that are greater than those predicted by theoretical models, resulting in what has been termed 'genetic rescue'. However, the opposite result (reduced fitness) can also be associated with immigration of genetically divergent individuals. Central to our understanding of genetic rescue are complex interactions among fundamental concepts in evolutionary and population biology, including both genetic and non-genetic (environmental, behavioral and demographic) factors. Developing testable models to predict when genetic rescue is likely to occur is a daunting challenge that will require carefully controlled, multi-generation experiments as well as creative use of information from natural 'experiments'.},
   author = {David A. Tallmon and Gordon Luikart and Robin S. Waples},
   doi = {10.1016/j.tree.2004.07.003},
   isbn = {0169-5347 (Print)\n0169-5347 (Linking)},
   issn = {01695347},
   issue = {9},
   journal = {Trends in Ecology and Evolution},
   pages = {489-496},
   pmid = {16701312},
   title = {The alluring simplicity and complex reality of genetic rescue},
   volume = {19},
   year = {2004},
}
@inbook{Avise,
   author = {J. C. Avise},
   journal = {Philosophies and Methods of Molecular Data Analysis},
   title = {Chapter 4 Molecular Clock},
}
@article{Osborne2016,
   abstract = {The New Zealand sea lion (NZSL) is of high conservation concern due to its limited distribution and its declining population size. Historically, it occupied most of coastal New Zealand, but is now restricted to a few coastal sites in southern mainland New Zealand and the sub-Antarctic Islands. NZSLs have experienced a recent reduction in population size due to sealing in the 1900s, which is expected to have resulted in increased inbreeding and a loss of genetic variation, potentially reducing the evolutionary capacity of the species and negatively impacting on its long-term prospects for survival. We used 17 microsatellite loci, previously shown to have cross-species applications in pinnipeds, to determine locus- and population-specific statistics for 1205 NZSLs from 7 consecutive breeding seasons. We show that the NZSL population has a moderate level of genetic diversity in comparison to other pinnipeds. We provide genetic evidence for a population reduction, likely caused by historical sealing, and a measure of allele sharing/parental relatedness (internal relatedness) that is suggestive of increased inbreeding in pups that died during recent epizootic episodes. We hypothesize that population bottlenecks and nonrandom mating have impacted on the population genetic architecture of NZSLs, affecting its population recovery.},
   author = {Amy J. Osborne and Sandra S. Negro and B. Louise Chilvers and Bruce C. Robertson and Martin A. Kennedy and Neil J. Gemmell},
   doi = {10.1093/jhered/esw015},
   isbn = {3037244941},
   issn = {14657333},
   issue = {5},
   journal = {Journal of Heredity},
   keywords = {Bottleneck,Diversity,Microsatellites,Structure},
   pages = {392-402},
   pmid = {26995741},
   title = {Genetic evidence of a population bottleneck and inbreeding in the endangered New Zealand sea lion Phocarctos hookeri},
   volume = {107},
   year = {2016},
}
@inbook{Avise2004,
   author = {J. C. Avise},
   journal = {Molecular Markers, Natural History and Evolution.},
   publisher = {Sinauer},
   title = {Chapter 2 THe History of Interest in Genetic Variation},
   year = {2004},
}
@inbook{Rodrigo2009,
   author = {Allen Rodrigo},
   editor = {Phillipe Lemey and Marco Salemi and Anne-Mieke Vandamme},
   journal = {The Phylogenetic Handbook: A Practical Approach to Phylogenetic Analysis and Hypothesis Testing},
   publisher = {Cambridge University Press},
   title = {The coalescent: population genetic inference using genealogies},
   year = {2009},
}
@misc{Kimura1991,
   abstract = {In sharp contrast to the Darwinian theory of evolution by natural selection, the neutral theory claims that the overwhelming majority of evolutionary changes at the molecular level are caused by random fixation (due to random sampling drift in finite populations) of selectively neutral (i.e., selectively equivalent) mutants under continued inputs of mutations. The theory also asserts that most of the genetic variability within species at the molecular level (such as protein and DNA polymorphism) are selectively neutral or very nearly neutral and that they are maintained in the species by the balance between mutational input and random extinction. The neutral theory is based on simple assumptions, enabling us to develop mathematical theories based on population genetics to treat molecular evolution and variation in quantitative terms. The theory can be tested against actual observations. Neo-Darwinians continue to criticize the neutral theory, but evidence for it has accumulated over the last two decades. The recent outpouring of DNA sequence data has greatly strengthened the theory. In this paper, I review some recent observations that strongly support the neutral theory. They include such topics as pseudoglobin genes of the mouse, alpha A-crystallin genes of the blind mole rat, genes of influenza A virus and nuclear vs. mitochondrial genes of fruit flies. I also discuss such topics as the evolution of deviant coding systems in Mycoplasma, the origin of life and the unified understanding of molecular and phenotypic evolution. I conclude that since the origin of life on Earth, neutral evolutionary changes have predominated over Darwinian evolutionary changes, at least in number.},
   author = {M Kimura},
   doi = {10.1266/jjg.66.367},
   isbn = {0021-504X (Print)\r0021-504X (Linking)},
   issn = {0021-504X},
   issue = {4},
   journal = {Japanese Journal of Genetics},
   pages = {367-386},
   pmid = {1954033},
   title = {The neutral theory of molecular evolution: a review of recent evidence.},
   volume = {66},
   year = {1991},
}
@article{Kimura1968,
   abstract = {Calculating the rate of evolution in terms of nucleotide substitutions seems to give a value so high that many of the mutations involved must be neutral ones.},
   author = {M Kimura},
   doi = {10.1038/217624a0},
   isbn = {0028-0836 (Print)\n0028-0836 (Linking)},
   issn = {00280836},
   issue = {5129},
   journal = {Nature},
   pages = {624-626},
   pmid = {5637732},
   title = {Evolutionary rate at the molecular level.},
   volume = {217},
   year = {1968},
}
@article{Olsen2018,
   author = {Björn Olsen and Vincent J Munster and Anders Wallensten and Jonas Waldenström and Albert D M E Osterhaus and Ron A M Fouchier},
   issue = {5772},
   pages = {384-388},
   title = {American Association for the Advancement of Science},
   volume = {312},
   year = {2018},
}
@article{Patterson2011,
   abstract = {Gene trees will often differ from the true species history, the species tree, as a result of processes such as incomplete lineage sorting. New methods such as Bayesian Estimation of the Species Tree (BEST) use the multispecies coalescent to model lineage sorting, and directly infer the species tree from multilocus DNA sequence data. The Sulidae (Aves: Pelecaniformes) is a family of ten booby and gannet species with a global distribution. We sequenced five nuclear intron loci and one mitochondrial locus to estimate a species tree for the Sulidae using both BEST and by concatenating nuclear loci. We also used fossil calibrated strict and relaxed molecular clocks in BEAST to estimate divergence times for major nodes in the sulid phylogeny. Individual gene trees showed little phylogenetic conflict but varied in resolution. With the exception of the mitochondrial gene tree, no gene tree was completely resolved. On the other hand, both the BEST and concatenated species trees were highly resolved, strongly supported, and topologically consistent with each other. The three sulid genera (Morus, Sula, Papasula) were monophyletic and the relationships within genera were mostly consistent with both a previously estimated mtDNA gene tree and the mtDNA gene tree estimated here. However, our species trees conflicted with the mtDNA gene trees in the relationships among the three genera. Most notably, we find that the endemic and endangered Abbott's booby (Papasula abbotti) is likely basal to all other members of the Sulidae and diverged from them approximately 22. million years ago. © 2010 Elsevier Inc.},
   author = {S. A. Patterson and J. A. Morris-Pocock and V. L. Friesen},
   doi = {10.1016/j.ympev.2010.11.021},
   isbn = {1055-7903},
   issn = {10557903},
   issue = {2},
   journal = {Molecular Phylogenetics and Evolution},
   keywords = {Booby,Gannet,Gene tree,Intron,Species tree,Sulidae},
   pages = {181-191},
   pmid = {21144905},
   publisher = {Elsevier Inc.},
   title = {A multilocus phylogeny of the Sulidae (Aves: Pelecaniformes)},
   volume = {58},
   url = {http://dx.doi.org/10.1016/j.ympev.2010.11.021},
   year = {2011},
}
@article{Fujita2012,
   abstract = {The statistical rigor of species delimitation has increased dramatically over the past decade. Coalescent theory provides powerful models for population genetic inference, and is now increasingly important in phylogenetics and speciation research. By applying probabilistic models, coalescent-based species delimitation provides clear and objective testing of alternative hypotheses of evolutionary independence. As acquisition of multilocus data becomes increasingly automated, coalescent-based species delimitation will improve the discovery, resolution, consistency, and stability of the taxonomy of species. Along with other tools and data types, coalescent-based species delimitation will play an important role in an integrative taxonomy that emphasizes the identification of species limits and the processes that have promoted lineage diversification. © 2012 Elsevier Ltd.},
   author = {Matthew K. Fujita and Adam D. Leaché and Frank T. Burbrink and Jimmy A. McGuire and Craig Moritz},
   doi = {10.1016/j.tree.2012.04.012},
   isbn = {0169-5347},
   issn = {01695347},
   issue = {9},
   journal = {Trends in Ecology and Evolution},
   pages = {480-488},
   pmid = {22633974},
   title = {Coalescent-based species delimitation in an integrative taxonomy},
   volume = {27},
   year = {2012},
}
@article{Edwards2009a,
   abstract = {The advent and maturation of algorithms for estimating species trees—phylogenetic trees that allow gene tree heterogeneity and whose tips represent lineages, populations and species, as opposed to genes—represent an exciting confluence of phylogenetics, phylogeography, and population genetics, and ushers in a new generation of concepts and challenges for the molecular systematist. In this essay I argue that to better deal with the large multilocus datasets brought on by phylogenomics, and to better align the fields of phylogeography and phylogenetics, we should embrace the primacy of species trees, not only as a new and useful practical tool for systematics, but also as a long-standing conceptual goal of systematics that, largely due to the lack of appropriate computational tools, has been eclipsed in the past few decades. I suggest that phylogenies as gene trees are a “local optimum” for systematics, and review recent advances that will bring us to the broader optimum inherent in species trees. In addition to adopting new methods of phylogenetic analysis (and ideally reserving the term “phylogeny” for species trees rather than gene trees), the new paradigm suggests shifts in a number of practices, such as sampling data to maximize not only the number of accumulated sites but also the number of independently segregating genes; routinely using coalescent or other models in computer simulations to allow gene tree heterogeneity; and understanding better the role of concatenation in influencing topologies and confidence in phylogenies. By building on the foundation laid by concepts of gene trees and coalescent theory, and by taking cues from recent trends in multilocus phylogeography, molecular systematics stands to be enriched. Many of the challenges and lessons learned for estimating gene trees will carry over to the challenge of estimating species trees, although adopting the species tree paradigm will clarify many issues (such as the nature of polytomies and the star tree paradox), raise conceptually new challenges, or provide new answers to old questions. [ABSTRACT FROM AUTHOR] Copyright of Evolution is the property of Blackwell Publishing Limited and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
   author = {Scott V. Edwards},
   doi = {10.1111/j.1558-5646.2008.00549.x},
   isbn = {00143820},
   issn = {00143820},
   issue = {1},
   journal = {Evolution},
   keywords = {Fossil,Genome,Macroevolution,Neanderthal,Phylogeography,Polytomy},
   pages = {1-19},
   pmid = {19146594},
   title = {Is a new and general theory of molecular systematics emerging?},
   volume = {63},
   year = {2009},
}
@article{Ayala1986,
   abstract = {"Informational" macromolecules--i.e., proteins and nucleic acids--have in their sequences a register of evolutionary history. Zuckerkandl and Pauling suggested in 1965 that these molecules might provide a "molecular clock" of evolution. The molecular clock would time evolutionary events and make it possible to reconstruct phylogenetic history--the branching relationships among lineages leading to modern species. Kimura's neutrality theory postulates that rates of molecular evolution are stochastically constant and, hence, that there is a molecular clock. A variety of tests have shown that molecular evolution does not behave like a stochastic clock. The variance in evolutionary rates is much too large and thus inconsistent with the neutrality theory. This, however, does not invalidate the clock, but rather leaves it without a theoretical foundation to anticipate its properties. Sequence comparisons show that molecular evolution is sufficiently regular to serve in many situations as a clock, but uncertainty concerning the properties of the clock (for example, about the circumstances that may yield large oscillations in substitution rates from time to time or from lineage to lineage) demands that it be used with caution. Few DNA or protein sequences are known from organisms that range from closely related, e.g., different mammals, to very remote, e.g., mammals and fungi. One example is cytochrome c, which has an acceptable clockwise behavior over the whole span, in spite of some irregularities. Another example is the copper-zinc superoxide dismutase (SOD), which behaves like a very erratic clock. The SOD average rate of amino acid substitution per 100 residues per 100 million years (MY) is 5.5 when fungi and animals are compared, 9.1 when comparisons are made between insects and mammals, and 27.8 when mammals are compared with each other. The question is which mode is more common over broad evolutionary spans: the regularity of cytochrome c or the capriciousness of SOD? Additional data sets will be required in order to obtain the answer and to develop expectations about the accuracy of the clock in particular instances. Until such data exist, conclusions solely based on the molecular clock are potentially fraught with error.},
   author = {Francisco J. Ayala},
   doi = {10.1093/oxfordjournals.jhered.a110227},
   isbn = {0022-1503},
   issn = {00221503},
   issue = {4},
   journal = {Journal of Heredity},
   pages = {226-235},
   pmid = {3020121},
   title = {On the virtues and pitfalls of the molecular evolutionary clock},
   volume = {77},
   year = {1986},
}
@article{Pybus2006,
   abstract = {A brief overview of the methods used to determine phylogenetic distances sets the stage for understanding new research published in PLoS Biology.},
   author = {Oliver G. Pybus},
   doi = {10.1371/journal.pbio.0040151},
   isbn = {1545-7885 (Electronic)},
   issn = {15457885},
   issue = {5},
   journal = {PLoS Biology},
   pages = {686-688},
   pmid = {16683863},
   title = {Model selection and the molecular clock},
   volume = {4},
   year = {2006},
}
@article{Cohen2004,
   abstract = {The article aims to introduce computer scientists to the new field of bioinformatics. This area has arisen from the needs of biologists to utilize and help interpret the vast amounts of data that are constantly being gathered in genomic research--and its more recent counterparts, proteomics and functional genomics. The ultimate goal of bioinformatics is to develop in silico models that will complement in vitro and in vivo biological experiments. The article provides a bird's eye view of the basic concepts in molecular cell biology, outlines the nature of the existing data, and describes the kind of computer algorithms and techniques that are necessary to understand cell behavior. The underlying motivation for many of the bioinformatics approaches is the evolution of organisms and the complexity of working with incomplete and noisy data. The topics covered include: descriptions of the current software especially developed for biologists, computer and mathematical cell models, and areas of computer science that play an important role in bioinformatics.},
   author = {Jacques Cohen},
   doi = {10.1145/1031120.1031122},
   isbn = {0360-0300},
   issn = {03600300},
   issue = {2},
   journal = {ACM Computing Surveys},
   pages = {122-158},
   title = {Bioinformatics---an introduction for computer scientists},
   volume = {36},
   url = {http://portal.acm.org/citation.cfm?doid=1031120.1031122},
   year = {2004},
}
@article{Edwards2009,
   author = {S. V. Edwards},
   doi = {10.1073/pnas.0904103106},
   isbn = {0027-8424\r1091-6490},
   issn = {0027-8424},
   issue = {22},
   journal = {Proceedings of the National Academy of Sciences},
   pages = {8799-8800},
   pmid = {19470454},
   title = {Natural selection and phylogenetic analysis},
   volume = {106},
   url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0904103106},
   year = {2009},
}
@article{Novichkov2004,
   author = {Pavel S Novichkov and Marina V Omelchenko and Mikhail S Gelfand and Andrei a Mironov and Yuri I Wolf and Eugene V Koonin},
   doi = {10.1128/JB.186.19.6575},
   isbn = {0021-9193},
   issn = {0021-9193},
   issue = {19},
   journal = {Journal of Bacteriology},
   pages = {6575-6585},
   pmid = {15375139},
   title = {Genome-wide molecular clock and horizontal gene ransfer in bacterial evolution},
   volume = {186},
   year = {2004},
}
@article{Shao2018,
   abstract = {Eukaryotic genomes are generally organized in multiple chromosomes. Here we have created a functional single-chromosome yeast from a Saccharomyces cerevisiae haploid cell containing sixteen linear chromosomes, by successive end-to-end chromosome fusions and centromere deletions. The fusion of sixteen native linear chromosomes into a single chromosome results in marked changes to the global three-dimensional structure of the chromosome due to the loss of all centromere-associated inter-chromosomal interactions, most telomere-associated inter-chromosomal interactions and 67.4% of intra-chromosomal interactions. However, the single-chromosome and wild-type yeast cells have nearly identical transcriptome and similar phenome profiles. The giant single chromosome can support cell life, although this strain shows reduced growth across environments, competitiveness, gamete production and viability. This synthetic biology study demonstrates an approach to exploration of eukaryote evolution with respect to chromosome structure and function.},
   author = {Yangyang Shao and Ning Lu and Zhenfang Wu and Chen Cai and Shanshan Wang and Ling Li Zhang and Fan Zhou and Shijun Xiao and Lin Liu and Xiaofei Zeng and Huajun Zheng and Chen Yang and Zhihu Zhao and Guoping Zhao and Jin Qiu Zhou and Xiaoli Xue and Zhongjun Qin},
   doi = {10.1038/s41586-018-0382-x},
   issn = {14764687},
   issue = {7718},
   journal = {Nature},
   pages = {331-335},
   pmid = {30069045},
   publisher = {Springer US},
   title = {Creating a functional single-chromosome yeast},
   volume = {560},
   url = {http://dx.doi.org/10.1038/s41586-018-0382-x},
   year = {2018},
}
@article{Palumbi1989,
   abstract = {Selective constraints on DNA sequence change were incorporated into a model of DNA divergence by restricting substitutions to a subset of nucleotide positions. A simple model showed that both mutation rate and the fraction of nucleotide positions free to vary are strong determinants of DNA divergence over time. When divergence between two species approaches the fraction of positions free to vary, standard methods that correct for multiple mutations yield severe underestimates of the number of substitutions per site. A modified method appropriate for use with DNA sequence, restriction site, or thermal renaturation data is derived taking this fraction into account. The model also showed that the ratio of divergence in two gene classes (e.g., nuclear and mitochondrial) may vary widely over time even if the ratio of mutation rates remains constant. DNA sequence divergence data are used increasingly to detect differences in rates of molecular evolution. Often, variation in divergence rate is assumed to represent variation in mutation rate. The present model suggests that differing divergence rates among comparisons (either among gene classes or taxa) should be interpreted cautiously. Differences in the fraction of nucleotide positions free to vary can serve as an important alternative hypothesis to explain differences in DNA divergence rates},
   author = {Stephen R. Palumbi},
   doi = {10.1007/BF02100116},
   issn = {00222844},
   issue = {2},
   journal = {Journal of Molecular Evolution},
   keywords = {Constraints,Divergence,Evolution,Evolutionary distance,Mitochondrial DNA,Molecular evolution,Mutation,Neutral space,Two-parameter model,Variable positions},
   pages = {180-187},
   pmid = {2509718},
   title = {Rates of molecular evolution and the fraction of nucleotide positions free to vary},
   volume = {29},
   year = {1989},
}
@article{Hu2013,
   abstract = {In 1963, Margoliash discovered the unexpected genetic equidistance result after comparing cytochrome c sequences from different species. This finding, together with the hemoglobin analyses of Zuckerkandl and Pauling in 1962, directly inspired the ad hoc molecular clock hypothesis. Unfortunately, however, many biologists have since mistakenly viewed the molecular clock as a genuine reality, which in turn inspired Kimura, King, and Jukes to propose the neutral theory of molecular evolution. Many years of studies have found numerous contradictions to the theory, and few today believe in a universal constant clock. What is being neglected, however, is that the failure of the molecular clock hypothesis has left the original equidistance result an unsolved mystery. In recent years, we fortuitously rediscovered the equidistance result, which remains unknown to nearly all researchers. Incorporating the proven virtues of existing evolutionary theories and introducing the novel concept of maximum genetic diversity, we proposed a more complete hypothesis of evolutionary genetics and reinterpreted the equidistance result and other major evolutionary phenomena. The hypothesis may rewrite molecular phylogeny and population genetics and solve major biomedical problems that challenge the existing framework of evolutionary biology.},
   author = {TaoBo B. Hu and MengPing P. Long and DeJian J. Yuan and ZhuBing B. Zhu and YiMin M. Huang and Shi Huang},
   doi = {10.1007/s11427-013-4452-x},
   isbn = {1142701344},
   issn = {16747305},
   issue = {3},
   journal = {Science China Life Sciences},
   keywords = {evolution,genetic equidistance,macroevolution,maximum genetic diversity hypothesis,microevolution,molecular clock,neutral theory,overlap feature},
   pages = {254-261},
   pmid = {23526392},
   title = {The genetic equidistance result: Misreading by the molecular clock and neutral theory and reinterpretation nearly half of a century later},
   volume = {56},
   year = {2013},
}
@article{Zuckerkandl1965,
   author = {Emile Zuckerkandl and Linus Pauling},
   doi = {10.1209/epl/i1998-00224-x},
   isbn = {0295-5075},
   issn = {02955075},
   journal = {Evolving Genes and Proteins, Academic Press, New York},
   pages = {97-166},
   pmid = {6453},
   title = {Evolutionary divergence and convergence in proteins},
   year = {1965},
}
@article{Reanney1986,
   author = {Darryl C. Reanney},
   issue = {February},
   pages = {41-46},
   title = {genome design},
   year = {1986},
}
@article{Ho2014,
   abstract = {The molecular clock has played an important role in biological research, both as a description of the evolutionary process and as a tool for inferring evolutionary timescales. Genomic data have provided valuable insights into the molecular clock, allowing the patterns and causes of evolutionary rate variation to be characterized in increasing detail. I explain how genome sequences offer exciting opportunities for estimating the timescale of the Tree of Life. I describe the different approaches that have been used to deal with the computational and statistical challenges encountered in molecular clock analyses of genomic data. Finally, I offer a perspective on the future of molecular clocks, highlighting some of the key limitations and the most promising research directions. © 2014 Elsevier Ltd.},
   author = {Simon Y.W. Ho},
   doi = {10.1016/j.tree.2014.07.004},
   isbn = {0169-5347},
   issn = {01695347},
   issue = {9},
   journal = {Trends in Ecology and Evolution},
   keywords = {Genomic data,Molecular clock,Pacemaker models,Phylogenetic analysis,Rate heterogeneity},
   pages = {496-503},
   pmid = {25086668},
   publisher = {Elsevier Ltd},
   title = {The changing face of the molecular evolutionary clock},
   volume = {29},
   url = {http://dx.doi.org/10.1016/j.tree.2014.07.004},
   year = {2014},
}
@article{Gillooly2005,
   abstract = {Observations that rates of molecular evolution vary widely within and among lineages have cast doubts on the existence of a single "molecular clock." Differences in the timing of evolutionary events estimated from genetic and fossil evidence have raised further questions about the accuracy of molecular clocks. Here, we present a model of nucleotide substitution that combines theory on metabolic rate with the now-classic neutral theory of molecular evolution. The model quantitatively predicts rate heterogeneity and may reconcile differences in molecular- and fossil-estimated dates of evolutionary events. Model predictions are supported by extensive data from mitochondrial and nuclear genomes. By accounting for the effects of body size and temperature on metabolic rate, this model explains heterogeneity in rates of nucleotide substitution in different genes, taxa, and thermal environments. This model also suggests that there is indeed a single molecular clock, as originally proposed by Zuckerkandl and Pauling [Zuckerkandl, E. & Pauling, L. (11965) in Evolving Genes and Proteins, eds. Bryson, V. & Vogel, H. J. (Academic, New York), pp. 97-166], but that it "ticks" at a constant substitution rate per unit of mass-specific metabolic energy rather than per unit of time. This model therefore links energy flux and genetic change. More generally, the model suggests that body size and temperature combine to control the overall rate of evolution through their effects on metabolism.},
   author = {J. F. Gillooly and A. P. Allen and G. B. West and J. H. Brown},
   doi = {10.1073/pnas.0407735101},
   isbn = {0027-8424},
   issn = {0027-8424},
   issue = {1},
   journal = {Proceedings of the National Academy of Sciences},
   pages = {140-145},
   pmid = {15618408},
   title = {The rate of DNA evolution: Effects of body size and temperature on the molecular clock},
   volume = {102},
   url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0407735101},
   year = {2005},
}
@article{Powell2004,
   author = {Donald R. Powell},
   pages = {18-21},
   title = {The Scientific Method of Inquiry},
   year = {2004},
}
@article{Morgan1998,
   abstract = {N/A},
   author = {Gregory J. Morgan},
   doi = {10.1023/A:1004394418084},
   isbn = {0022-5010},
   issn = {00225010},
   issue = {2},
   journal = {Journal of the History of Biology},
   pages = {155-178},
   pmid = {11620303},
   title = {Emile Zuckerkandl, Linus Pauling, and the Molecular Evolutionary Clock, 1959-1965},
   volume = {31},
   year = {1998},
}
@article{Luo2018,
   abstract = {Extant species have wildly different numbers of chromosomes, even among taxa with relatively similar genome sizes (for example, insects)1,2. This is likely to reflect accidents of genome history, such as telomere–telomere fusions and genome duplication events3–5. Humans have 23 pairs of chromosomes, whereas other apes have 24. One human chromosome is a fusion product of the ancestral state6. This raises the question: how well can species tolerate a change in chromosome numbers without substantial changes to genome content? Many tools are used in chromosome engineering in Saccharomyces cerevisiae7–10, but CRISPR–Cas9-mediated genome editing facilitates the most aggressive engineering strategies. Here we successfully fused yeast chromosomes using CRISPR–Cas9, generating a near-isogenic series of strains with progressively fewer chromosomes ranging from sixteen to two. A strain carrying only two chromosomes of about six megabases each exhibited modest transcriptomic changes and grew without major defects. When we crossed a sixteen-chromosome strain with strains with fewer chromosomes, we noted two trends. As the number of chromosomes dropped below sixteen, spore viability decreased markedly, reaching less than 10% for twelve chromosomes. As the number of chromosomes decreased further, yeast sporulation was arrested: a cross between a sixteen-chromosome strain and an eight-chromosome strain showed greatly reduced full tetrad formation and less than 1% sporulation, from which no viable spores could be recovered. However, homotypic crosses between pairs of strains with eight, four or two chromosomes produced excellent sporulation and spore viability. These results indicate that eight chromosome–chromosome fusion events suffice to isolate strains reproductively. Overall, budding yeast tolerates a reduction in chromosome number unexpectedly well, providing a striking example of the robustness of genomes to change.},
   author = {Jingchuan Luo and Xiaoji Sun and Brendan P. Cormack and Jef D. Boeke},
   doi = {10.1038/s41586-018-0374-x},
   issn = {14764687},
   issue = {7718},
   journal = {Nature},
   pages = {392-396},
   pmid = {30069047},
   publisher = {Springer US},
   title = {Karyotype engineering by chromosome fusion leads to reproductive isolation in yeast},
   volume = {560},
   url = {http://dx.doi.org/10.1038/s41586-018-0374-x},
   year = {2018},
}
@article{Lee2016,
   abstract = {In the 1960s, several groups of scientists, including Emile Zuckerkandl and Linus Pauling, had noted that proteins experience amino acid replacements at a surprisingly consistent rate across very different species. This presumed single, uniform rate of genetic evolution was subsequently described using the term 'molecular clock'. Biologists quickly realised that such a universal pacemaker could be used as a yardstick for measuring the timescale of evolutionary divergences: estimating the rate of amino acid exchanges per unit of time and applying it to protein differences across a range of organisms would allow deduction of the divergence times of their respective lineages (Figure 1).},
   author = {Michael S.Y. Lee and Simon Y.W. Ho},
   doi = {10.1016/j.cub.2016.03.071},
   isbn = {9780470015902},
   issn = {09609822},
   issue = {10},
   journal = {Current Biology},
   pages = {R399-R402},
   pmid = {27218841},
   publisher = {Elsevier},
   title = {Molecular clocks},
   volume = {26},
   url = {http://dx.doi.org/10.1016/j.cub.2016.03.071},
   year = {2016},
}
@inbook{Witten2011cc,
   abstract = {Index 0 − 1 loss function, 160 0.632 bootstrap, 155 1R (1-rule), 86–90 discretization, 315 example use, 87t learning procedure, 89–90 missing values and numeric data, 87–89 overfitting for, 88 pseudocode, 86f 11-point average recall, 175 A accuracy, of association rules, 72, 73, 116 minimum, 72, 119, 122–123 accuracy, of classification rules, 110, 205 activation function, 241–242 acuity parameter, 281 AD trees. See all-dimensions trees AdaBoost, 358–361 AdaBoostM1 algorithm, 358–359, 475t, 476–477 Add filter, 433t–435t, 436 AddClassification filter, 444t, 445 AddCluster filter, 433t–435t, 436–437 AddExpression filter, 433t–435t, 437 AddID filter, 433t–435t, 436 additive logistic regression, 364–365 additive regression, 362–365 AdditiveRegression algorithm, 475t, 476 AddNoise filter, 433t–435t, 441, 568 AddValues filter, 433t–435t, 438 ADTree algorithm, 446t–450t, 457 adversarial data mining, 393–395 agglomerative clustering, 273, 275–276 Akaike Information Criterion (AIC), 267, 456 algorithms. See specific Weka algorithms all-dimensions (AD) trees, 270–271 generation, 271–272 illustrated examples, 271f alternating decision trees, 366–367 example, 367, 367f prediction nodes, 366–367 splitter nodes, 366–367 Analyze panel, 505–509, 512–515 ancestor-of relation, 46 AND, 233 anomalies, detecting, 334–335 antecedent, of rule, 67, 69 AODE. See averaged one-dependence estimator AODE algorithm, 446t–450t, 451 AODEsr algorithm, 446t–450t, 451 applications, 375–399 automation, 28 challenge of, 375 data stream learning, 380–383 diagnosis, 25–26 fielded, 21–28 incorporating domain knowledge, 384–386 massive datasets, 378–380 message classifier, 531–538 text mining, 386–389 Apriori algorithm, 216 Apriori rule learner, 485–486, 486t default options, 582 output for association rules, 430f parameters, 584 area under the curve (AUC), 177, 580 area under the precision-recall curve (AUPRC), 177 ARFF files, 52–56 attribute specifications in, 54 attribute types in, 54 converting files to, 417–419 defined, 52–56 illustrated, 53f in Weka, 407 arithmetic underflow, 266–267 assignment of key phrases, 387–388 association learning, 40 association rule mining, 582–584 association rules, 11, 72–73. See also rules accuracy (confidence), 72–73, 116 characteristics, 72 computation requirement, 123–124 converting item sets to, 119 coverage (support), 72, 116 double-consequent, 123 examples, 11 finding, 116 finding large item sets, 219–222 frequent-pattern tree, 216–219 mining, 116–124 predicting multiple consequences, 72 relationships between, 73},
   author = {Ian H. Witten and Eibe Frank and Hall and Mark A.},
   doi = {10.1016/B978-0-12-374856-0.00024-9},
   isbn = {0080890369},
   issn = {14337851},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {607-629},
   pmid = {11221713},
   title = {Index},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000249},
   year = {2011},
}
@inbook{Witten2011q,
   author = {Ian H. Witten and Eibe Frank and Mark A. Hall},
   doi = {10.1016/B978-0-12-374856-0.00018-3},
   issue = {Third Edition},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {i-iii},
   title = {Front Matter},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000183},
   year = {2011},
}
@article{Liti2016,
   author = {Gianni Liti},
   doi = {10.1038/nrclinonc.2017.11},
   journal = {Nature},
   pages = {8-9},
   title = {Chromosomes get together},
   year = {2016},
}
@inbook{Witten2011bb,
   author = {Ian H. Witten},
   doi = {10.1016/B978-0-12-374856-0.00025-0},
   isbn = {9780123748560},
   issn = {14337851},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {iv},
   title = {Copyright},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000250},
   year = {2011},
}
@article{Kaufmann2011,
   author = {Morgan Kaufmann},
   doi = {10.1016/B978-0-12-374856-0.00023-7},
   isbn = {0080890369},
   issn = {14337851},
   issue = {2006},
   pages = {767-772},
   pmid = {11221713},
   title = {Abe, N., Zadrozny, B., & Langford, J. (2006). Outlier detection by active learning. In},
   year = {2011},
}
@inbook{Witten2011z,
   author = {Witten Ian H.},
   doi = {10.1016/B978-0-12-374856-0.00020-1},
   editor = {Witten Ian H. and Eibe Frank},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {xix-xx},
   title = {List of Tables},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000201},
   year = {2011},
}
@inbook{Witten2011d,
   author = {Ian H. Witten and Eibe Frank and Mark A. Hall},
   doi = {10.1016/B978-0-12-374856-0.00009-2},
   isbn = {9780123748560},
   issn = {14337851},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {375-399},
   pmid = {11221713},
   title = {Moving on},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000092},
   year = {2011},
}
@inbook{Witten2011l,
   author = {Ian H. Witten and Eibe Frank and Mark A. Hall},
   doi = {10.1016/B978-0-12-374856-0.00004-3},
   isbn = {0080890369},
   issn = {14337851},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {85-145},
   pmid = {11221713},
   title = {Algorithms},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000043},
   year = {2011},
}
@inbook{Witten2011aa,
   author = {Ian H. Witten},
   doi = {10.1016/B978-0-12-374856-0.00021-3},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {xxi-xxvii},
   title = {Preface},
   year = {2011},
}
@inbook{Larose2011,
   author = {Daniel T. Larose and Chantal D. Larose},
   doi = {10.1016/B978-0-12-374856-0.00019-5},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {xv-xviii},
   title = {List of Figures},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000195},
   year = {2011},
}
@inbook{Witten2011m,
   abstract = {Most of the techniques in this book produce easily comprehensible descriptions of the structural patterns in the data. Before looking at how these techniques work, we have to see how structural patterns can be expressed. There are many different ways for representing the patterns that can be discovered by machine learning, and each one dictates the kind of technique that can be used to infer that output structure from data. Once you understand how the output is represented, you have come a long way toward understanding how it can be generated. We saw many examples of data mining in Chapter 1. In these cases the output took the form of decision trees and classification rules, which are basic knowledge representation styles that many machine learning methods use. Knowledge is really too imposing a word for a decision tree or a collection of rules, and by using it we don't mean to imply that these structures vie with the real kind of knowledge that we carry in our heads—it's just that we need some word to refer to the structures that learning methods produce. There are more complex varieties of rules that allow exceptions to be specified, and ones that can express relations among the values of the attributes of different instances. Some problems have a numeric class, and—as mentioned in Chapter 1—the classic way of dealing with these is to use linear models. Linear models can also be adapted to deal with binary classification. More-over, special forms of trees can be developed for numeric prediction. Instance-based representations focus on the instances themselves rather than rules that govern their attribute values. Finally, some learning schemes generate clusters of instances. These different knowledge representation methods parallel the different kinds of learning problems introduced in Chapter 2. 3.1 TABLES The simplest, most rudimentary way of representing the output from machine learn-ing is to make it just the same as the input—a table. For example, Table 1.2 is a decision table for the weather data: You just look up the appropriate conditions to decide whether or not to play. Exactly the same process can be used for numeric prediction too—in this case, the structure is sometimes referred to as a regression table. Less trivially, creating a decision or regression table might involve selecting},
   author = {Ian H. Witten and Eibe Frank and Mark A. Hall},
   doi = {10.1016/B978-0-12-374856-0.00003-1},
   isbn = {9780123748560},
   issn = {14337851},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {61-83},
   title = {Output},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000031},
   year = {2011},
}
@inbook{Witten2011c,
   author = {Ian H. Witten and Eibe Frank and Mark A. Hall},
   doi = {10.1016/B978-0-12-374856-0.00002-X},
   isbn = {0080890369},
   issn = {14337851},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {39-60},
   pmid = {11221713},
   title = {Input},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B978012374856000002X},
   year = {2011},
}
@inbook{Witten2011,
   abstract = {FIGURE 17.1   The data viewer.      FIGURE 17.2   Output after building and testing the classifier: (a) screenshot and (b) decision tree.      FIGURE 17.3   The decision tree that has been built.      Table 17.1   Accuracy Obtained Using  IBk , for ...},
   author = {Ian H. Witten and Eibe Frank and Mark A. Hall},
   doi = {10.1016/B978-0-12-374856-0.00017-1},
   isbn = {978-0-12-374856-0},
   issn = {14337851},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {559-585},
   pmid = {11221713},
   title = {Tutorial Exercises for the Weka Explorer},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000171},
   year = {2011},
}
@inbook{Witten2011n,
   author = {Ian H. Witten and Eibe Frank and Mark A. Hall},
   doi = {10.1016/B978-0-12-374856-0.00008-0},
   isbn = {9780123748560},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {351-373},
   title = {Ensemble Learning},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000080},
   year = {2011},
}
@inbook{Witten2011f,
   abstract = {Analysis of Ecological Communities offers a rationale and guidance for selecting appropriate, effective, analytical methods in community ecology. The book is suitable as a textbook and reference book on methods for multivariate analysis of ecological communities and their environments. The book covers distance measures, data transformation, outlier analysis, coordination, cluster analysis, PCA RA, CA, DCA, NMS, NMS, CCA, Bray-Curtis, MRPP, Mantel test, discriminant analysis, twinspan, classification and regression trees, structural equation modeling, and more. It also includes brief treatments of community sampling and diversity measures. The 304 page book is richly illustrated. It provides many examples from the literature and demonstrations of basic principles with simulated and real data sets.},
   author = {Ian H. Witten and Eibe Frank and Mark A. Hall},
   doi = {10.1016/B978-0-12-374856-0.00007-9},
   isbn = {9780123748560},
   issn = {14337851},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {305-349},
   pmid = {8215535},
   title = {Data Transformations},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000079},
   year = {2011},
}
@inbook{Witten2011g,
   abstract = {Evaluation is the key to making real progress in data mining. There are lots of ways of inferring structure from data: We have encountered many already and will see further refinements, and new methods, in Chapter 6. However, in order to determine which ones to use on a particular problem we need systematic ways to evaluate how different methods work and to compare one with another. But evaluation is not as simple as it might appear at first sight. What's the problem? We have the training set; surely we can just look at how well different methods do on that. Well, no: As we will see very shortly, performance on the training set is definitely not a good indicator of performance on an indepen-dent test set. We need ways of predicting performance bounds in practice, based on experiments with whatever data can be obtained. When a vast supply of data is available, this is no problem: Just make a model based on a large training set, and try it out on another large test set. But although data mining sometimes involves " big data " —particularly in marketing, sales, and customer support applications—it is often the case that data, quality data, is scarce. The oil slicks mentioned in Chapter 1 (page 23) had to be detected and marked manually—a skilled and labor-intensive process—before being used as training data. Even in the personal loan application data (page 22), there turned out to be only 1000 training examples of the appropriate type. The electricity supply data (page 24) went back 15 years, 5000 days—but only 15 Christmas days and Thanks-givings, and just four February 29s and presidential elections. The electromechanical diagnosis application (page 25) was able to capitalize on 20 years of recorded experience, but this yielded only 300 usable examples of faults. The marketing and sales applications (page 26) certainly involve big data, but many others do not: Training data frequently relies on specialist human expertise—and that is always in short supply. The question of predicting performance based on limited data is an interesting, and still controversial, one. We will encounter many different techniques, of which one—repeated cross-validation—is probably the method of choice in most practical limited-data situations. Comparing the performance of different machine learning schemes on a given problem is another matter that is not as easy as it sounds: To be sure that apparent differences are not caused by chance effects, statistical tests are needed. 148 CHAPTER Credibility: Evaluating What's Been Learned So far we have tacitly assumed that what is being predicted is the ability to clas-sify test instances accurately; however, some situations involve predicting class probabilities rather than the classes themselves, and others involve predicting numeric rather than nominal values. Different methods are needed in each case. Then we look at the question of cost. In most practical data mining situations, the cost of a misclassification error depends on the type of error it is—whether, for example, a positive example was erroneously classified as negative or vice versa. When doing data mining, and evaluating its performance, it is often essential to take these costs into account. Fortunately, there are simple techniques to make most learning schemes cost sensitive without grappling with the algorithm's internals. Finally, the whole notion of evaluation has fascinating philosophical connections. For 2000 years, philosophers have debated the question of how to evaluate scientific theories, and the issues are brought into sharp focus by data mining because what is extracted is essentially a " theory " of the data.},
   author = {Ian H. Witten and Eibe Frank and Mark A. Hall},
   doi = {10.1016/B978-0-12-374856-0.00005-5},
   isbn = {9780123748560},
   issn = {14337851},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {147-187},
   pmid = {11221713},
   title = {Credibility},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000055},
   year = {2011},
}
@inbook{Witten2011b,
   abstract = {With the Knowledge Flow interface, users select Weka components from a tool bar, place them on a layout canvas, and connect them into a directed graph that processes and analyzes data. It provides an alternative to the Explorer for those who like think-ing in terms of how data flows through the system. It also allows the design and execution of configurations for streamed data processing, which the Explorer cannot do. You invoke the Knowledge Flow interface by selecting KnowledgeFlow from the choices in the right panel shown in Figure 11.3(a). 12.1 GETTING STARTED Here is a step-by-step example that loads an ARFF file and performs a cross-validation using J4.8. We describe how to build up the final configuration shown in Figure 12.1. First, create a source of data by clicking on the DataSources tab (left-most entry in the bar at the top) and selecting ARFFLoader from the toolbar. The mouse cursor changes to crosshairs to signal that you should next place the compo-nent. Do this by clicking anywhere on the canvas, whereupon a copy of the ARFF loader icon appears there. To connect it to an ARFF file, right-click it to bring up the pop-up menu shown in Figure 12.2(a). Click Configure to get the file browser in Figure 12.2(b), from which you select the desired ARFF file (alternatively, double-clicking on a component's icon is a short-cut for selecting Configure from the pop-up menu). Now we specify which attribute is the class using a ClassAssigner object. This is on the Evaluation panel, so click the Evaluation tab, select the ClassAssigner, and place it on the canvas. To connect the data source to the class assigner, right-click the data source icon and select dataSet from the menu, as shown in Figure 12.2(a). A rubber-band line appears. Move the mouse over the class assigner com-ponent and left-click. A red line labeled dataSet appears, joining the two compo-nents. Having connected the class assigner, choose the class by right-clicking it, selecting Configure, and entering the location of the class attribute. We will perform cross-validation on the J48 classifier. In the data flow model, we first connect the CrossValidationFoldMaker to create the folds on which the classifier will run, and then pass its output to an object representing J48.},
   author = {Ian H. Witten and Eibe Frank and Mark A. Hall},
   doi = {10.1016/B978-0-12-374856-0.00012-2},
   isbn = {9780123748560},
   issn = {14337851},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {495-503},
   pmid = {11221713},
   title = {The Knowledge Flow Interface},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000122},
   year = {2011},
}
@inbook{Witten2011i,
   author = {Ian H. Witten and Eibe Frank and Mark A. Hall},
   doi = {10.1016/B978-0-12-374856-0.00006-7},
   isbn = {0080890369},
   issn = {14337851},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {191-304},
   pmid = {11221713},
   title = {Implementations},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000067},
   year = {2011},
}
@inbook{Witten2011k,
   abstract = {Weka's main graphical user interface, the Explorer, gives access to all its facilities using menu selection and form filling. It is illustrated in Figure 11.1. There are six different panels, selected by the tabs at the top, corresponding to the various data mining tasks that Weka supports. 11.1 GETTING STARTED Suppose you have some data and you want to build a decision tree from it. First, you need to prepare the data, then fire up the Explorer and load it in. Next, you select a decision tree construction method, build a tree, and interpret the output. It's easy to do it again with a different tree construction algorithm or a different evalu-ation method. In the Explorer you can flip back and forth between the results you have obtained, evaluate the models that have been built on different datasets, and visualize graphically both the models and the datasets themselves, including any classification errors the models make. Preparing the Data The data is often presented in a spreadsheet or database. However, Weka's native data storage method is the ARFF format (see Section 2.4, page 52). You can easily convert from a spreadsheet to ARFF. The bulk of an ARFF file consists of a list of the instances, and the attribute values for each instance are separated by commas (see Figure 2.2). Most spreadsheet and database programs allow you to export data into a file in comma-separated value (CSV) format as a list of records with commas between items. Having done this, you need only load the file into a text editor or word processor; add the dataset's name using the @relation tag, the attribute infor-mation using @attribute, and an @data line; then save the file as raw text. For example, Figure 11.2 shows an Excel spreadsheet containing the weather data from Section 1.2 (page 9), the data in CSV form loaded into Microsoft Word, and the result of converting it manually into an ARFF file. However, you don't actually have to go through these steps to create the ARFF file yourself because the Explorer can read CSV spreadsheet files directly, as described later.},
   author = {Ian H. Witten and Eibe Frank and Mark A. Hall},
   doi = {10.1016/B978-0-12-374856-0.00011-0},
   isbn = {9780123748560},
   issn = {14337851},
   issue = {page 9},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {407-494},
   pmid = {11221713},
   title = {The Explorer},
   volume = {2},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000110},
   year = {2011},
}
@inbook{Witten2011p,
   abstract = {Technology now allows us to capture and store vast quantities of data. Finding patterns, trends, and anomalies in these datasets, and summarizing them with simple quantitative models, is one of the grand challenges of the infor- mation age—turning data into information and turning information into knowledge. There has been stunning progress in data mining and machine learning. The synthesis of statistics, machine learning, information theory, and computing has created a solid science, with a firm mathematical base, and with very powerful tools.Witten and Frank present much of this progress in this book and in the companion implementation of the key algorithms. As such, this is a milestone in the synthesis of data mining, data analysis, information theory, and machine learning. If you have not been following this field for the last decade, this is a great way to catch up on this exciting progress. If you have, then Witten and Frank’s presentation and the companion open-source workbench, called Weka, will be a useful addition to your toolkit. They present the basic theory of automatically extracting models from data, and then validating those models. The book does an excellent job of explaining the various models (decision trees, association rules, linear models, clustering, Bayes nets, neural nets) and how to apply them in practice.With this basis, they then walk through the steps and pitfalls of various approaches. They describe how to safely scrub datasets, how to build models, and how to evaluate a model’s predictive quality.Most of the book is tutorial, but Part II broadly describes how commercial systems work and gives a tour of the publicly available data mining workbench that the authors provide through a website. This Weka workbench has a graphical user interface that leads you through data mining tasks and has excellent data visualization tools that help understand the models. It is a great companion to the text and a useful and popular tool in its own right. This book presents this new discipline in a very accessible form: as a text both to train the next generation of practitioners and researchers and to inform lifelong learners like myself. Witten and Frank have a passion for simple and elegant solutions. They approach each topic with this mindset, grounding all concepts in concrete examples, and urging the reader to consider the simple techniques first, and then progress to the more sophisticated ones if the simple ones prove inadequate. If you are interested in databases, and have not been following the machine learning field, this book is a great way to catch up on this exciting progress. If you have data that you want to analyze and understand, this book and the asso- ciated Weka toolkit are an excellent way to start.},
   author = {Ian H. Witten and Eibe Frank and Mark A. Hall},
   doi = {10.1016/B978-0-12-374856-0.00010-9},
   isbn = {9780123748560},
   issn = {14337851},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {403-406},
   pmid = {11221713},
   title = {Introduction to Weka},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000109},
   year = {2011},
}
@inbook{Witten2011h,
   abstract = {"Data Mining: Practical Machine Learning Tools and Techniques" offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. This highly anticipated third edition of the most acclaimed work on data mining and machine learning will teach you everything you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining. Thorough updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including new material on Data Transformations, Ensemble Learning, Massive Data Sets, Multi-instance Learning, plus a new version of the popular Weka machine learning software developed by the authors. Witten, Frank, and Hall include both tried-and-true techniques of today as well as methods at the leading edge of contemporary research. Provides a thorough grounding in machine learning concepts as well as practical advice on applying the tools and techniques to your data mining projects Offers concrete tips and techniques for performance improvement that work by transforming the input or output in machine learning methods Includes downloadable Weka software toolkit, a collection of machine learning algorithms for data mining tasks-in an updated, interactive interface. Algorithms in toolkit cover: data pre-processing, classification, regression, clustering, association rules, visualization},
   author = {Ian H. Witten and Eibe Frank and Mark A. Hall},
   doi = {10.1016/B978-0-12-374856-0.00001-8},
   isbn = {0080890369},
   issn = {14337851},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {3-38},
   pmid = {11221713},
   title = {What's It All About?},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000018},
   year = {2011},
}
@inbook{Witten2011r,
   abstract = {Suppose you need to implement a special-purpose learning algorithm that is not included in Weka. Or suppose you are engaged in machine learning research and want to investigate a new learning scheme. Or suppose you just want to learn more about the inner workings of an induction algorithm by actually programming it yourself. This section uses a simple example to show how to make full use of Weka's class hierarchy when writing classifiers. Weka includes the elementary learning schemes listed in Table 16.1, mainly for educational purposes. None take any scheme-specific command-line options. They are all useful for understanding the inner workings of a classifier. As an example, we describe the weka.classifiers.trees.Id3 scheme, which implements the ID3 deci-sion tree learner from Section 4.3 (page 99). Other schemes, such as clustering algorithms and association rule learners, are organized in a similar manner. 16.1 AN EXAMPLE CLASSIFIER Figure 16.1 gives the source code of weka.classifiers.trees.Id3, which extends the Classifier class, as you can see from what is shown in the eight-page figure that follows the next page. Every classifier in Weka does so, whether it predicts a nominal class or a numeric one. It also implements two interfaces, Technical­ InformationHandler and Sourcable, which, respectively, allow the implementing class to provide bibliographical references for display in Weka's graphical user interface and a source code representation of its learned model. The first method in weka.classifiers.trees.Id3 is globalInfo(): We mention it here before moving on to the more interesting parts. It simply returns a string that is displayed in Weka's graphical user interface when this scheme is selected. Part of the string includes information generated by the second method, getTech­ nicalInformation(), which formats a bibliographic reference for the ID3 algorithm. The third method, getCapabilities(), returns information on the data characteristics that Id3 can handle, namely nominal attributes and a nominal class—and the fact that it can deal with missing class values and data that contains no instances (although the latter does not produce a useful model!). Capabilities are described in Section 16.2.},
   author = {Ian H. Witten and Eibe Frank and Mark A. Hall},
   doi = {10.1016/B978-0-12-374856-0.00016-X},
   isbn = {9780123748560},
   issn = {14337851},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {539-557},
   pmid = {11221713},
   title = {Writing New Learning Schemes},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B978012374856000016X},
   year = {2011},
}
@inbook{Witten2011o,
   author = {Ian H. Witten and Eibe Frank and Mark A. Hall},
   doi = {10.1016/B978-0-12-374856-0.00015-8},
   isbn = {9780123748560},
   issn = {14337851},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {531-538},
   pmid = {11221713},
   title = {Embedded Machine Learning},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000158},
   year = {2011},
}
@inbook{Witten2011a,
   author = {Ian H. Witten and Eibe Frank and Mark A. Hall},
   doi = {10.1016/B978-0-12-374856-0.00014-6},
   isbn = {9780123748560},
   issn = {14337851},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {519-529},
   pmid = {11221713},
   title = {The Command-Line Interface},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000146},
   year = {2011},
}
@inbook{Witten2011e,
   author = {Ian H. Witten and Eibe Frank and Mark A. Hall},
   doi = {10.1016/B978-0-12-374856-0.00013-4},
   isbn = {9780123748560},
   issn = {14337851},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {505-517},
   pmid = {11221713},
   title = {The Experimenter},
   volume = {1},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000134},
   year = {2011},
}
@article{Soleymani2017k,
   author = {Mehdi Soleymani},
   title = {Stats 707},
   year = {2017},
}
@book{Kuhn2013,
   abstract = {Applied Predictive Modeling covers the overall predictive modeling process, beginning with the crucial steps of data preprocessing, data splitting and foundations of model tuning.  The text then provides intuitive explanations of numerous common and modern regression and classification techniques, always with an emphasis on illustrating and solving real data problems.  Addressing practical concerns extends beyond model fitting to topics such as handling class imbalance, selecting predictors, and pinpointing causes of poor model performance―all of which are problems that occur frequently in practice.
 
The text illustrates all parts of the modeling process through many hands-on, real-life examples.  And every chapter contains extensive R code for each step of the process.  The data sets and corresponding code are available in the book's companion AppliedPredictiveModeling R package, which is freely available on the CRAN archive.
 
This multi-purpose text can be used as an introduction to predictive models and the overall modeling process, a practitioner's reference handbook, or as a text for advanced undergraduate or graduate level predictive modeling courses.  To that end, each chapter contains problem sets to help solidify the covered concepts and uses data available in the book's R package.
 
Readers and students interested in implementing the methods should have some basic knowledge of R.  And a handful of the more advanced topics require some mathematical knowledge.},
   author = {Max Kuhn and Kjell Johnson},
   doi = {10.1007/978-1-4614-6849-3},
   isbn = {1461468485},
   issn = {03781119},
   pages = {620},
   pmid = {17629633},
   title = {Applied Predictive Modeling [Hardcover]},
   url = {http://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485/ref=pd_bxgy_b_img_z},
   year = {2013},
}
@inbook{Cunningham2011,
   author = {Jo Cunningham and Matt Humphrey and Lyn Hunt and Bob Mcqueen and Lloyd Smith and Stuart Inglis and Craig Nevill-manning},
   doi = {10.1016/B978-0-12-374856-0.00022-5},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {xxix-xxxi},
   title = {Acknowledgments},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000225},
   year = {2011},
}
@inbook{Witten2011j,
   author = {Ian H. Witten and Eibe Frank and Mark A. Hall},
   doi = {10.1016/B978-0-12-374856-0.00026-2},
   edition = {Third Edit},
   issue = {1999},
   journal = {Data Mining: Practical Machine Learning Tools and Techniques},
   pages = {xxxiii},
   publisher = {Elsevier Inc.},
   title = {About the Authors},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000262},
   year = {2011},
}
@article{Soleymani2017m,
   author = {Mehdi Soleymani},
   title = {Stats 707},
   year = {2017},
}
@article{Soleymani2017o,
   author = {Mehdi Soleymani},
   title = {Stats 707},
   year = {2017},
}
@article{Soleymani2017f,
   author = {Mehdi Soleymani},
   title = {Stats 707},
   year = {2017},
}
@article{Soleymani2017p,
   author = {Mehdi Soleymani},
   title = {Stats 707},
   year = {2017},
}
@article{Soleymani2017j,
   author = {Mehdi Soleymani},
   title = {Stats 707},
   year = {2017},
}
@article{Soleymani2017h,
   author = {Mehdi Soleymani},
   pages = {1-47},
   title = {Stats 707},
   year = {2017},
}
@article{Soleymani2017c,
   author = {Mehdi Soleymani},
   title = {Warm-up exercise},
   year = {2017},
}
@article{Soleymani2017q,
   author = {Mehdi Soleymani},
   title = {Stats 707},
   year = {2017},
}
@article{Soleymani2017i,
   author = {Mehdi Soleymani},
   title = {Stats 707},
   year = {2017},
}
@article{Soleymani2017d,
   author = {Mehdi Soleymani},
   title = {Recall ( testing hypotheses )},
   year = {2017},
}
@article{Soleymani2017,
   author = {Mehdi Soleymani},
   title = {Stats 707},
   year = {2017},
}
@article{Soleymani2017e,
   author = {Mehdi Soleymani},
   title = {Stats 707},
   year = {2017},
}
@article{Soleymani2017a,
   author = {Mehdi Soleymani},
   title = {Stats 707},
   year = {2017},
}
@article{Soleymani2017n,
   author = {Mehdi Soleymani},
   title = {Stats 707},
   year = {2017},
}
@article{Soleymani2017l,
   author = {Mehdi Soleymani},
   title = {Stats 707},
   year = {2017},
}
@article{Soleymani2017g,
   author = {Mehdi Soleymani},
   pages = {1-16},
   title = {Stats 707},
   year = {2017},
}
@misc{Unay2011,
   author = {S D G Unay and H B Kavanoz},
   doi = {10.1142/S0217979211101806},
   isbn = {0217979211},
   issue = {c},
   keywords = {bredig transition,molecular dynamics simulation,mopysical properties,ther-,uranium dioxide},
   pages = {3211-3223},
   title = {Assignment 3 Instructions},
   volume = {25},
   year = {2011},
}
@article{Soleymani2017b,
   author = {Mehdi Soleymani},
   title = {Stats 707},
   year = {2017},
}
@inbook{Ruggiero,
   author = {Kathy Ruggiero},
   isbn = {9781139022095},
   title = {Appendix A - A brief introduction to R},
}
@article{Anderson2001,
   abstract = {The most appropriate strategy to be used to create a permutation distribution for tests of individual terms in complex experimental designs is currently unclear. There are often many possibilities, including restricted permutation or permutation of some form of residuals. This paper provides a summary of recent empirical and theoretical results concerning available methods and gives recommendations for their use in univariate and multivariate applications. The focus of the paper is on complex designs in analysis of variance and multiple regression (i.e., linear models). The assumption of exchangeability required for a permutation test is assured by random allocation of treatments to units in experimental work. For observational data, exchangeability is tantamount to the assumption of independent and identically distributed errors under a null hypothesis. For partial regression, the method of permutation of residuals under a reduced model has been shown to provide the best test. For analysis of variance, o..., La strat\{é\}gie la plus appropri\{é\}e pour g\{é\}n\{é\}rer une distribution de permutation en vue de tester les termes individuels d'un plan exp\{é\}rimental complexe n'est pas \{é\}vidente \{à\} l'heure actuelle. Il y a souvent plusieurs options, dont la permutation restreinte et la permutation d'une quelconque forme des r\{é\}siduels. On trouvera ici un r\{é\}sum\{é\} d'informations r\{é\}centes empiriques et th\{é\}oriques sur les m\{é\}thodes disponibles, ainsi que des recommandations pour leur utilisation dans des applications unidimensionnelles et multidimensionnelles. L'emphase est mise sur les plans complexes d'analyse de variance et de r\{é\}gression multiple (i.e. les mod\{è\}les lin\{é\}aires). Dans un travail exp\{é\}rimental, la supposition d'\{é\}changeabilit\{é\} requise pour un test par permutation est assur\{é\}e par l'assignation au hasard \{à\} des unit\{é\}s des divers traitements. Dans le cas d'observations, l'\{é\}changeabilit\{é\} \{é\}quivaut \{à\} supposer que les erreurs, dans une hypoth\{è\}se nulle, sont ind\{é\}pendantes et distribu\{é\}es de fa\{ç\}on identique. Pour la r\{é\}gression partielle,...},
   author = {Marti J Anderson},
   doi = {10.1139/f01-004},
   isbn = {0706-652X},
   issn = {0706-652X},
   issue = {3},
   journal = {Canadian Journal of Fisheries and Aquatic Sciences},
   pages = {626-639},
   pmid = {5684003},
   title = {Permutation tests for univariate or multivariate analysis of variance and regression},
   volume = {58},
   url = {http://www.nrcresearchpress.com/doi/abs/10.1139/f01-004},
   year = {2001},
}
@article{Solutions2000,
   author = {Partial Solutions},
   issue = {c},
   keywords = {f_miss,togram of data},
   pages = {4000},
   title = {Histogram of data [, 1 ]},
   volume = {5},
   year = {2000},
}
@article{Ruggiero2016,
   author = {Kathy Ruggiero},
   issue = {April},
   title = {Week 5:},
   year = {2016},
}
@article{Ruggiero2018a,
   author = {Kathy Ruggiero},
   issue = {April},
   title = {Week 6 : Resampling procedures},
   year = {2018},
}
@article{Ruggiero2018b,
   author = {Kathy Ruggiero},
   issue = {April},
   title = {Week 4 : More on design and analysis of experiments : Last of the three principles of design ; Split-block designs},
   year = {2018},
}
@article{Park2008,
   author = {Jungkyu Park},
   issue = {June},
   pages = {2018},
   title = {Assignment # 5},
   volume = {2018},
   year = {2008},
}
@article{Clarke1997,
   author = {G M Clarke and R E Kempson},
   issue = {March},
   title = {Introduction to the Design and Analysis of Experiments},
   year = {1997},
}
@article{Moore2004,
   abstract = {The continuing revolution in computing is having a dramatic influence on statistics. Exploratory analysis of data becomes easier as graphs and calcula- tions are automated. Statistical study of very large and very complex data sets becomes feasible. Another impact of fast and cheap computing is less obvious: new methods that apply previously unthinkable amounts of computation to small sets of data to produce confidence intervals and tests of significance in settings that don’t meet the conditions for safe application of the usual meth- ods of inference.},
   author = {David S Moore and George P McCabe and Bruce Craig and Tim Hesterberg and Shaun Monaghan and Ashley Clipson and Rachel Epstein},
   doi = {10.1016/j.fishres.2006.11.017},
   isbn = {978-0716766544},
   issn = {0040-1706},
   journal = {Introduction to the Practice of Statistics},
   pages = {1-70},
   pmid = {21401589},
   title = {Bootstrap Methods and Permutation Tests (Chapter 14)},
   url = {papers2://publication/uuid/861C080D-C170-4EBA-BCDD-07A1DEDE1AA6},
   year = {2004},
}
@article{Ruggiero2018d,
   author = {Kathy Ruggiero},
   issue = {March},
   title = {Errors in hypothesis testing ; The multiple testing problem ; Factorial experiments},
   year = {2018},
}
@article{Ruggiero2018c,
   author = {Kathy Ruggiero},
   issue = {March},
   title = {Week 2 More on the analysis of CRDs ; Errors in hypothesis testing ; The},
   year = {2018},
}
@article{Lennerfors2018,
   author = {Thomas Lennerfors},
   issue = {May},
   pages = {4-6},
   title = {Assignment 4},
   year = {2018},
}
@article{Physics2013,
   author = {Atomic Physics and I I Prof and Wolfgang Ketterle Spring},
   issue = {May},
   pages = {1-6},
   title = {Assignment # 3},
   volume = {010},
   year = {2013},
}
@article{First2017,
   author = {Read M E First and New Zealand and Plant Vigour},
   issue = {March},
   pages = {1-7},
   title = {Workshop 2 Read the data into R},
   volume = {1},
   year = {2017},
}
@article{Rankin2016,
   author = {Hallie Rankin},
   doi = {10.1017/CBO9781107415324.004},
   isbn = {9202122232425},
   issn = {1098-6596},
   issue = {March},
   pages = {2016},
   pmid = {25246403},
   title = {Assignment 1},
   year = {2016},
}
@article{Ruggiero2018,
   author = {Kathy Ruggiero},
   issue = {January},
   title = {BIOSCI 738 - Assignment 1 Model Answers},
   volume = {1},
   year = {2018},
}
@article{Russell2018,
   author = {James Russell and Western Indian Ocean},
   issue = {May 2016},
   pages = {65-71},
   title = {Workshop 6 The study},
   volume = {2018},
   year = {2018},
}
@article{Pritchard2010,
   abstract = {We describe a model-based clustering method for using multilocus genotype data to infer population structure and assign individuals to populations. We assume a model in which there are K populations (where K may be unknown), each of which is characterized by a set of allele frequencies at each locus. Individuals in the sample are assigned (probabilistically) to populations, or jointly to two or more populations if their genotypes indicate that they are admixed. Our model does not assume a particular mutation process, and it can be applied to most of the commonly used genetic markers, provided that they are not closely linked. Applications of our method include demonstrating the presence of population structure, assigning individuals to populations, studying hybrid zones, and identifying migrants and admired individuals. We show that the method can produce highly accurate assignments using modest numbers of loci-e.g., seven microsatellite loci in an example using genotype data from an endangered bird species. The software used for this article is available from http://www.stats.ox.ac.uk/similar to pritch/home.html},
   author = {Jonathan K Pritchard and X Wen and Daniel Falush},
   doi = {10.1002/spe.4380060305},
   isbn = {<null>},
   issn = {00380644},
   issue = {3},
   journal = {University of Chicago. IL},
   pages = {321-326},
   pmid = {21565030},
   title = {Documentation for structure software : Version 2 . 3},
   volume = {6},
   url = {http://pritch.bsd.uchicago.edu/software/readme_structure2.pdf},
   year = {2010},
}
@misc{Meyer2017,
   abstract = {Meyer, K. (2006). WOMBAT– Digging deep for quantitative genetic analyses by restricted maximum likelihood. Proc. 8thWorld Congr. Genet. Appl. Livest. Prod., Communication No. 27–14. Meyer, K. (2006). WOMBAT – A program for mixed model analyses by restricted maximum likelihood. User notes. Animal Genetics and Breeding Unit, Armidale, npp. All},
   author = {Karin Meyer},
   keywords = {Chromosome,Covariance components,DNA content,Estimation,Gene Expression Profiling,Gene Expression Profiling: methods,Genes,Genes: physiology,Mixed Model,Nucleic Acids,Nucleic Acids: isolation & purification,Quality Control,REML,Reference Standards,Reproducibility of Results,Research Design,Research Design: standards,Reverse Transcriptase Polymerase Chain Reaction,Reverse Transcriptase Polymerase Chain Reaction: s,Software,Statistics as Topic,Workflow,flow cytometry,genome size,grasspea,karyotype,lathyrus sativus},
   title = {WOMBAT A program for Mixed Model Analyses by Restricted Maximum Likelihood - User Notes},
   year = {2017},
}
@article{June2013,
   author = {M June},
   pages = {1-4},
   title = {‘Single-step’ genetic evaluation in WOMBAT 1},
   year = {2013},
}
@article{Model2013,
   author = {Conceptual Model},
   issue = {April},
   pages = {1-9},
   title = {Workshop # 2},
   year = {2013},
}
@misc{Lecturea,
   author = {BIOINF 703 Lecture},
   pages = {1-8},
   title = {Hardy-Weinberg Equilibrium},
}
@article{August2012,
   author = {M August},
   pages = {1-4},
   title = {Estimating “ social ” genetic e ff ects using WOMBAT},
   year = {2012},
}
@inbook{April2012,
   author = {M April},
   pages = {1-13},
   title = {Pooling estimates of covariance components by penalized maximum likelihood using WOMBAT Background : The likelihood approach},
   year = {2012},
}
@misc{M2011,
   author = {Karrin M},
   isbn = {1211001011011},
   pages = {1-3},
   title = {‘Automatic’ GWAS analyses using WOMBAT},
   year = {2011},
}
@article{Reticulation2005,
   author = {Minimum Reticulation},
   pages = {1-7},
   title = {Phylogenetic networks II : algorithmic aspects The Minimum Hybridisation problem},
   year = {2005},
}
@article{Browning2011,
   abstract = {Determination of haplotype phase is becoming increasingly important as we enter the era of large-scale sequencing because many of its applications, such as imputing low-frequency variants and characterizing the relationship between genetic variation and disease susceptibility, are particularly relevant to sequence data. Haplotype phase can be generated through laboratory-based experimental methods, or it can be estimated using computational approaches. We assess the haplotype phasing methods that are available, focusing in particular on statistical methods, and we discuss the practical aspects of their application. We also describe recent developments that may transform this field, particularly the use of identity-by-descent for computational phasing.},
   author = {Sharon R. Browning and Brian L. Browning},
   doi = {10.1038/nrg3054},
   isbn = {1471-0064 (Electronic)\r1471-0056 (Linking)},
   issn = {14710056},
   issue = {10},
   journal = {Nature Reviews Genetics},
   pages = {703-714},
   pmid = {21921926},
   title = {Haplotype phasing: Existing methods and new developments},
   volume = {12},
   year = {2011},
}
@article{Lecture,
   author = {BIOINF 703 Lecture},
   issue = {2},
   pages = {1-3},
   title = {1 Mathematical},
   volume = {1},
}
@inbook{Kitching1998,
   author = {Ian J. Kitching and Peter L. Forey and Christopher J. Humphries and David M. Williams},
   journal = {Cladistics: the theory and practice of parsimony analysis.},
   pages = {139-150},
   title = {Consensus trees},
   year = {1998},
}
@article{Price2006,
   abstract = {Population stratification--allele frequency differences between cases and controls due to systematic ancestry differences-can cause spurious associations in disease studies. We describe a method that enables explicit detection and correction of population stratification on a genome-wide scale. Our method uses principal components analysis to explicitly model ancestry differences between cases and controls. The resulting correction is specific to a candidate marker's variation in frequency across ancestral populations, minimizing spurious associations while maximizing power to detect true associations. Our simple, efficient approach can easily be applied to disease studies with hundreds of thousands of markers.},
   author = {Alkes L. Price and Nick J. Patterson and Robert M. Plenge and Michael E. Weinblatt and Nancy A. Shadick and David Reich},
   doi = {10.1038/ng1847},
   isbn = {1061-4036 (Print)\r1061-4036 (Linking)},
   issn = {10614036},
   issue = {8},
   journal = {Nature Genetics},
   pages = {904-909},
   pmid = {16862161},
   title = {Principal components analysis corrects for stratification in genome-wide association studies},
   volume = {38},
   year = {2006},
}
@article{Pickrell2016,
   abstract = {We performed a genome-wide scan for genetic variants that influence multiple human phenotypes by comparing large genome-wide association studies (GWAS) of 40 traits or diseases, including anthropometric traits (e.g. nose size and male pattern baldness), immune traits (e.g. susceptibility to childhood ear infections and Crohn's disease), metabolic phenotypes (e.g. type 2 diabetes and lipid levels), and psychiatric diseases (e.g. schizophrenia and Parkinson's disease). First, we identified 307 loci (at a false discovery rate of 10%) that influence multiple traits (excluding “trivial” phenotype pairs like type 2 diabetes and fasting glucose). Several loci influence a large number of phenotypes; for example, variants near the blood group gene ABO influence eleven of these traits, including risk of childhood ear infections (rs635634: log-odds ratio = 0.06, P = 1.4 × 10−8) and allergies (log-odds ratio = 0.05, P = 2.5 × 10−8), among others. Similarly, a nonsynonymous variant in the zinc transporter SLC39A8 influences seven of these traits, including risk of schizophrenia (rs13107325: log-odds ratio = 0.15, P = 2 × 10−12) and Parkinson’s disease (log-odds ratio = -0.15, P = 1.6 × 10−7), among others. Second, we used these loci to identify traits that share multiple genetic causes in common. For example, genetic variants that delay age of menarche in women also, on average, delay age of voice drop in men, decrease body mass index (BMI), increase adult height, and decrease risk of male pattern baldness. Finally, we identified four pairs of traits that show evidence of a causal relationship. For example, we show evidence that increased BMI causally increases triglyceride levels, and that increased liability to hypothyroidism causally decreases adult height.},
   author = {Joseph K. Pickrell and Tomaz Berisa and Jimmy Z. Liu and Laure Ségurel and Joyce Y. Tung and David A. Hinds},
   doi = {10.1038/ng.3570},
   isbn = {9780128000977},
   issn = {15461718},
   issue = {7},
   journal = {Nature Genetics},
   pages = {709-717},
   pmid = {27182965},
   publisher = {Nature Publishing Group},
   title = {Detection and interpretation of shared genetic influences on 42 human traits},
   volume = {48},
   url = {http://dx.doi.org/10.1038/ng.3570},
   year = {2016},
}
@article{Slatkin2008,
   abstract = {Linkage disequilibrium--the nonrandom association of alleles at different loci--is a sensitive indicator of the population genetic forces that structure a genome. Because of the explosive growth of methods for assessing genetic variation at a fine scale, evolutionary biologists and human geneticists are increasingly exploiting linkage disequilibrium in order to understand past evolutionary and demographic events, to map genes that are associated with quantitative characters and inherited diseases, and to understand the joint evolution of linked sets of genes. This article introduces linkage disequilibrium, reviews the population genetic processes that affect it and describes some of its uses. At present, linkage disequilibrium is used much more extensively in the study of humans than in non-humans, but that is changing as technological advances make extensive genomic studies feasible in other species.},
   author = {Montgomery Slatkin},
   doi = {10.1038/nrg2361},
   isbn = {1471-0064 (Electronic)\r1471-0056 (Linking)},
   issn = {14710056},
   issue = {6},
   journal = {Nature Reviews Genetics},
   pages = {477-485},
   pmid = {18427557},
   title = {Linkage disequilibrium - Understanding the evolutionary past and mapping the medical future},
   volume = {9},
   year = {2008},
}
@article{Umu2016,
   abstract = {Many genes carry information for making proteins. To make a protein, a working copy of the information stored in DNA is first copied into a molecule of messenger RNA. These RNA messages are then interpreted by the ribosome, the molecular machine that makes proteins. Many messages are produced from each gene, and each message can be read multiple times. Thus, it should follow that the number of messages produced dictates the number of proteins made. However, this is not the case and the number of proteins produced cannot be completely predicted from knowing the number of messenger RNAs. Cells control how much of a given protein they produce through interactions between the messenger RNAs and other regulatory RNAs. The regulatory RNAs bind directly to a message and impede protein production. Because there are millions of RNAs in a cell, these interactions have evolved to be highly specific. Nevertheless, it seems inevitable that messenger RNAs would encounter other RNAs too, which could short-circuit gene regulation and lead to less protein being produced. Umu et al. have now asked if such short-circuit events are selected against during evolution. Computational tools were used to predict the strength of binding between the RNAs found in the dominant forms of microbial life on Earth: the bacteria and the archaea. This approach revealed that the majority of messenger RNAs bind more weakly to the most common RNA molecules found in cells than would be expected by chance. Weakened binding should prevent the RNA molecules from becoming tangled with each other and ensure that protein levels are not perturbed by unintended interactions between highly expressed messages and other RNAs. To test this hypothesis further, Umu et al. generated versions of the gene for a green fluorescent protein that differed only in how well their messenger RNAs could avoid interacting with the most abundant RNAs in E. coli cells. Those messengers that were designed to avoid interacting with other RNAs yielded far more protein than those that were not. The findings show that taking this kind of avoidance into account can improve predictions about how much protein will be produced and should therefore make it easier to control protein production in experimental systems. Finally, the messenger RNAs of some bacteria do not show such clear avoidance. However, these bacteria have a more complex internal cell structure. This finding hints at an alternative means for avoiding short-circuiting events that could be used by more complicated cells, such of those of animals and plants, which also contain much larger numbers of RNAs.},
   author = {Sinan Uğur Umu and Anthony M. Poole and Renwick C.J. Dobson and Paul P. Gardner},
   doi = {10.7554/eLife.13479},
   isbn = {1044071060},
   issn = {2050084X},
   issue = {September},
   journal = {eLife},
   pages = {3-5},
   pmid = {27642845},
   title = {Avoidance of stochastic RNA interactions can be harnessed to control protein expression levels in bacteria and archaea},
   volume = {5},
   year = {2016},
}
@article{Drummond,
   author = {Alexei J. Drummond},
   title = {Genome editing to the fittest / healthiest},
}
@book{IlluminaInc.2015,
   author = {Illumina Inc.},
   doi = {# TG-450-9001DOC Material # 20000923 Document # 15049528 v01},
   isbn = {1000000001},
   issue = {September},
   pages = {36},
   title = {ForenSeq™ DNA Signature Prep Reference Guide},
   year = {2015},
}
@article{IlluminaInc.2016,
   author = {Illumina Inc.},
   doi = {FC-121-9006DOC},
   issue = {January},
   journal = {Sample Preparation Guide},
   pages = {1-28},
   title = {Nextera® XT Library Prep Reference Guide},
   url = {http://support.illumina.com/downloads/nextera_xt_sample_preparation_guide_15031942.html},
   year = {2016},
}
@article{Shane2016,
   author = {Coordinator Shane and Steffen Klaere and Shane Lavery},
   issue = {2004},
   title = {Neutral theory and the molecular clock},
   volume = {2016},
   year = {2016},
}
@misc{RStudio2015,
   author = {R Studio},
   doi = {10.1016/S0097-8493(02)00051-1},
   isbn = {9780470258866},
   issn = {00978493},
   title = {GGplot2 cheat sheet},
   url = {https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf%5Cnhttps://www.rstudio.com/wp-content/uploads/2015/06/ggplot2-french.pdf},
   year = {2015},
}
@article{Stekhoven2012,
   abstract = {Modern data acquisition based on high-throughput technology is often facing the problem of missing data. Algorithms commonly used in the analysis of such large-scale data often depend on a complete set. Missing value imputation offers a solution to this problem. However, the majority of available imputation methods are restricted to one type of variable only: continuous or categorical. For mixed-type data the different types are usually handled separately. Therefore, these methods ignore possible relations between variable types. We propose a nonparametric method which can cope with different types of variables simultaneously. We compare several state of the art methods for the imputation of missing values. We propose and evaluate an iterative imputation method (missForest) based on a random forest. By averaging over many unpruned classification or regression trees random forest intrinsically constitutes a multiple imputation scheme. Using the built-in out-of-bag error estimates of random forest we are able to estimate the imputation error without the need of a test set. Evaluation is performed on multiple data sets coming from a diverse selection of biological fields with artificially introduced missing values ranging from 10% to 30%. We show that missForest can successfully handle missing values, particularly in data sets including different types of variables. In our comparative study missForest outperforms other methods of imputation especially in data settings where complex interactions and nonlinear relations are suspected. The out-of-bag imputation error estimates of missForest prove to be adequate in all settings. Additionally, missForest exhibits attractive computational efficiency and can cope with high-dimensional data.},
   author = {Daniel J. Stekhoven and Peter Bühlmann},
   doi = {10.1093/bioinformatics/btr597},
   isbn = {1367-4811 (Electronic)\n1367-4803 (Linking)},
   issn = {13674803},
   issue = {1},
   journal = {Bioinformatics},
   pages = {112-118},
   pmid = {22039212},
   title = {Missforest-Non-parametric missing value imputation for mixed-type data},
   volume = {28},
   year = {2012},
}
@article{Albani2017,
   author = {Patricia Pearl Albani},
   title = {Enhancement of mRNA-Based Methods for Body Fluid and Celltype Identification},
   year = {2017},
}
@article{Stekhoven2011,
   author = {Daniel J Stekhoven},
   pages = {1-11},
   title = {Using the missForest Package},
   year = {2011},
}
@article{Lee2017d,
   author = {Alan Lee},
   pages = {1-45},
   title = {Lecture 9 : Non-linear Classification Rules Outline Introduction},
   year = {2017},
}
@article{Sheet2016,
   author = {Cheat Sheet},
   title = {R Markdown : : CHEAT SHEET},
   volume = {5},
   year = {2016},
}
@article{Lee2017g,
   author = {Alan Lee},
   pages = {1-30},
   title = {Lecture 8 : Linear methods for classification Outline Introduction},
   year = {2017},
}
@article{Buuren2011,
   abstract = {The R package mice imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice 2.9, which extends the functionality of mice 1.0 in several ways. In mice 2.9, the analysis of imputed data is made completely general, whereas the range ofmodels under which pooling works is substantially extended. mice 2.9 adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. mice 2.9 can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.},
   author = {Stef van Buuren and Karin Groothuis-Oudshoorn},
   doi = {10.18637/jss.v045.i03},
   isbn = {9067436771},
   issn = {1548-7660},
   issue = {3},
   journal = {Journal of Statistical Software},
   keywords = {chained equations,fully conditional specification,gibbs sampler,mice,multiple imputation,passive imputation,predictor selection,r},
   pmid = {22289957},
   title = {MICE : Multivariate Imputation by Chained Equations in R},
   volume = {45},
   url = {http://www.jstatsoft.org/v45/i03/},
   year = {2011},
}
@article{Lee2017c,
   author = {Alan Lee},
   pages = {1-20},
   title = {Lecture 7 : Data Preprocessing and Feature Engineering Outline Introduction},
   year = {2017},
}
@article{Talia2016,
   abstract = {Abstract We introduce in this chapter the main concepts of data mining. This scientific field, together with Cloud computing, discussed in Chapter 2, is a basic pillar on which the contents of this book are built. Section 1.1 explores the main notions and principles of data mining introducing readers to this scientific field and giving them the needed information on sequential data mining techniques and algorithms that will be used in other sections and chapters of this book. Section 1.2 outlines the most important parallel and distributed data mining strategies and techniques. },
   author = {Domenico Talia and Paolo Trunfio and Fabrizio Marozzo},
   doi = {http://dx.doi.org/10.1016/B978-0-12-802881-0.00001-9},
   isbn = {978-0-12-802881-0},
   issue = {July},
   journal = {Data Analysis in the Cloud},
   keywords = {association rules,classification,clustering,collective data mining,data mining,distributed data mining,ensemble learning,meta-learning,parallel data mining},
   pages = {1-25},
   title = {Chapter 1 - Introduction to Data Mining},
   url = {http://www.sciencedirect.com/science/article/pii/B9780128028810000019},
   year = {2016},
}
@article{Lee2017f,
   author = {Alan Lee},
   pages = {1-26},
   title = {Lecture 6 : Boosting and Bagging Outline Introduction},
   year = {2017},
}
@article{Hofner2012,
   abstract = {Data 19 503 observations of the following covariates: 1. damage–response variable, no= healthy, yes= damaged tree. 2. year–ordered factor with levels 1991 and 2002. 3. species– factor with four levels (beech, fir, rowan, sycamore). 4. height–one-dimensional ... \n},
   author = {Benjamin Hofner and Andreas Mayr and Nikolay Robinzonov and Matthias Schmid},
   doi = {10.1007/s00180-012-0382-5.The},
   isbn = {0018001203825},
   issn = {0943-4062},
   journal = {Imbe.Med.Uni-Erlangen.De},
   pages = {1-26},
   title = {Model-based boosting in R},
   url = {http://www.imbe.med.uni-erlangen.de/ma/M.Schmid/Tutorial/06_casestudy2.pdf},
   year = {2012},
}
@article{Kuhn2008,
   abstract = {The caret package, short for classification and regression training, contains numerous tools for developing predictive models using the rich set of models available in R. The package focuses on simplifying model training and tuning across a wide variety of modeling techniques. It also includes methods for pre-processing training data, calculating variable importance, and model visualizations. An example from computational chemistry is used to illustrate the functionality on a real data set and to benchmark the benefits of parallel processing with several types of models.},
   author = {Max Kuhn},
   issn = {15487660},
   issue = {5},
   journal = {Journal Of Statistical Software},
   keywords = {model building,networkspaces,parallel processing,r,tuning parameters},
   pages = {1–26},
   title = {caret Package},
   volume = {28},
   url = {http://www.jstatsoft.org/v28/i05/paper},
   year = {2008},
}
@article{Lee2017h,
   author = {Alan Lee},
   pages = {1-27},
   title = {Lecture 11 : Imputation Outline Introduction Iris data},
   year = {2017},
}
@article{Lee2017e,
   author = {Alan Lee},
   pages = {1-34},
   title = {Lecture 1 : What is Data Mining ? Outline Housekeeping},
   year = {2017},
}
@article{Lee2017,
   author = {Alan Lee},
   pages = {1-28},
   title = {Lecture 10 : Regularization Outline Introduction Lasso},
   year = {2017},
}
@book{Lee2017a,
   author = {Alan Lee},
   isbn = {7900875794},
   pages = {1-37},
   title = {Lecture 5 : Non-Linear Prediction Methods ( Cont ) Outline Introduction},
   year = {2017},
}
@article{Lee2017b,
   author = {Alan Lee},
   pages = {1-30},
   title = {Lecture 4 : Non-Linear Prediction Methods Outline Introduction Smoothing Models},
   year = {2017},
}
@article{Lee2017i,
   author = {Alan Lee},
   pages = {1-34},
   title = {Lecture 3 : Linear methods for prediction ( cont ) Outline Introduction Cross-validation},
   year = {2017},
}
@article{Lee2017j,
   author = {Alan Lee},
   pages = {1-23},
   title = {Lecture 2 : Linear methods for prediction Outline Introduction Linear predictors Prediction error},
   year = {2017},
}
@article{DepartmentofStatisticsUOA2017,
   author = {Department of Statistics UOA},
   issue = {1},
   pages = {6-7},
   title = {STATS 784 Statistical Data Mining Assignment 4},
   year = {2017},
}
@article{Rathjen2013,
   author = {Philipp Rathjen},
   isbn = {9210125150},
   issue = {x},
   pages = {3002},
   title = {Question 1 Question 1 Question 2},
   year = {2013},
}
@article{Rizki2018,
   author = {Muhammad Agus Rizki},
   issue = {March},
   pages = {1-8},
   title = {Question 1},
   volume = {580},
   year = {2018},
}
@article{Mining2015,
   author = {Statistical Data Mining},
   pages = {1-10},
   title = {STATS 784 SC Terms Test Instructions : Surname : First name :},
   year = {2015},
}
@article{County2017,
   author = {King County and Washington State},
   pages = {1-2},
   title = {Department of Statistics STATS 784 : Data Mining Consult the references in Lecture 1 and identify and describe two applications of data The dataset for this question contains house sale prices for King County , Washington function train in the caret packa},
   year = {2017},
}
@book{Hastie2009,
   abstract = {During the past decade there has been an explosion in computation and information tech-nology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting—the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression & path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for " wide " data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.},
   author = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
   doi = {10.1007/978-0-387-84858-7},
   isbn = {978-0-387-84857-0},
   issn = {0964-1998},
   journal = {Elements of Statistical Learning},
   keywords = {',+,.->ZXCaszxdcfvgbhnm,.-gfvvbvfvvvbnbnnmhgbvxcc,.-mm<asaassasa<zxasaAASDFGHJCVGFDFDFGHJKJJJBNMKLÆN,.0441,0,q11+p´´+pååå897eawer5689+>aA<,¨'pæh4441æh542266455577747451010774778749988,øøASDFGHJNM},
   pages = {+0,525258258525252787787474*/==741æø' m,.mnbvcx12¨},
   pmid = {15512507},
   title = {The Elements of Statistical Learning},
   url = {https://link.springer.com/content/pdf/10.1007%2F978-0-387-84858-7.pdf%0Ahttp://link.springer.com/10.1007/978-0-387-84858-7},
   year = {2009},
}
@article{Taurus2009,
   author = {Bos Taurus},
   doi = {10.1016/B978-0-12-373580-5.50043-0},
   isbn = {9781617790959},
   pages = {187-208},
   title = {Chapter 9; Neural Networks},
   volume = {7},
   year = {2009},
}
@inbook{Larose2014b,
   author = {Daniel T. Larose and Chantal D. Larose},
   edition = {Second},
   isbn = {9780471687535},
   journal = {Discovering Knowledge in Data: An Introduction to Data Mining},
   title = {Index},
   year = {2014},
}
@inbook{Larose,
   author = {Daniel T. Larose},
   title = {Preface; KNOWLEDGE IN DATA WILEY SERIES ON METHODS AND APPLICATIONS},
}
@article{Venables2010,
   abstract = {This chapter provides a background to research on Northern krill biology, starting with a description of its morphology and identifying features, and the historical path to its eventual position as a single-species genus. There is a lack of any euphausiid fossil material, so phylogenetic analysis has relied on comparative morphology and ontogeny and, more recently, genetic methods. Although details differ, the consensus of these approaches is that Meganyctiphanes is most closely related to the genus Thysanoessa. The light organs (or photophores) are well developed in Northern krill and the control of luminescence in these organs is described. A consideration of the distribution of the species shows that it principally occupies shelf and slope waters of both the western and eastern coasts of the North Atlantic, with a southern limit at the boundary with sub-tropical waters (plus parts of the Mediterranean) and a northern limit at the boundary with Arctic water masses. Recent evidence of a northward expansion of these distributional limits is considered further. There have been a variety of techniques used to sample and survey Northern krill populations for a variety of purposes, which this chapter collates and assesses in terms of their effectiveness. Northern krill play an important ecological role, both as a contributor to the carbon pump through the transport of faecal material to the deeper layers, and as a key prey item for groundfish, squid, baleen whales, and seabirds. The commercial exploitation of Northern krill has been slow to emerge since its potential was considered by Mauchline [Mauchline, J (1980). The biology of mysids and euphausiids. Adv. Mar. Biol. 18, 1-681]. However, new uses for products derived from krill are currently being found, which may lead to a new wave of exploitation. © 2010 Elsevier Ltd.},
   author = {W.N. Venables and D.M. Smith},
   doi = {10.1016/B978-0-12-381308-4.00001-7},
   isbn = {9780123813084},
   issn = {00652881},
   issue = {C},
   journal = {Advances in Marine Biology},
   pages = {1-40},
   pmid = {20955887},
   title = {An Introduction to R},
   volume = {57},
   year = {2010},
}
@article{DeJonge2013,
   abstract = {Data cleaning, or data preparation is an essential part of statistical analysis. In fact, in practice it is often more time-consuming than the statistical analysis itself. These lecture notes describe a range of techniques, implemented in the R statistical environment, that allow the reader to build data cleaning scripts for data suffering from a wide range of errors and inconsistencies, in textual format. These notes cover technical as well as subject-matter related aspects of data cleaning. Technical aspects include data reading, type conversion and string matching and manipulation. Subject-matter related aspects include topics like data checking, error localization and an introduction to imputation methods in R. References to relevant literature and R packages are provided throughout.},
   author = {Edwin de Jonge and Mark van der Loo},
   doi = {60083 201313- X-10-13},
   isbn = {1572-0314},
   issn = {1572-0314},
   journal = {Statistics Netherlands},
   keywords = {data editing,methodology,statistical software},
   pages = {53},
   title = {An introduction to data cleaning with R},
   url = {http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf},
   year = {2013},
}
@book{Gareth2013,
   abstract = {Review From the reviews: .,."There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006 "The reader sees not only how measure theory is used to develop probability theory, but also how probability theory is used in applications. a The discourse is delivered in a theorem proof format and thus is better suited for classroom a . The authors prose is generally well thought out a . will make an attractive choice for a two-semester course on measure and probability, or as a second course for students with a semester of measure or probability theory under their belt." (Peter C. Kiessler, Journal of the American Statistical Association, Vol. 102 (479), 2007) "The book is a well written self-contained textbook on measure and probability theory. It consists of 18 chapters. Every chapter contains many well chosen examples and ends with several problems related to the earlier developed theory (some with hints). a At the very end of the book there is an appendix collecting necessary facts from set theory, calculus and metric spaces. The authors suggest a few possibilities on how to use their book." (Kazimierz Musial, Zentralblatt MATH, Vol. 1125 (2), 2008) "The title of the book consists of the names of its two basic parts. The booka (TM)s third part is comprised of some special topics from probability theory. a The authors suggest using the book intwo-semester graduate programs in statistics or a one-semester seminar on special topics. The material of the book is standard a is clear, comprehensive and a ~without being intimidatinga (TM)." (Rimas NorvaiAa, Mathematical Reviews, Issue 2007 f) Product Description This is a graduate level textbook on measure theory and probability theory. The book can be used as a text for a two semester sequence of courses in measure theory and probability theory, with an option to include supplemental material on stochastic processes and special topics. It is intended primarily for first year Ph.D. students in mathematics and statistics although mathematically advanced students from engineering and economics would also find the book useful. Prerequisites are kept to the minimal level of an understanding of basic real analysis concepts such as limits, continuity, differentiability, Riemann integration, and convergence of sequences and series. A review of this material is included in the appendix. The book starts with an informal introduction that provides some heuristics into the abstract concepts of measure and integration theory, which are then rigorously developed. The first part of the book can be used for a standard real analysis course for both mathematics and statistics Ph.D. students as it provides full coverage of topics such as the construction of Lebesgue-Stieltjes measures on real line and Euclidean spaces, the basic convergence theorems, L p spaces, signed measures, Radon-Nikodym theorem, Lebesgue's decomposition theorem and the fundamental theorem of Lebesgue integration on R, product spaces and product measures, and Fubini-Tonelli theorems. It also provides an elementary introduction to Banach and Hilbert spaces, convolutions, Fourier series and Fourier and Plancherel transforms. Thus part I would be particularly useful for students in a typical Statistics Ph.D. program if a separate course on real analysis is not a standard requirement. Part II (chapters 6-13) provides full coverage of standard graduate level probability theory. It starts with Kolmogorov's probability model and Kolmogorov's existence theorem. It then treats thoroughly the laws of large numbers including renewal theory and ergodic theorems with applications and then weak convergence of probability distributions, characteristic functions, the Levy-Cramer continuity theorem and the central limit theorem as well as stable laws. It ends with conditional expectations and conditional probability, and an introduction to the theory of discrete time martingales. Part III (chapters 14-18) provides a modest coverage of discrete time Markov chains with countable and general state spaces, MCMC, continuous time discrete space jump Markov processes, Brownian motion, mixing sequences, bootstrap methods, and branching processes. It could be used for a topics/seminar course or as an introduction to stochastic processes. From the reviews: "...There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006},
   author = {James Gareth and Witten Daniela and Hastie Trevor and Robert Tibshirani},
   doi = {10.1016/j.peva.2007.06.006},
   isbn = {9780387781884},
   issn = {01621459},
   pmid = {10911016},
   publisher = {Springer},
   title = {An Introduction to Statistical Learning},
   year = {2013},
}
@inbook{Larose2014e,
   author = {Daniel T. Larose and Chantal D. Larose},
   edition = {Second},
   journal = {Discovering Knowledge in Data: An Introduction to Data Mining},
   pages = {91-108},
   title = {Chapter 4 Univariate Statistical Analysis},
   year = {2014},
}
@article{Evaluation2014,
   author = {Model Evaluation},
   pages = {277-293},
   title = {Chapter 14; Model Evaluation Techniques},
   year = {2014},
}
@inbook{Larose2014d,
   author = {Daniel T. Larose and Chantal D. Larose},
   edition = {Second},
   journal = {Discovering Knowledge in Data: An Introduction to Data Mining},
   pages = {165-186},
   title = {Chapter 8 Decision Trees},
   year = {2014},
}
@inbook{Larose2014a,
   author = {Daniel T. Larose and Chantal D. Larose},
   edition = {Second},
   journal = {Discovering Knowledge in Data: An Introduction to Data Mining},
   pages = {51-90},
   title = {Chapter 3 Exploratory Data Analysis},
   year = {2014},
}
@inbook{Larose2014h,
   author = {Daniel T. Larose and Chantal D. Larose},
   doi = {10.1111/j.1600-0404.1995.tb01704.x},
   edition = {Second},
   isbn = {9780585181875},
   issn = {1422-6405},
   journal = {Discovering Knowledge in Data: An Introduction to Data Mining},
   pages = {1-30},
   pmid = {21728113},
   title = {Chapter 2 Data Preprocessing},
   year = {2014},
}
@inbook{Larose2014f,
   author = {Daniel T. Larose and Chantal D. Larose},
   edition = {Second},
   journal = {Discovering Knowledge in Data: An Introduction to Data Mining},
   pages = {138-148},
   title = {Chapter 6 Preparing to Model The Data},
   year = {2014},
}
@article{Modeling2001,
   author = {Nonlinear Modeling},
   doi = {10.1227/01.NEU.0000028161.91504.4F},
   isbn = {9789966029171},
   pages = {109-137},
   title = {Chapter 5; Multivariate Statistics},
   year = {2001},
}
@article{Task2014,
   author = {Classification Task},
   pages = {149-164},
   title = {Chapter 7; k-Nearest Neighbor Algorithm},
   year = {2014},
}
@inbook{Larose2014g,
   author = {Daniel T. Larose and Chantal D. Larose},
   edition = {Second},
   journal = {Discovering Knowledge in Data: An Introduction to Data Mining},
   pages = {1-15},
   title = {Chapter 1 An Introduction To Data Mining},
   year = {2014},
}
@inbook{Dean2014u,
   abstract = {With big data analytics comes big insights into profitabilityBig data is big business. But having the data and the computational power to process it isn't nearly enough to produce meaningful results. Big Data, Data Mining, and Machine Learning: Value Creation for Business Leaders and Practitioners is a complete resource for technology and marketing executives looking to cut through the hype and produce real results that hit the bottom line. Providing an engaging, thorough overview of the current state of big data analytics and the growing trend toward high performance computing architectures, the book is a detail-driven look into how big data analytics can be leveraged to foster positive change and drive efficiency.With continued exponential growth in data and ever more competitive markets, businesses must adapt quickly to gain every competitive advantage available. Big data analytics can serve as the linchpin for initiatives that drive business, but only if the underlying technology and analysis is fully understood and appreciated by engaged stakeholders. This book provides a view into the topic that executives, managers, and practitioners require, and includes:A complete overview of big data and its notable characteristicsDetails on high performance computing architectures for analytics, massively parallel processing (MPP), and in-memory databasesComprehensive coverage of data mining, text analytics, and machine learning algorithmsA discussion of explanatory and predictive modeling, and how they can be applied to decision-making processesBig Data, Data Mining, and Machine Learning provides technology and marketing executives with the complete resource that has been notably absent from the veritable libraries of published books on the topic. Take control of your organization's big data analytics to produce real results with a resource that is comprehensive in scope and light on hyperbole.},
   author = {Jared Dean},
   doi = {10.1126/science.1247727},
   isbn = {9781118618042},
   journal = {Big Data, Data Mining and Machine Learning. Value Creation for Business Leaders and Practitioners},
   pages = {265},
   title = {References},
   url = {http://www.amazon.com/Data-Mining-Machine-Learning-Practitioners/dp/1118618041},
   year = {2014},
}
@article{Of2014,
   author = {Imputation Of and Missing Data},
   pages = {266-276},
   title = {Chapter 13; Imputation of Missing Data},
   year = {2014},
}
@inbook{Larose2014c,
   author = {Daniel T. Larose and Chantal D. Larose},
   edition = {Second},
   journal = {Discovering Knowledge in Data: An Introduction to Data Mining},
   pages = {294-307},
   title = {Appendex Data Summarization And Visualization},
   year = {2014},
}
@article{Networks2014,
   author = {Kohonen Networks},
   pages = {228-246},
   title = {Chapter 11; Kohonen networks},
   year = {2014},
}
@inbook{Dean2014n,
   author = {Jared Dean},
   doi = {10.1002/9781118691786.part3},
   journal = {Big Data, Data Mining and Machine Learning. Value Creation for Business Leaders and Practitioners},
   pages = {193-195},
   title = {Part 3: Success Stories of Putting it all Together},
   url = {http://doi.wiley.com/10.1002/9781118691786.part3},
   year = {2014},
}
@inbook{Larose2014,
   author = {Daniel T. Larose and Chantal D. Larose},
   edition = {Second},
   journal = {Discovering Knowledge in Data: An Introduction to Data Mining},
   pages = {209-227},
   title = {Chapter 10 Hierarchial And k-Means Clustering},
   year = {2014},
}
@inbook{Dean2014m,
   author = {Jared Dean},
   isbn = {1107015359 9781107015357},
   journal = {Big Data, Data Mining, and Machine Learning: Value Creation for Business Leaders and Practitioners},
   title = {Chapter 9 Recommendation Systems},
   year = {2014},
}
@article{Statsoft2011,
   author = {Statsoft},
   pages = {247-265},
   title = {Chapter 12; Association Rules},
   url = {http://www.statsoft.com/textbook/association-rules/},
   year = {2011},
}
@inbook{Dean2014p,
   abstract = {In almost every scientific field, measurements are performed over time. These observations lead to a collection of organized data called time series. The purpose of time series data mining is to try to extract all meaningful knowledge from the shape of data. Even if humans have a natural capacity to perform these tasks, it remains a complex problem for computers. In this paper we intend to provide a survey of the techniques applied for time series data mining. The first part is devoted to an overview of the tasks that have captured most of the interest of researchers. Considering that in most cases, time series task relies on the same components for implementation, we divide the literature depending on these common aspects, namely representation techniques, distance measures and indexing methods. The study of the relevant literature has been categorized for each individual aspects. Four types of robustness could then be formalized and any kind of distance could then be classified. Finally, the study submit various research trends and avenues that can be explored in the near future. We hope that this paper can provide a broad and deep understanding of the time series data mining research field.},
   author = {Jared Dean},
   doi = {10.1145/0000000.0000000},
   issn = {23344547},
   journal = {Big Data, Data Mining, and Machine Learning: Value Creation for Business Leaders and Practitioners.},
   keywords = {Distance measures,Time Series,data indexing,data mining,query by content,sequence matching,similarity measures,stream analysis,temporal analysis},
   title = {Chapter 8 Time series data mining},
   year = {2014},
}
@inbook{Dean2014l,
   author = {Jared Dean},
   doi = {10.1002/9781118691786.ch7},
   journal = {Big Data, Data Mining and Machine Learning. Value Creation for Business Leaders and Practitioners},
   pages = {141-148},
   title = {7: Incremental Response Modeling},
   url = {http://doi.wiley.com/10.1002/9781118691786.ch7},
   year = {2014},
}
@inbook{Dean2014c,
   author = {Jared Dean},
   journal = {Big Data, Data Mining and Machine Learning. Value Creation for Business Leaders and Practitioners},
   pages = {127-139},
   title = {Ch6 Segmentation},
   year = {2014},
}
@inbook{Dean2014j,
   abstract = {The article offers information on data integration, modeling, and analytics as the areas of focus and investment of several Property and Casualty (P&C) carriers. It is noted that business intelligence platforms, which perform in-depth analysis over a wide range of data sources, have improved continuously over the years. Excellent outputs were seen by carriers after converting data into real insight for product development teams. Moreover, the author also mentioned that the demand to transfer wide and complex data stores that can be accessed faster and flexible will be an exciting challenge to the Property and Casualty insurance's technology staffs.},
   author = {Jared Dean},
   issn = {10540733},
   journal = {Big Data, Data Mining and Machine Learning. Value Creation for Business Leaders and Practitioners},
   keywords = {BUSINESS development,BUSINESS enterprises,BUSINESS intelligence,DATA analysis,FUNCTIONAL integration,INFORMATION retrieval,INSURANCE,MANAGEMENT,TECHNOLOGY},
   pages = {53-54},
   title = {Part 2: Turning Data Into Intelligence},
   url = {http://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=28017316&site=ehost-live},
   year = {2014},
}
@article{Wickens1987,
   author = {Fred Wickens},
   doi = {10.1016/0010-4655(87)90135-4},
   issn = {00104655},
   issue = {1-3},
   journal = {Computer Physics Communications},
   pages = {1-8},
   title = {part1 The computing environment},
   volume = {45},
   year = {1987},
}
@inbook{Dean2014k,
   abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-α-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 Å for the interface backbone atoms) increased from 21% with default Glide SP settings to 58% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
   author = {Jared Dean},
   doi = {10.1017/CBO9781107415324.004},
   isbn = {9788578110796},
   issn = {16130073},
   journal = {Big Data, Data Mining and Machine Learning. Value Creation for Business Leaders and Practitioners},
   keywords = {Mobile,Named entity disambiguation,Natural language processing,News,Recommender system},
   pages = {ev},
   pmid = {25246403},
   title = {Index},
   year = {2014},
}
@inbook{Dean2014e,
   author = {Jared Dean},
   journal = {Big Data, Data Mining and Machine Learning. Value Creation for Business Leaders and Practitioners},
   pages = {i-xix},
   title = {Front Matter},
   year = {2014},
}
@inbook{Dean2014a,
   author = {Jared Dean},
   doi = {10.4324/9781315818719},
   isbn = {9781315818719},
   journal = {Big Data, Data Mining and Machine Learning. Value Creation for Business Leaders and Practitioners},
   pages = {233-241},
   title = {17: Looking to the future},
   year = {2014},
}
@inbook{Dean2014t,
   author = {Jared Dean},
   journal = {Big Data, Data Mining and Machine Learning. Value Creation for Business Leaders and Practitioners},
   pages = {229-232},
   title = {16: Case Study of a High‐ Tech Product Manufacturer},
   year = {2014},
}
@inbook{Dean2014q,
   author = {Jared Dean},
   journal = {Big Data, Data Mining and Machine Learning. Value Creation for Business Leaders and Practitioners},
   pages = {221-224},
   title = {14: Case Study of Online Brand Management},
   year = {2014},
}
@inbook{Dean2014h,
   author = {Jared Dean},
   isbn = {9781118618042},
   journal = {Big Data, Data Mining and Machine Learning. Value Creation for Business Leaders and Practitioners},
   pages = {71-126},
   title = {Common Predictive Modeling Techniques},
   year = {2014},
}
@inbook{Dean2014g,
   author = {Jared Dean},
   journal = {Big Data, Data Mining and Machine Learning. Value Creation for Business Leaders and Practitioners},
   pages = {43-52},
   title = {ch3 Analytical Tools},
   year = {2014},
}
@inbook{Dean2014v,
   author = {Jared Dean},
   journal = {Big Data, Data Mining and Machine Learning. Value Creation for Business Leaders and Practitioners},
   pages = {225-228},
   title = {15: Case Study of Mobile Application Recommendations},
   year = {2014},
}
@inbook{Dean2014,
   author = {Jared Dean},
   issue = {1},
   journal = {Big Data, Data Mining and Machine Learning. Value Creation for Business Leaders and Practitioners},
   pages = {35-41},
   title = {2: Distributed System},
   year = {2014},
}
@inbook{Dean2014f,
   author = {Jared Dean},
   journal = {Big Data, Data Mining and Machine Learning. Value Creation for Business Leaders and Practitioners},
   pages = {233-239},
   title = {ch4 Predictive Modeling},
   year = {2014},
}
@inbook{Dean2014w,
   author = {Jared Dean},
   doi = {10.1002/9781118691786},
   journal = {Big Data, Data Mining and Machine Learning. Value Creation for Business Leaders and Practitioners},
   pages = {1-21},
   title = {Introduction},
   year = {2014},
}
@inbook{Dean2014o,
   author = {Jared Dean},
   journal = {Big Data, Data Mining and Machine Learning. Value Creation for Business Leaders and Practitioners},
   pages = {245-246},
   title = {Appendix 1: Nike+ fuelband script to retrieve information},
   year = {2014},
}
@inbook{Dean2014s,
   author = {Jared Dean},
   journal = {Big Data, Data Mining and Machine Learning. Value Creation for Business Leaders and Practitioners},
   pages = {205-214},
   title = {12: Case Study of a Major Health Care Provider},
   year = {2014},
}
@inbook{Dean2014d,
   author = {Jared Dean},
   journal = {Big Data, Data Mining and Machine Learning. Value Creation for Business Leaders and Practitioners},
   pages = {197-204},
   title = {Ch 11 Case Study of a Large U.S. ‐ Based Financial Services Company},
   year = {2014},
}
@inbook{Dean2014r,
   author = {Jared Dean},
   doi = {10.1016/B978-0-12-404648-1.00018-1},
   journal = {Big Data, Data Mining, and Machine Learning: Value Creation for Business Leaders and Practitioners.},
   pages = {xii},
   title = {About the Author},
   url = {http://linkinghub.elsevier.com/retrieve/pii/B9780124046481000181},
   year = {2014},
}
@article{Eddy2004,
   abstract = {Programs such as MFOLD and ViennaRNA are widely used to predict RNA secondary structures. How do these algorithms work? Why can't they predict RNA pseudoknots? How accurate are they, and will they get better?},
   author = {Sean R. Eddy},
   doi = {10.1038/nbt1104-1457},
   isbn = {1087-0156},
   issn = {10870156},
   issue = {11},
   journal = {Nature Biotechnology},
   pages = {1457-1458},
   pmid = {15529172},
   title = {How do RNA folding algorithms work?},
   volume = {22},
   year = {2004},
}
@inbook{Dean2014i,
   author = {Jared Dean},
   journal = {Big Data, Data Mining and Machine Learning. Value Creation for Business Leaders and Practitioners},
   pages = {215-219},
   title = {13: Case Study of a Technology Manufacturer},
   year = {2014},
}
@inbook{Dean2014b,
   author = {Jared Dean},
   journal = {Big Data, Data Mining and Machine Learning. Value Creation for Business Leaders and Practitioners},
   pages = {175-191},
   title = {ch10 Text Analytics},
   year = {2014},
}
@article{Eddy2004b,
   abstract = {Don't expect much enlightenment from the etymology of the term 'dynamic programming,' though. Dynamic programming was formalized in the early 1950s by mathematician Richard Bellman, who was working at RAND Corporation on},
   author = {Sean R. Eddy},
   doi = {10.1038/nbt0704-909},
   isbn = {1087-0156},
   issn = {1087-0156},
   issue = {7},
   journal = {Nature biotechnology},
   pages = {909–910},
   pmid = {15229554},
   title = {What is dynamic programming?},
   volume = {22},
   url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:What+is+dynamic+programming?#0},
   year = {2004},
}
@article{Eddy2004c,
   abstract = {Many sequence alignment programs use the BLOSUM62 score matrix to score pairs of aligned residues. Where did BLOSUM62 come from?},
   author = {Sean R. Eddy},
   doi = {10.1038/nbt0804-1035},
   isbn = {1087-0156 (Print)\r1087-0156 (Linking)},
   issn = {10870156},
   issue = {8},
   journal = {Nature Biotechnology},
   pages = {1035-1036},
   pmid = {15286655},
   title = {Where did the BLOSUM62 alignment score matrix come from?},
   volume = {22},
   year = {2004},
}
@article{Drummond2006,
   abstract = {In phylogenetics, the unrooted model of phylogeny and the strict molecular clock model are two extremes of a continuum. Despite their dominance in phylogenetic inference, it is evident that both are biologically unrealistic and that the real evolutionary process lies between these two extremes. Fortunately, intermediate models employing relaxed molecular clocks have been described. These models open the gate to a new field of "relaxed phylogenetics." Here we introduce a new approach to performing relaxed phylogenetic analysis. We describe how it can be used to estimate phylogenies and divergence times in the face of uncertainty in evolutionary rates and calibration times. Our approach also provides a means for measuring the clocklikeness of datasets and comparing this measure between different genes and phylogenies. We find no significant rate autocorrelation among branches in three large datasets, suggesting that autocorrelated models are not necessarily suitable for these data. In addition, we place these datasets on the continuum of clocklikeness between a strict molecular clock and the alternative unrooted extreme. Finally, we present analyses of 102 bacterial, 106 yeast, 61 plant, 99 metazoan, and 500 primate alignments. From these we conclude that our method is phylogenetically more accurate and precise than the traditional unrooted model while adding the ability to infer a timescale to evolution.},
   author = {Alexei J. Drummond and Simon Y.W. Ho and Matthew J. Phillips and Andrew Rambaut},
   doi = {10.1371/journal.pbio.0040088},
   isbn = {1544-9173},
   issn = {15457885},
   issue = {5},
   journal = {PLoS Biology},
   pages = {699-710},
   pmid = {16683862},
   title = {Relaxed phylogenetics and dating with confidence},
   volume = {4},
   year = {2006},
}
@article{Easteal1992,
   author = {Simon Easteal},
   issue = {6},
   journal = {BioEssays},
   pages = {415-419},
   title = {A Mammalian Molecular Clock?},
   volume = {14},
   year = {1992},
}
@article{Welch2018a,
   author = {David Welch},
   pages = {1-15},
   title = {Lecture Notes 703 2018 Graphs , networks and their algorithms in bioinformatics and systems biology},
   year = {2018},
}
@article{Lavery,
   author = {Shane Lavery},
   title = {Population genetic analyses Evolutionary fate ( and rate ) of mutations},
}
@article{Welch,
   author = {David Welch},
   title = {L01 : Introduction to graphs , networks},
}
@article{DHaeseleer2006,
   abstract = {Sequence motifs are becoming increasingly important in the analysis of gene regulation. How do we define sequence motifs, and why should we use sequence logos instead of consensus sequences to represent them? Do they have any relation with binding affinity? How do we search for new instances of a motif in this sea of DNA?},
   author = {Patrik D'Haeseleer},
   doi = {10.1038/nbt0406-423},
   isbn = {1087-0156 (Print)\r1087-0156 (Linking)},
   issn = {1087-0156},
   issue = {4},
   journal = {Nature Biotechnology},
   pages = {423-425},
   pmid = {16601727},
   title = {What are DNA sequence motifs?},
   volume = {24},
   url = {http://dx.doi.org/10.1038/nbt0406-423},
   year = {2006},
}
@article{Properties2007,
   author = {Network Structural Properties},
   title = {When is one graph different from another ?},
   volume = {119},
   year = {2007},
}
@article{Brent2007,
   abstract = {Computational prediction of gene structure is crucial for interpreting genomic sequences. But how do the algorithms involved work and how accurate are they?},
   author = {Michael R. Brent},
   doi = {10.1038/nbt0807-883},
   isbn = {1087-0156 (Print)},
   issn = {10870156},
   issue = {8},
   journal = {Nature Biotechnology},
   pages = {883-885},
   pmid = {17687368},
   title = {How does eukaryotic gene prediction work?},
   volume = {25},
   year = {2007},
}
@article{Dhaeseleer2006,
   abstract = {How can we computationally extract an unknown motif from a set of target sequences? What are the principles behind the major motif discovery algorithms? Which of these should we use, and how do we know we've found a 'real' motif? Extracting regulatory motifs1 from DNA sequences seems to be all the rage these days. Take your favorite cluster of coexpressed genes, and with some luck you might hope to find a short pattern of nucleotides upstream of the transcription start sites of these genes, indicating a common transcription factor binding site responsible for their coordinate regulation.},
   author = {Patrik D'haeseleer},
   doi = {10.1038/nbt0806-959},
   isbn = {1087-0156 (Print)\r1087-0156 (Linking)},
   issn = {10870156},
   issue = {8},
   journal = {Nature Biotechnology},
   pages = {959-961},
   pmid = {16900144},
   title = {How does DNA sequence motif discovery work?},
   volume = {24},
   year = {2006},
}
@article{Compeau2011,
   abstract = {A mathematical concept known as a de Bruijn graph turns the formidable challenge of assembling a contiguous genome from billions of short sequencing reads into a tractable computational problem.},
   author = {Phillip E.C. Compeau and Pavel A. Pevzner and Glenn Tesler},
   doi = {10.1038/nbt.2023},
   isbn = {0000110010111},
   issn = {10870156},
   issue = {11},
   journal = {Nature Biotechnology},
   pages = {987-991},
   pmid = {22068540},
   publisher = {Nature Publishing Group},
   title = {How to apply de Bruijn graphs to genome assembly},
   volume = {29},
   url = {http://dx.doi.org/10.1038/nbt.2023},
   year = {2011},
}
@article{Park2003,
   abstract = {Jarzynski’s equality is applied to free energy calculations from steered molecular dynamics simulations of biomolecules. The helix-coil transition of deca-alanine in vacuum is used as an example. With about ten trajectories sampled, the second order cumulant expansion, among the various averaging schemes examined, yields the most accurate estimates. We compare umbrella sampling and the present method, and find that their efficiencies are comparable.},
   author = {Sanghyun Park and Fatemeh Khalili-Araghi and Emad Tajkhorshid and Klaus Schulten},
   doi = {10.1063/1.1590311},
   isbn = {00219606},
   issn = {00219606},
   issue = {6},
   journal = {The Journal of Chemical Physics},
   pages = {3559},
   pmid = {184350300066},
   title = {Free energy calculation from steered molecular dynamics simulations using Jarzynski’s equality},
   volume = {119},
   url = {http://scitation.aip.org/content/aip/journal/jcp/119/6/10.1063/1.1590311},
   year = {2003},
}
@article{Motion,
   author = {Thermal Motion},
   pages = {1-5},
   title = {Slide 1 : Chromatophore},
}
@misc{VMD,
   author = {VMD},
   title = {Slide 1 : Brownian Motion Slides 2- ­ ‐ 7 : Hydrophobic Effect},
}
@article{Modelling,
   author = {Biochemical Modelling},
   pages = {1-12},
   title = {246.201 bioinf703},
}
@article{Golding1983,
   author = {G B Golding},
   issue = {1},
   journal = {Molecular Biology and Evolution},
   pages = {Pages 125–142},
   title = {Estimates of DNA and protein sequence divergence: an examination of some assumptions.,},
   volume = {1},
   url = {https://doi.org/10.1093/oxfordjournals.molbev.a040303},
   year = {1983},
}
@inbook{Zuckerkandl1963,
   author = {Emile Zuckerkandl and Linus Pauling},
   journal = {Molecular Disease Evolution, And The Gene},
   pages = {p189 -},
   title = {Molecular Disease, Evolution, and Genic Heterogeneity},
   year = {1963},
}
@article{Goodman1974,
   author = {Morris Goodman and G. William Moore and John Barnabas and Genji Matsuda},
   issue = {1},
   journal = {Journal of Molecular Evolution},
   pages = {1–48},
   title = {The phylogeny of human globin genes investigated by the maximum parsimony method},
   volume = {3},
   year = {1974},
}
@book{Kimura1983,
   author = {M Kimura},
   city = {Cambridge},
   publisher = {Cambridge Univ. Press},
   title = {The Neutral Theory of Molecular Evolution.},
   year = {1983},
}
@article{Li1987,
   author = {WH. Li and M. Tanimura and P.M. Sharp},
   issue = {330},
   journal = {Journal of Molecular Evolution},
   title = {An Evaluation of the Molecular Clock Hypothesis Using Mammalian DNA Sequences.},
   volume = {25},
   url = {https://doi.org/10.1007/BF02603118},
   year = {1987},
}
@article{Gillespie1979,
   author = {John H. Gillespie and Charles H. Langley},
   issue = {1},
   journal = {Journal of Molecular Evolution},
   pages = {27–34},
   title = {Are evolutionary rates really variable?},
   volume = {13},
   year = {1979},
}
@article{Hasegawa1985,
   abstract = {A new statistical method for estimating divergence dates of species from DNA sequence data by a molecular clock approach is developed. This method takes into account effectively the information contained in a set of DNA sequence data. The molecular clock of mitochondrial DNA (mtDNA) was calibrated by setting the date of divergence between primates and ungulates at the Cretaceous-Tertiary boundary (65 million years ago), when the extinction of dinosaurs occurred. A generalized leastsquares method was applied in fitting a model to mtDNA sequence data, and the clock gave dates of 92.3±11.7, 13.3±1.5, 10.9±1.2, 3.7±0.6, and 2.7±0.6 million years ago (where the second of each pair of numbers is the standard deviation) for the separation of mouse, gibbon, orangutan, gorilla, and chimpanzee, respectively, from the line leading to humans. Although there is some uncertainty in the clock, this dating may pose a problem for the widely believed hypothesis that the bipedal creatureAustralopithecus afarensis, which lived some 3.7 million years ago at Laetoli in Tanzania and at Hadar in Ethiopia, was ancestral to man and evolved after the human-ape splitting. Another likelier possibility is that mtDNA was transferred through hybridization between a proto-human and a protochimpanzee after the former had developed bipedalism.},
   author = {Masami Hasegawa and Hirohisa Kishino and Taka-aki Yano},
   doi = {10.1007/BF02101694},
   issn = {1432-1432},
   issue = {2},
   journal = {Journal of Molecular Evolution},
   pages = {160-174},
   title = {Dating of the human-ape splitting by a molecular clock of mitochondrial DNA},
   volume = {22},
   url = {https://doi.org/10.1007/BF02101694},
   year = {1985},
}
@article{jukes1969evolution,
   author = {Thomas H Jukes and Charles R Cantor},
   issue = {21},
   journal = {Mammalian protein metabolism},
   pages = {132},
   publisher = {New York},
   title = {Evolution of protein molecules},
   volume = {3},
   year = {1969},
}
@book{mayr1982growth,
   author = {Ernst Mayr},
   isbn = {9780674364462},
   publisher = {Belknap Press of Harvard University Press},
   title = {The Growth of Biological Thought: Diversity, Evolution, and Inheritance},
   url = {https://books.google.co.nz/books?id=pHThtE2R0UQC},
   year = {1982},
}
@article{tajima1984estimation,
   author = {Fumio Tajima and Masatoshi Nei},
   issue = {3},
   journal = {Molecular biology and evolution},
   pages = {269-285},
   title = {Estimation of evolutionary distance between nucleotide sequences.},
   volume = {1},
   year = {1984},
}
@article{10.2307/1715117,
   author = {George Gaylord Simpson},
   issn = {00368075, 10959203},
   issue = {3651},
   journal = {Science},
   pages = {1535-1538},
   publisher = {American Association for the Advancement of Science},
   title = {Organisms and Molecules in Evolution},
   volume = {146},
   url = {http://www.jstor.org/stable/1715117},
   year = {1964},
}
@article{Rambaut1998,
   abstract = {The ability to date the time of divergence between lineages using molecular data provides the opportunity to answer many important questions in evolutionary biology. However, molecular dating techniques have previously been criticized for failing to adequately account for variation in the rate of molecular evolution. We present a maximum-likelihood approach to estimating divergence times that deals explicitly with the problem of rate variation. This method has many advantages over previous approaches including the following: (1) a rate constancy test excludes data for which rate heterogeneity is detected; (2) date estimates are generated with confidence intervals that allow the explicit testing of hypotheses regarding divergence times; and (3) a range of sequences and fossil dates are used, removing the reliance on a single calculated calibration rate. We present tests of the accuracy of our method, which show it to be robust to the effects of some modes of rate variation. In addition, we test the effect of substitution model and length of sequence on the accuracy of the dating technique. We believe that the method presented here offers solutions to many of the problems facing molecular dating and provides a platform for future improvements to such analyses.},
   author = {A Rambaut and L Bromham},
   issn = {0737-4038},
   issue = {4},
   journal = {Molecular Biology and Evolution},
   month = {4},
   note = {10.1093/oxfordjournals.molbev.a025940},
   pages = {442-448},
   title = {Estimating divergence dates from molecular sequences.},
   volume = {15},
   url = {http://dx.doi.org/10.1093/oxfordjournals.molbev.a025940},
   year = {1998},
}
@article{Sanderson1997,
   abstract = {A new method for estimating divergence times when evolutionary rates are variable across lineages is proposed. The method, called nonparametric rate smoothing (NPRS), relies on minimization of ancestor-descendant local rate changes and is motivated by the likelihood that evolutionary rates are autocorrelated in time. Fossil information pertaining to minimum and/or maximum ages of nodes in a phylogeny is incorporated into the algorithms by constrained optimization techniques. The accuracy of NPRS was examined by comparison to a clock-based maxi- mum-likelihood method in computer simulations. NPRS provides more accurate estimates of divergence times when (1) sequence lengths are sufficiently long, (2) rates are truly nonclocklike, and (3) rates are moderately to highly autocorrelated in time. The algorithms were applied to estimate divergence times in seed plants based on data from the chloroplast rbcL gene. Both constrained and unconstrained NPRS methods tended to produce divergence time estimates more consistent with paleobotanical evidence than did clock-based estimates.},
   author = {Michael J Sanderson},
   doi = {10.1093/oxfordjournals.molbev.a025731},
   isbn = {0737-4038},
   issn = {0737-4038},
   issue = {12},
   journal = {Molecular Biology and Evolution},
   pages = {1218-1231},
   title = {A Nonparametric Approach of Rate Constancy to Estimating Divergence Times in the Absence},
   volume = {14},
   year = {1997},
}
@article{Felsenstein1981,
   abstract = {The application of maximum likelihood techniques to the estimation of evolutionary trees from nucleic acid sequence data is discussed. A computationally feasible method for finding such maximum likelihood estimates is developed, and a computer program is available. This method has advantages over the traditional parsimony algorithms, which can give misleading results if rates of evolution differ in different lineages. It also allows the testing of hypotheses about the constancy of evolutionary rates by likelihood ratio tests, and gives rough indication of the error of the estimate of the tree.},
   author = {Joseph Felsenstein},
   doi = {10.1007/BF01734359},
   issn = {1432-1432},
   issue = {6},
   journal = {Journal of Molecular Evolution},
   pages = {368-376},
   title = {Evolutionary trees from DNA sequences: A maximum likelihood approach},
   volume = {17},
   url = {https://doi.org/10.1007/BF01734359},
   year = {1981},
}
@article{Mccann2015,
   author = {Kristine Mccann},
   isbn = {0002321026073},
   pages = {2014},
   title = {Digital Receipt},
   year = {2015},
}
@article{Cook2016,
   abstract = {New technologies are revolutionising biological re- search and its applications by making it easier and cheaper to generate ever-greater volumes and types of data. In response, the services and infrastruc- ture of the European Bioinformatics Institute (EMBL- EBI, www.ebi.ac.uk) are continually expanding: total disk capacity increases significantly every year to keep pace with demand (75 petabytes as of Decem- ber 2015), and interoperability between resources remains a strategic priority. Since 2014 we have launched two newresources: the European Variation Archive for genetic variation data and EMPIAR for two-dimensional electron microscopy data, as well as a Resource Description Framework platform. We also launched the Embassy Cloud service, which al- lows users to run large analyses in a virtual environ- ment next to EMBL-EBI’s vast public data resources.},
   author = {Charles E Cook and Mary Todd Bergman and Robert D Finn and Guy Cochrane and Ewan Birney and Rolf Apweiler},
   doi = {10.1093/nar/gkv1352},
   isbn = {13624962 (Electronic)},
   issn = {13624962},
   issue = {D1},
   journal = {Nucleic Acids Research},
   month = {1},
   pages = {D20-D26},
   pmid = {26673705},
   publisher = {Oxford University Press},
   title = {The European Bioinformatics Institute in 2016: Data growth and integration},
   volume = {44},
   url = {http://www.ncbi.nlm.nih.gov/pubmed/26673705 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4702932},
   year = {2016},
}
@article{Reanney1984,
   abstract = {Heat induces a number of premutational lesions (for example, the deamination of cytosine to uracil) in DNA and RNA. These kinds of errors occur in resting as well as replicating polynucleotides. However, an increase in temperature also raises the probability of copying error occurring in nucleic acids because of increased thermal noise in the replicative machinery. In most modern genetic systems, the majority of heat-induced lesions are efficiently repaired. It follows that the importance of heat-induced error increases as the effectiveness of repair declines. We show in this paper that the error rate of enzymatic polynucleotide copying is expected to increase monotonically with temperature. We also explore the effects of temperature variations on the early evolution of biological information transmission mechanisms.},
   author = {Darryl C. Reanney and J. Pressing},
   doi = {10.1007/BF02100629},
   issn = {00222844},
   issue = {1},
   journal = {Journal of Molecular Evolution},
   keywords = {Evolution,Heat,Rates of copy error},
   pages = {72-75},
   pmid = {6442360},
   title = {Temperature as a determinative factor in the evolution of genetic systems},
   volume = {21},
   year = {1984},
}
@misc{Output,
   author = {N-Migrate Output},
   pages = {1-10},
   title = {RW 3 pops based on BioSci733 _ rightwh . nxs + RWamov},
}
@article{Shannon1948,
   abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange band-width for signal-to-noise ratio has intensified the interest in a general theory of communication.\n},
   author = {Claude Edmund Shannon},
   doi = {10.1002/j.1538-7305.1948.tb01338.x},
   isbn = {0252725484},
   issn = {15591662},
   issue = {3},
   journal = {Bell System Technical Journal},
   pages = {3},
   pmid = {9230594},
   title = {A Mathematical Theory of Communication},
   volume = {5},
   url = {http://portal.acm.org/citation.cfm?doid=584091.584093%5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6773024},
   year = {1948},
}
@misc{N-MigrateOutput,
   author = {N-Migrate Output},
   isbn = {1881505405},
   title = {Example : Microsatellite data set},
}
@misc{Output2013a,
   author = {N-Migrate Output},
   pages = {1-15},
   title = {Example : Microsatellite data set},
   year = {2013},
}
@misc{Output2013b,
   author = {N-Migrate Output},
   pages = {1-15},
   title = {Example : Microsatellite data set},
   year = {2013},
}
@article{Excoffier2010,
   abstract = {Arlequin ver 3.0 is a software package integrating several basic and advanced methods for population genetics data analysis, like the computation of standard genetic diversity indices, the estimation of allele and haplotype frequencies, tests of departure from linkage equilibrium, departure from selective neutrality and demographic equilibrium, estimation or parameters from past population expansions, and thorough analyses of population subdivision under the AMOVA framework. Arlequin 3 introduces a completely new graphical interface written in C++, a more robust semantic analysis of input files, and two new methods: a Bayesian estimation of gametic phase from multi-locus genotypes, and an estimation of the parameters of an instantaneous spatial expansion from DNA sequence polymorphism. Arlequin can handle several data types like DNA sequences, microsatellite data, or standard multi-locus genotypes. A Windows version of the software is freely available on http://cmpg.unibe.ch/software/arlequin3.},
   author = {Laurent Excoffier and H E L Lischer},
   doi = {10.1111/j.1755-0998.2010.02847.x},
   isbn = {1755-0998},
   issn = {1755-0998},
   journal = {Molecular Ecology Resources},
   pages = {564-567},
   pmid = {21565059},
   title = {An Integrated Software Package for Population Genetics Data Analysis},
   volume = {10},
   year = {2010},
}
@article{Boeke2018,
   author = {Jef D. Boeke and George M. Church and Andrew Hessel and Nancy J Kelley},
   issue = {May},
   title = {GP-write : A Grand Challenge Project to Build and Test Genomes in Living Cells},
   year = {2018},
}
@article{Drake1976,
   author = {John W. Drake and Richard H. Baltz},
   journal = {Annu. Rev. Biochem.},
   pages = {11-37},
   title = {The Biochemistry of Mutagenesis},
   volume = {45},
   year = {1976},
}
@misc{Output2013,
   author = {N-Migrate Output},
   pages = {1-15},
   title = {Example : Microsatellite data set},
   year = {2013},
}
@misc{Nielsen1996,
   abstract = {A novel class of compounds, known as peptide nucleic acids, bind complementary ssDNA and RNA strands more strongly than a corresponding DNA. The peptide nucleic acids generally comprise ligands such as naturally occurring DNA bases attached to a peptide backbone through a suit able linker.},
   author = {Peter E Nielsen and Ole Buchardt and Michael Egholm and Rolf H. Berg},
   doi = {10.1126/science.1174577},
   isbn = {978-1-62703-552-1},
   journal = {US Patent 5,539,082},
   title = {Peptide Nucleic Acids},
   year = {1996},
}
@article{Kricker1990,
   abstract = {Heat induces transversions (as well as transitions) at G-C base pairs in bacteriophage T4. The target base for transversions is guanine,which is converted to a product which is sometimes replicated and transcribed as a pyrimidine.A model for this process is proposed in which the deoxyguanosine glycosidic bond migrates from N9 to N2: the resulting deoxyneoguanosine may pair with normal guanine to produce G-C leads to C-G transversions.},
   author = {M. C. Kricker and John W. Drake},
   doi = {10.1128/jb.172.6.3037-3039.1990},
   issn = {00219193},
   issue = {6},
   journal = {Journal of Bacteriology},
   pages = {3037-3039},
   pmid = {1069305},
   title = {Heat mutagenesis in bacteriophage T4: Another walk down the transversion pathway},
   volume = {172},
   year = {1990},
}
@inproceedings{Furrer2018,
   author = {Simeon Furrer and Mark A Lantz},
   city = {Milpitas, CA},
   journal = {The 29th Magnetic Recording Conference (TMRC)},
   title = {Timing Recovery For Low-SNR Magnetic Tape Recording Channels},
   year = {2018},
}
@article{Birkelund2017,
   author = {Klaus Birkelund and Abildgaard Jensen},
   title = {PhD Thesis Tape as Primary Storage for Large Scientific Data Sets},
   year = {2017},
}
@article{Roberts2000,
   abstract = {To keep pace with the Internet's growth, the maximum speed of core\nrouters and switches must increase at the same rate. In a study\nconducted in 1969, the author analyzed 39 scientific computers released\nor planned for release from 1958 to 1972 to determine optimal computer\nreplacement strategy (http://www.ziplink.net/lroberts/Forecast69.htm).\nThis study looked at the trend of CPU throughput per dollar and\npredicted that computer performance would double every 18.6 months.\nUpdating the study using data for 1999 PCs shows that the trend over 41\nyears is a doubling of computer performance every 21 months, a\nremarkably small correction. A similar study tracking the costs from the\nfirst ARPA packet switches in 1969 to the most modern routers and ATM\nswitches in 1999 confirms that packet switches have followed the same\ntrend as computers, with performance per dollar doubling every 21\nmonths. Although the computer performance rate predicted in the updated\n1969 study is similar to Moore's law, the trends are not identical. It\nwould appear that both the performance per dollar for computers and the\nserial interface speed for communications are increasing at 94 percent\nof the yearly growth rate of semiconductor performance. We can use this\ninformation about performance and cost trends to predict the cost of\ncomputers and communications and to understand the Internet traffic\ngrowth. Keeping up with these trends will be a major engineering\nchallenge},
   author = {Lawrence G. Roberts},
   doi = {10.1109/2.963131},
   isbn = {0018-9162},
   issn = {00189162},
   issue = {1},
   journal = {Computer},
   pages = {117-119},
   title = {Beyond Moore's Law: Internet Growth Trends},
   volume = {33},
   url = {http://ieeexplore.ieee.org/document/963131/},
   year = {2000},
}
@article{Marr2015,
   author = {Bernard Marr},
   journal = {Forbes},
   title = {Big Data: 20 Mind-Boggling Facts Everyone Must Read},
   url = {https://www.forbes.com/sites/bernardmarr/2015/09/30/big-data-20-mind-boggling-facts-everyone-must-read/#76d3b30917b1},
   year = {2015},
}
@misc{Switch2018,
   author = {Switch},
   title = {Switch Data Center - The Citidel},
   url = {https://www.switch.com/tahoe-reno},
   year = {2018},
}
@article{Zwolenski2014,
   author = {Matt Zwolenski and Lee Weatherill},
   issue = {3},
   journal = {The Australian Journal of Telecommunications and the Digital Economy},
   pages = {1-9},
   title = {The Digital Universe: Rich Data and the Increasing Value of the Internet of Things},
   volume = {2},
   year = {2014},
}
@misc{Oracle2018,
   author = {Oracle},
   title = {SuperCluster M8 | Oracle},
   url = {https://www.oracle.com/engineered-systems/supercluster/supercluster-m8/index.html},
   year = {2018},
}
@misc{Morgan2018,
   author = {Casey Morgan},
   journal = {StorageCraft},
   title = {Data storage lifespans: How long will media really last?},
   url = {https://blog.storagecraft.com/data-storage-lifespan/},
   year = {2018},
}
@inproceedings{Williams2008,
   author = {Paul Williams and David S. H. Rosenthal and Mema Roussopoulos and Steve Georgis},
   city = {Bern, Switzerland},
   isbn = {9780892082773},
   journal = {The Imaging Science & Technology (digital) Archiving Conference},
   pages = {188-192},
   title = {Predicting Archival Life of Removable Hard Disk Drives},
   url = {http://www.ingentaconnect.com/content/ist/ac/2008/00002008/00000001/art00038},
   year = {2008},
}
@misc{Millenniata2018,
   author = {Millenniata},
   title = {M-DISC Technology | M-DISC},
   url = {http://www.mdisc.com/mdisc-technology/},
   year = {2018},
}
@misc{IGCSEICT,
   author = {IGCSE ICT},
   title = {Comparison of Storage Media},
   url = {https://www.ictlounge.com/html/comparison_of_storage_media.htm},
}
@misc{Zetta2016,
   author = {Zetta},
   title = {Advances in Data Storage Technology: A Timeline},
   url = {https://www.zetta.net/about/blog/history-data-storage-technology},
   year = {2016},
}
@article{Stojanovic2003,
   abstract = {We describe a molecular automaton, called MAYA, which encodes a version of the game of tic-tac-toe and interactively competes against a human opponent. The automaton is a Boolean network of deoxyribozymes that incorporates 23 molecular-scale logic gates and one constitutively active deoxyribozyme arrayed in nine wells (3x3) corresponding to the game board. To make a move, MAYA carries out an analysis of the input oligonucleotide keyed to a particular move by the human opponent and indicates a move by fluorescence signaling in a response well. The cycle of human player input and automaton response continues until there is a draw or a victory for the automaton. The automaton cannot be defeated because it implements a perfect strategy.},
   author = {Milan N. Stojanovic and Darko Stefanovic},
   doi = {10.1038/nbt862},
   isbn = {1087-0156},
   issn = {10870156},
   issue = {9},
   journal = {Nature Biotechnology},
   pages = {1069-1074},
   pmid = {12923549},
   title = {A deoxyribozyme-based molecular automaton},
   volume = {21},
   year = {2003},
}
@article{Adleman1994,
   author = {Leonard M Adleman},
   issue = {5187},
   journal = {Science},
   pages = {1021-1024},
   title = {Molecular Computation of Solutions to Combinatorial Problems},
   volume = {266},
   year = {1994},
}
@article{Meyer2014,
   abstract = {Excavations of a complex of caves in the Sierra de Atapuerca in northern Spain have unearthed hominin fossils that range in age from the early Pleistocene to the Holocene. One of these sites, the 'Sima de los Huesos' ('pit of bones'), has yielded the world's largest assemblage of Middle Pleistocene hominin fossils, consisting of at least 28 individuals dated to over 300,000 years ago. The skeletal remains share a number of morphological features with fossils classified as Homo heidelbergensis and also display distinct Neanderthal-derived traits. Here we determine an almost complete mitochondrial genome sequence of a hominin from Sima de los Huesos and show that it is closely related to the lineage leading to mitochondrial genomes of Denisovans, an eastern Eurasian sister group to Neanderthals. Our results pave the way for DNA research on hominins from the Middle Pleistocene.},
   author = {Matthias Meyer and Qiaomei Fu and Ayinuer Aximu-Petri and Isabelle Glocke and Birgit Nickel and Juan Luis Arsuaga and Ignacio Martínez and Ana Gracia and José María Bermúdez De Castro and Eudald Carbonell and Svante Pääbo},
   doi = {10.1038/nature12788},
   isbn = {1476-4687 (Electronic)\n0028-0836 (Linking)},
   issn = {00280836},
   issue = {7483},
   journal = {Nature},
   pages = {403-406},
   pmid = {24305051},
   title = {A mitochondrial genome sequence of a hominin from Sima de los Huesos},
   volume = {505},
   year = {2014},
}
@article{Hoss1996,
   abstract = {Gas chromatography/mass spectrometry (GC/MS) was used to determine the amounts of eight oxidative base modifications in DNA extracted from 11 specimens of bones and soft tissues, ranging in age from 40 to >50 000 years. Among the compounds assayed hydantoin derivatives of pyrimidines were quantitatively dominant. From five of the specimens endogenous ancient DNA sequences could be amplified by PCR. The DNA from these specimens contained substantially lower amounts of hydantoins than the six specimens from which no DNA could be amplified. Other types of damage, e.g. oxidation products of purines, did not correlate with the inability to retrieve DNA sequences. Furthermore, all samples with low amounts of damage and from which DNA could be amplified stemmed from regions where low temperatures have prevailed throughout the burial period of the specimens.},
   author = {Matthias Höss and Pawel Jaruga and Tomasz H. Zastawny and Mirai Dizdaroglu and Svante Pääbo},
   doi = {10.1093/nar/24.7.1304},
   isbn = {0305-1048 (Print)\r0305-1048 (Linking)},
   issn = {03051048},
   issue = {7},
   journal = {Nucleic Acids Research},
   pages = {1304-1307},
   pmid = {8614634},
   title = {DNA damage and DMA sequence retrieval from ancient tissues},
   volume = {24},
   year = {1996},
}
@article{Heupink2016,
   abstract = {The publication in 2001 by Adcock et al. [Adcock GJ, et al. (2001) Proc Natl Acad Sci USA 98(2):537-542] in PNAS reported the recovery of short mtDNA sequences from ancient Australians, including the 42,000-y-old Mungo Man [Willandra Lakes Hominid (WLH3)]. This landmark study in human ancient DNA suggested that an early modern human mitochondrial lineage emerged in Asia and that the theory of modern human origins could no longer be considered solely through the lens of the "Out of Africa" model. To evaluate these claims, we used second generation DNA sequencing and capture methods as well as PCR-based and single-primer extension (SPEX) approaches to reexamine the same four Willandra Lakes and Kow Swamp 8 (KS8) remains studied in the work by Adcock et al. Two of the remains sampled contained no identifiable human DNA (WLH15 and WLH55), whereas the Mungo Man (WLH3) sample contained no Aboriginal Australian DNA. KS8 reveals human mitochondrial sequences that differ from the previously inferred sequence. Instead, we recover a total of five modern European contaminants from Mungo Man (WLH3). We show that the remaining sample (WLH4) contains ∼1.4% human DNA, from which we assembled two complete mitochondrial genomes. One of these was a previously unidentified Aboriginal Australian haplotype belonging to haplogroup S2 that we sequenced to a high coverage. The other was a contaminating modern European mitochondrial haplotype. Although none of the sequences that we recovered matched those reported by Adcock et al., except a contaminant, these findings show the feasibility of obtaining important information from ancient Aboriginal Australian remains.},
   author = {Tim H. Heupink and Sankar Subramanian and Joanne L. Wright and Phillip Endicott and Michael Carrington Westaway and Leon Huynen and Walther Parson and Craig D. Millar and Eske Willerslev and David M. Lambert},
   doi = {10.1073/pnas.1521066113},
   isbn = {0027-8424},
   issn = {0027-8424},
   issue = {25},
   journal = {Proceedings of the National Academy of Sciences},
   pages = {6892-6897},
   pmid = {27274055},
   title = {Ancient mtDNA sequences from the First Australians revisited},
   volume = {113},
   url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1521066113},
   year = {2016},
}
@article{Lazaridis2017,
   abstract = {New genome-wide data for ancient, Bronze Age individuals, including Minoans, Mycenaeans, and southwestern Anatolians, show that Minoans and Mycenaeans were genetically very similar yet distinct, supporting the idea of continuity but not isolation in the history of populations of the Aegean.},
   author = {Iosif Lazaridis and Alissa Mittnik and Nick Patterson and Swapan Mallick and Nadin Rohland and Saskia Pfrengle and Anja Furtwängler and Alexander Peltzer and Cosimo Posth and Andonis Vasilakis and P. J.P. McGeorge and Eleni Konsolaki-Yannopoulou and George Korres and Holley Martlew and Manolis Michalodimitrakis and Mehmet Özsait and Nesrin Özsait and Anastasia Papathanasiou and Michael Richards and Songül Alpaslan Roodenberg and Yannis Tzedakis and Robert Arnott and Daniel M. Fernandes and Jeffery R. Hughey and Dimitra M. Lotakis and Patrick A. Navas and Yannis Maniatis and John A. Stamatoyannopoulos and Kristin Stewardson and Philipp Stockhammer and Ron Pinhasi and David Reich and Johannes Krause and George Stamatoyannopoulos},
   doi = {10.1038/nature23310},
   isbn = {0008-5472 (Print)\r0008-5472 (Linking)},
   issn = {14764687},
   issue = {7666},
   journal = {Nature},
   pages = {214-218},
   pmid = {28783727},
   title = {Genetic origins of the Minoans and Mycenaeans},
   volume = {548},
   year = {2017},
}
@article{Meyer2016,
   abstract = {A unique assemblage of 28 hominin individuals, found in Sima de los Huesos in the Sierra de Atapuerca in Spain, has recently been dated to approximately 430,000 years ago. An interesting question is how these Middle Pleistocene hominins were related to those who lived in the Late Pleistocene epoch, in particular to Neanderthals in western Eurasia and to Denisovans, a sister group of Neanderthals so far known only from southern Siberia. While the Sima de los Huesos hominins share some derived morphological features with Neanderthals, the mitochondrial genome retrieved from one individual from Sima de los Huesos is more closely related to the mitochondrial DNA of Denisovans than to that of Neanderthals. However, since the mitochondrial DNA does not reveal the full picture of relationships among populations, we have investigated DNA preservation in several individuals found at Sima de los Huesos. Here we recover nuclear DNA sequences from two specimens, which show that the Sima de los Huesos hominins were related to Neanderthals rather than to Denisovans, indicating that the population divergence between Neanderthals and Denisovans predates 430,000 years ago. A mitochondrial DNA recovered from one of the specimens shares the previously described relationship to Denisovan mitochondrial DNAs, suggesting, among other possibilities, that the mitochondrial DNA gene pool of Neanderthals turned over later in their history.  **hi ha vídeo: https://vimeo.com/158883454},
   author = {Matthias Meyer and Juan Luis Arsuaga and Cesare De Filippo and Sarah Nagel and Ayinuer Aximu-Petri and Birgit Nickel and Ignacio Martínez and Ana Gracia and José María Bermúdez De Castro and Eudald Carbonell and Bence Viola and Janet Kelso and Kay Prüfer and Svante Pääbo},
   doi = {10.1038/nature17405},
   isbn = {1476-4687},
   issn = {14764687},
   issue = {7595},
   journal = {Nature},
   pages = {504-507},
   pmid = {26976447},
   publisher = {Nature Publishing Group},
   title = {Nuclear DNA sequences from the Middle Pleistocene Sima de los Huesos hominins},
   volume = {531},
   url = {http://dx.doi.org/10.1038/nature17405},
   year = {2016},
}
@article{Noonan2006,
   abstract = {Our knowledge of Neanderthals is based on a limited number of remains and artifacts from which we must make inferences about their biology, behavior, and relationship to ourselves. Here, we describe the characterization of these extinct hominids from a new perspective, based on the development of a Neanderthal metagenomic library and its high-throughput sequencing and analysis. Several lines of evidence indicate that the 65,250 base pairs of hominid sequence so far identified in the library are of Neanderthal origin, the strongest being the ascertainment of sequence identities between Neanderthal and chimpanzee at sites where the human genomic sequence is different. These results enabled us to calculate the human-Neanderthal divergence time based on multiple randomly distributed autosomal loci. Our analyses suggest that on average the Neanderthal genomic sequence we obtained and the reference human genome sequence share a most recent common ancestor ~706,000 years ago, and that the human and Neanderthal ancestral populations split ~370,000 years ago, before the emergence of anatomically modern humans. Our finding that the Neanderthal and human genomes are at least 99.5% identical led us to develop and successfully implement a targeted method for recovering specific ancient DNA sequences from metagenomic libraries. This initial analysis of the Neanderthal genome advances our understanding of the evolutionary relationship of Homo sapiens and Homo neanderthalensis and signifies the dawn of Neanderthal genomics.},
   author = {James P Noonan and Graham Coop and Sridhar Kudaravalli and Doug Smith and Johannes Krause and Joe Alessi and Feng Chen and Darren Platt and Svante Pääbo and Jonathan K Pritchard and Edward M Rubin},
   doi = {10.1126/science.1131412},
   issn = {1095-9203},
   issue = {5802},
   journal = {Science},
   pages = {1113-1118},
   title = {Sequencing and analysis of Neanderthal genomic DNA},
   volume = {314},
   year = {2006},
}
@article{ElEzzi2010,
   abstract = {Congenital hypothyroidism is screened using blood spotted on filter paper that may be transported from remote areas to central testing facilities. However, storage conditions and transportation may affect sample quality.},
   author = {Asmahan A. El Ezzi and Mohammed A. El-Saidi and Ruhul H. Kuddus},
   doi = {10.1111/j.1442-200X.2010.03101.x},
   issn = {13288067},
   issue = {4},
   journal = {Pediatrics International},
   keywords = {DNA extraction,congenital hypothyroidism,dried blood spot,polymerase chain reaction,radioimmunoassay,thyrotropin,thyroxin},
   pages = {631-639},
   pmid = {20202157},
   title = {Long-term stability of thyroid hormones and DNA in blood spots kept under varying storage conditions},
   volume = {52},
   year = {2010},
}
@article{Krings1997,
   abstract = {DNA was extracted from the Neandertal-type specimen found in 1856 in western Germany. By sequencing clones from short overlapping PCR products, a hitherto unknown mitochondrial (mt) DNA sequence was determined. Multiple controls indicate that this sequence is endogenous to the fossil. Sequence comparisons with human mtDNA sequences, as well as phylogenetic analyses, show that the Neandertal sequence falls outside the variation of modern humans. Furthermore, the age of the common ancestor of the Neandertal and modern human mtDNAs is estimated to be four times greater than that of the common ancestor of human mtDNAs. This suggests that Neandertals went extinct without contributing mtDNA to modern humans.},
   author = {M Krings and Anne C Stone and R W Schmitz and H Krainitzki and M Stoneking and S Paabo},
   doi = {10.1016/S0092-8674(00)80310-4},
   isbn = {0092-8674 (Print)},
   issn = {614911677},
   issue = {1},
   journal = {Cell},
   pages = {19-30},
   pmid = {9230299},
   title = {Neandertal DNA sequences and the origin of modern humans.},
   volume = {90},
   url = {http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&id=9230299&retmode=ref&cmd=prlinks%5Cnpapers3://publication/doi/10.1016/S0092-8674(00)80310-4},
   year = {1997},
}
@article{Roder2010,
   abstract = {Real-time PCR is dependent upon a calibration function for quantification. While long-term storage of standards saves cost and time, solutions of DNA are prone to degradation. We present here the benchmark treatment for preservation of DNA standards, involving storage in 50% glycerol-double-distilled water, whereby a deviation of 0.2 threshold cycle (C(T)) values resulted after 100 days of storage.},
   author = {Barbara Röder and Karin Frühwirth and Claus Vogl and Martin Wagner and Peter Rossmanith},
   doi = {10.1128/JCM.01230-10},
   isbn = {1098-660X (Electronic)\r0095-1137 (Linking)},
   issn = {00951137},
   issue = {11},
   journal = {Journal of Clinical Microbiology},
   pages = {4260-4262},
   pmid = {20810770},
   title = {Impact of long-term storage on stability of standard DNA for nucleic acid-based methods},
   volume = {48},
   year = {2010},
}
@article{Anthonappa2013,
   abstract = {OBJECTIVES: The objectives of this paper are to determine the storage stability of saliva at 37 °C over an 18-month period, and its influence on the DNA yield, purity, PCR protocols and genotyping efficacy.\n\nMATERIALS AND METHODS: Of the 60 participants, blood samples were obtained from 10 and saliva from 50. Samples were subjected to different storage conditions: DNA extracted immediately; DNA extracted following storage at 37 °C for 1, 6, 12 and 18 months. Subsequently, DNA yield, OD(260/280) and OD(260/230) ratios were measured. The isolated DNA was used to amplify exons 0-7 of the RUNX2 gene and subsequently sequenced. Furthermore, 25 SNPs were genotyped.\n\nRESULTS: The mean DNA yield, OD(260/280) and OD(260/230) ratios obtained from blood were 67.4 ng/μl, 1.8 ± 0.05 and 1.8 ± 0.4 respectively. DNA yield obtained from saliva was significantly higher than blood (p < 0.0001), ranging from 97.4 to 125.8 ng/μl while the OD(260/280) ratio ranged from 1.8 ± 0.13 to 1.9 ± 0.1. The success rates for the 25 SNPs ranged from 98 to 100 % for blood and 96-99 % for saliva samples with the genotype frequencies in Hardy-Weinberg equilibrium (>0.01).\n\nCONCLUSIONS: Saliva can be stored at 37 °C for 18 months without compromising its quality and ability to endure genetic analyses.\n\nCLINICAL RELEVANCE: Saliva is a viable source of human DNA to facilitate the feasibility of large-scale genetic studies.},
   author = {Robert P. Anthonappa and Nigel M. King and A. Bakr M. Rabie},
   doi = {10.1007/s00784-012-0871-5},
   isbn = {0078401208},
   issn = {14326981},
   issue = {7},
   journal = {Clinical Oral Investigations},
   keywords = {Blood,DNA,RUNX2,Saliva},
   pages = {1719-1725},
   pmid = {23103961},
   title = {Evaluation of the long-term storage stability of saliva as a source of human DNA},
   volume = {17},
   year = {2013},
}
@misc{OpenXtra2018,
   author = {Open Xtra},
   title = {Recommended Server Room Temperature},
   url = {https://www.openxtra.co.uk/kb/recommended-server-room-temperature.html},
   year = {2018},
}
@misc{CropTrust2018,
   author = {Crop Trust},
   title = {Svalbard Global Seed Vault},
   url = {https://www.croptrust.org/our-work/svalbard-global-seed-vault/},
   year = {2018},
}
@article{Vijayaraghavan2010,
   abstract = {DNA was sol. and exhibited long-term stability in hydrated ionic liqs. based on choline dihydrogenphosphate, choline nitrate, or choline lactate. [on SciFinder(R)]},
   author = {Ranganathan Vijayaraghavan and Aleksey Izgorodin and Venkatraman Ganesh and Mahadevan Surianarayanan and Douglas R. MacFarlane},
   doi = {10.1002/anie.200906610},
   isbn = {1433-7851},
   issn = {14337851},
   issue = {9},
   journal = {Angewandte Chemie - International Edition},
   keywords = {DNA,Fluorescence,Helical structures,Ionic liquids,Solubility},
   pages = {1631-1633},
   pmid = {20108297},
   title = {Long-term structural and chemical stability of DNA in hydrated ionic liquids},
   volume = {49},
   year = {2010},
}
@article{Bonnet2009,
   abstract = {There is currently wide interest in room temperature storage of dehydrated DNA. However, there is insufficient knowledge about its chemical and structural stability. Here, we show that solid-state DNA degradation is greatly affected by atmospheric water and oxygen at room temperature. In these conditions DNA can even be lost by aggregation. These are major concerns since laboratory plastic ware is not airtight. Chain-breaking rates measured between 70 degrees C and 140 degrees C seemed to follow Arrhenius' law. Extrapolation to 25 degrees C gave a degradation rate of about 1-40 cuts/10(5) nucleotides/century. However, these figures are to be taken as very tentative since they depend on the validity of the extrapolation and the positive or negative effect of contaminants, buffers or additives. Regarding the secondary structure, denaturation experiments showed that DNA secondary structure could be preserved or fully restored upon rehydration, except possibly for small fragments. Indeed, below about 500 bp, DNA fragments underwent a very slow evolution (almost suppressed in the presence of trehalose) which could end in an irreversible denaturation. Thus, this work validates using room temperature for storage of DNA if completely protected from water and oxygen.},
   author = {Jacques Bonnet and Marthe Colotte and Delphine Coudy and Vincent Couallier and Joseph Portier and Bénédicte Morin and Sophie Tuffet},
   doi = {10.1093/nar/gkp1060},
   isbn = {1362-4962 (Electronic) 0305-1048 (Linking)},
   issn = {03051048},
   issue = {5},
   journal = {Nucleic Acids Research},
   pages = {1531-1546},
   pmid = {19969539},
   title = {Chain and conformation stability of solid-state DNA: Implications for room temperature storage},
   volume = {38},
   year = {2009},
}
@misc{DARPA2017,
   author = {DARPA},
   title = {Turning to Chemistry for New “Computing” Concepts},
   url = {https://www.darpa.mil/news-events/2017-03-23},
   year = {2017},
}
@misc{IlluminaInc.2018,
   author = {Illumina Inc.},
   title = {Sequencing and array-based solutions for genetic research},
   url = {https://www.illumina.com/},
   year = {2018},
}
@misc{PacificBiosciences2018,
   author = {Pacific Biosciences},
   title = {Home},
   url = {https://www.pacb.com/},
   year = {2018},
}
@misc{ThermoFisherScientific2018,
   author = {ThermoFisher Scientific},
   title = {Home},
   url = {https://www.thermofisher.com/us/en/home.html},
   year = {2018},
}
@article{Ranger2017,
   author = {Steve Ranger},
   journal = {ZDNet},
   month = {4},
   title = {Microsoft is buying another 10 million strands of DNA for storage research},
   url = {www.zdnet.com/article/microsoft-is-buying-another-10-million-strands-of-dna-for-storage-research/},
   year = {2017},
}
@misc{IlluminaInc.2018a,
   author = {Illumina Inc.},
   title = {Sequencing Platform Comparison Tool},
   url = {https://www.illumina.com/systems/sequencing-platforms/comparison-tool.html},
   year = {2018},
}
@article{Tilley2017,
   author = {Aaron Tilley},
   journal = {Forbes},
   month = {5},
   title = {HPE Has Constructed The Largest Single-Memory Computer System Ever Built},
   url = {www.forbes.com/sites/aarontilley/2017/05/16/hpe-160-terabytes-memory/#5e24ccf8383f},
   year = {2017},
}
@article{Mowbray2017,
   author = {Miranda Mowbray and William Horne and Prasad Rao},
   keywords = {bioinformatics,network security,pattern matching,regular expressions,s},
   title = {Efficient classification of strings using regular expressions},
   url = {https://www.labs.hpe.com/techreports/2017/HPE-2017-03.pdf},
   year = {2017},
}
@misc{Wetterstrand2018,
   author = {KA Wetterstrand},
   institution = {NIH: National Human Genome Research Institute},
   journal = {The NHGRI Genome Sequencing Program (GSP)},
   title = {DNA Sequencing Costs},
   url = {www.genome.gov/sequencingcostsdata},
   year = {2018},
}
@article{Zavodna2014,
   abstract = {To date we have little knowledge of how accurate next-generation sequencing (NGS) technologies are in sequencing repetitive sequences beyond known limitations to accurately sequence homopolymers. Only a handful of previous reports have evaluated the potential of NGS for sequencing short tandem repeats (microsatellites) and no empirical study has compared and evaluated the performance of more than one NGS platform with the same dataset. Here we examined yeast microsatellite variants from both long-read (454-sequencing) and short-read (Illumina) NGS platforms and compared these to data derived through Sanger sequencing. In addition, we investigated any locus-specific biases and differences that might have resulted from variability in microsatellite repeat number, repeat motif or type of mutation. Out of 112 insertion/deletion variants identified among 45 microsatellite amplicons in our study, we found 87.5% agreement between the 454-platform and Sanger sequencing in frequency of variant detection after Benjamini-Hochberg correction for multiple tests. For a subset of 21 microsatellite amplicons derived from Illumina sequencing, the results of short-read platform were highly consistent with the other two platforms, with 100% agreement with 454-sequencing and 93.6% agreement with the Sanger method after Benjamini-Hochberg correction. We found that the microsatellite attributes copy number, repeat motif and type of mutation did not have a significant effect on differences seen between the sequencing platforms. We show that both long-read and short-read NGS platforms can be used to sequence short tandem repeats accurately, which makes it feasible to consider the use of these platforms in high-throughput genotyping. It appears the major requirement for achieving both high accuracy and rare variant detection in microsatellite genotyping is sufficient read depth coverage. This might be a challenge because each platform generates a consistent pattern of non-uniform sequence coverage, which, as our study suggests, may affect some types of tandem repeats more than others.},
   author = {Monika Zavodna and Andrew Bagshaw and Rudiger Brauning and Neil J. Gemmell},
   doi = {10.1371/journal.pone.0113862},
   isbn = {1932-6203 (Electronic)\r1932-6203 (Linking)},
   issn = {19326203},
   issue = {12},
   journal = {PLoS ONE},
   pages = {1-14},
   pmid = {25436869},
   title = {The accuracy, feasibility and challenges of sequencing short tandem repeats using next-generation sequencing platforms},
   volume = {9},
   year = {2014},
}
@misc{Birren2001,
   abstract = {The human genome holds an extraordinary trove of information about human development, physiology, medicine and evolution. Here we report the results of an international collaboration to produce and make freely available a draft sequence of the human genome. We also present an initial analysis of the data, describing some of the insights that can be gleaned from the sequence. The rediscovery of Mendel's laws of heredity in the opening weeks of the 20th century 1±3 sparked a scienti®c quest to understand the nature and content of genetic information that has propelled biology for the last hundred years. The scienti®c progress made falls naturally into four main phases, corresponding roughly to the four quarters of the century. The ®rst established the cellular basis of heredity: the chromosomes. The second de®ned the molecular basis of heredity: the DNA double helix. The third unlocked the informa-tional basis of heredity, with the discovery of the biological mechanism by which cells read the information contained in genes and with the invention of the recombinant DNA technologies of cloning and sequencing by which scientists can do the same. The last quarter of a century has been marked by a relentless drive to decipher ®rst genes and then entire genomes, spawning the ®eld of genomics. The fruits of this work already include the genome sequences of 599 viruses and viroids, 205 naturally occurring plasmids, 185 organelles, 31 eubacteria, seven archaea, one fungus, two animals and one plant. Here we report the results of a collaboration involving 20 groups from the United States, China to produce a draft sequence of the human genome. The draft genome sequence was generated from a physical map covering more than 96% of the euchromatic part of the human genome and, together with additional sequence in public databases, it covers about 94% of the human genome. The sequence was produced over a relatively short period, with coverage rising from about 10% to more than 90% over roughly ®fteen months. The sequence data have been made available without restriction and updated daily throughout the project. The task ahead is to produce a ®nished sequence, by closing all gaps and resolving all ambiguities. Already about one billion bases are in ®nal form and the task of bringing the vast majority of the sequence to this standard is now straightforward and should proceed rapidly. The sequence of the human genome is of interest in several respects. It is the largest genome to be extensively sequenced so far, being 25 times as large as any previously sequenced genome and eight times as large as the sum of all such genomes. It is the ®rst vertebrate genome to be extensively sequenced. And, uniquely, it is the genome of our own species. Much work remains to be done to produce a complete ®nished sequence, but the vast trove of information that has become available through this collaborative effort allows a global perspective on the human genome. Although the details will change as the sequence is ®nished, many points are already clear. X The genomic landscape shows marked variation in the distribution of a number of features, including genes, transposable elements, GC content, CpG islands and recombination rate. This gives us important clues about function. For example, the developmentally important HOX gene clusters are the most repeat-poor regions of the human genome, probably re¯ecting the very complex coordinate regulation of the genes in the clusters. X There appear to be about 30,000±40,000 protein-coding genes in the human genomeÐonly about twice as many as in worm or ¯y. However, the genes are more complex, with more alternative splicing generating a larger number of protein products. X The full set of proteins (thèproteome') encoded by the human genome is more complex than those of invertebrates. This is due in part to the presence of vertebrate-speci®c protein domains and motifs (an estimated 7% of the total), but more to the fact that vertebrates appear to have arranged pre-existing components into a richer collection of domain architectures. X Hundreds of human genes appear likely to have resulted from horizontal transfer from bacteria at some point in the vertebrate lineage. Dozens of genes appear to have been derived from trans-posable elements. X Although about half of the human genome derives from trans-posable elements, there has been a marked decline in the overall activity of such elements in the hominid lineage. DNA transposons appear to have become completely inactive and long-terminal repeat (LTR) retroposons may also have done so. X The pericentromeric and subtelomeric regions of chromosomes are ®lled with large recent segmental duplications of sequence from elsewhere in the genome. Segmental duplication is much more frequent in humans than in yeast, ¯y or worm. X Analysis of the organization of Alu elements explains the long-standing mystery of their surprising genomic distribution, and suggests that there may be strong selection in favour of preferential retention of Alu elements in GC-rich regions and that thesèsel®sh' elements may bene®t their human hosts. X The mutation rate is about twice as high in male as in female meiosis, showing that most mutation occurs in males. X Cytogenetic analysis of the sequenced clones con®rms suggestions that large GC-poor regions are strongly correlated with`darkwith`dark G-bands' in karyotypes. X Recombination rates tend to be much higher in distal regions (around 20 megabases (Mb)) of chromosomes and on shorter chromosome arms in general, in a pattern that promotes the occurrence of at least one crossover per chromosome arm in each meiosis. X More than 1.4 million single nucleotide polymorphisms (SNPs) in the human genome have been identi®ed. This collection should allow the initiation of genome-wide linkage disequilibrium mapping of the genes in the human population. In this paper, we start by presenting background information on the project and describing the generation, assembly and evaluation of the draft genome sequence. We then focus on an initial analysis of the sequence itself: the broad chromosomal landscape; the repeat elements and the rich palaeontological record of evolutionary and biological processes that they provide; the human genes and proteins and their differences and similarities with those of other articles 860 NATURE | VOL 409 | 15 FEBRUARY 2001 | www.nature.com},
   author = {Bruce Birren and Chad Nusbaum and Michael C Zody and Jennifer Baldwin and Keri Devon and Ken Dewar and Michael Doyle and William FitzHugh and Roel Funke and Diane Gage and Katrina Harris and Andrew Heaford and John Howland and Lisa Kann and Jessica Lehoczky and Rosie LeVine and Paul McEwan and Kevin McKernan and James Meldrim and Jill P Mesirov and Cher Miranda and William Morris and Jerome Naylor and Christina Raymond and Mark Rosetti and Ralph Santos and Andrew Sheridan and Carrie Sougnez and Nicole Stange-Thomann and Nikola Stojanovic and Aravind Subramanian and Dudley Wyman and Jane Rogers and John Sulston and Rachael Ainscough and Stephan Beck and David Bentley and John Burton and Christopher Clee and Nigel Carter and Alan Coulson and Rebecca Deadman and Panos Deloukas and Andrew Dunham and Ian Dunham and Richard Durbin and Lisa French and Darren Grafham and Simon Gregory and Tim Hubbard and Sean Humphray and Adrienne Hunt and Matthew Jones and Christine Lloyd and Amanda McMurray and Lucy Matthews and Simon Mercer and Sarah Milne and James C Mullikin and Andrew Mungall and Robert Plumb and Mark Ross and Ratna Shownkeen and Sarah Sims and Robert H Waterston and Richard K Wilson and LaDeana W Hillier and John D McPherson and Marco A Marra and Elaine R Mardis and Lucinda A Fulton and Asif T Chinwalla and Kymberlie H Pepin and Warren R Gish and Stephanie L Chissoe and Michael C Wendl and Kim D Delehaunty and Tracie L Miner and Andrew Delehaunty and Jason B Kramer and Lisa L Cook and Robert S Fulton and Douglas L Johnson and Patrick J Minx and Sandra W Clifton and Trevor Hawkins and Elbert Branscomb and Paul Predki and Paul Richardson and Sarah Wenning and Tom Slezak and Norman Doggett and Jan-Fang Cheng and Anne Olsen and Susan Lucas and Christopher Elkin and Edward Uberbacher and Marvin Frazier and Richard A Gibbs and Donna M Muzny and Steven E Scherer and John B Bouck and Erica J Sodergren and Kim C Worley and Catherine M Rives and James H Gorrell and Michael L Metzker and Susan L Naylor and Raju S Kucherlapati and David L Nelson and George M Weinstock and Yoshiyuki Sakaki and Asao Fujiyama and Masahira Hattori and Tetsushi Yada and Atsushi Toyoda and Takehiko Itoh and Chiharu Kawagoe and Hidemi Watanabe and Yasushi Totoki and Todd Taylor and CNRS Umr- and Jean Weissenbach and Roland Heilig and William Saurin and Francois Artiguenave and Philippe Brottier and Thomas Bruls and Eric Pelletier and Catherine Robert and Patrick Wincker and Douglas R Smith and Lynn Doucette-Stamm and Marc Ruben and Keith Weinstock and Hong Mei Lee and JoAnn Dubois and Andre Â Rosenthal and Matthias Platzer and Gerald Nyakatura and Stefan Taudien and Andreas Rump and Huanming Yang and Jun Yu and Jian Wang and Guyang Huang and Jun Gu and Stanford Genome Technology Center and Ronald W Davis and Nancy A Federspiel and A Pia Abola and Michael J Proctor and Stanford Human Genome Center and Richard M Myers and Jeremy Schmutz and Mark Dickson and Jane Grimwood and David R Cox and Maynard V Olson and Rajinder Kaul and Christopher Raymond and Glen A Evans and Maria Athanasiou and Roger Schultz and Bruce A Roe and Feng Chen and Huaqin Pan and Juliane Ramser and Hans Lehrach and Richard Reinhardt and W Richard McCombie and Melissa de la Bastide and Neilay Dedhia and Helmut Blo È cker and Klaus Hornischer and Gabriele Nordsiek and Richa Agarwala and L Aravind and Jeffrey A Bailey and Alex Bateman and Ewan Birney and Peer Bork and Daniel G Brown and Christopher B Burge and Lorenzo Cerutti and Hsiu-Chuan Chen and Deanna Church and Michele Clamp and Richard R Copley and Tobias Doerks and Sean R Eddy and Evan E Eichler and Terrence S Furey and James Galagan and James G R Gilbert and Cyrus Harmon and Yoshihide Hayashizaki and David Haussler and Henning Hermjakob and Karsten Hokamp and Wonhee Jang and L Steven Johnson and Thomas A Jones and Simon Kasif and Arek Kaspryzk and Scot Kennedy and W James Kent and Paul Kitts and Eugene V Koonin and Ian Korf and David Kulp and Doron Lancet and Todd M Lowe and Aoife McLysaght and Tarjei Mikkelsen and John V Moran and Nicola Mulder and Victor J Pollara and Chris P Ponting and Greg Schuler and Jo È rg Schultz and Guy Slater and Arian F A Smit and Elia Stupka and Joseph Szustakowki and Danielle Thierry-Mieg and Jean Thierry-Mieg and Lukas Wagner and John Wallis and Raymond Wheeler and Alan Williams and Yuri I Wolf and Kenneth H Wolfe and Shiaw-Pyng Yang and Ru-Fang Yeh},
   journal = {NATURE},
   title = {Initial sequencing and analysis of the human genome International Human Genome Sequencing Consortium* The Sanger Centre: Beijing Genomics Institute/Human Genome Center},
   volume = {409},
   url = {www.nature.com},
   year = {2001},
}
@misc{OxfordNanopore2018,
   author = {Oxford Nanopore},
   title = {Oxford Nanopore Sequencing Products},
   url = {https://nanoporetech.com/products},
   year = {2018},
}
@misc{Carlson2017,
   author = {Rob Carlson},
   title = {Guesstimating the Size of the Global Array Synthesis Market},
   url = {http://www.synthesis.cc/synthesis/2017/8/guesstimating-the-size-of-the-global-array-synthesis-market?rq=storage},
   year = {2017},
}
@article{Biskeborn2018,
   abstract = {This paper highlights the development of tunnel magnetoresistive (TMR) sensors for magnetic tape recording applications. This has led to the introduction of a tape drives supporting a 15 TB native tape cartridge, currently the highest capacity available. Underscoring this development is the fact that the TMR sensors must run in continual contact with the tape media. This is contrasted with modern hard disk drive (hdd) sensors, which fly above the disk platters. Various challenges encountered in developing and deploying TMR are presented. In addition, advances to the write transducer are also discussed. Lastly, the authors show that future density scaling for tape recording, unlike that for hdd, is not facing limits imposed by photolithography or superparamagnetic physics, suggesting that cartridge capacity improvements of 4 to 6x will be achieved in the next 4 to 8 years.},
   author = {Robert G. Biskeborn and Robert E. Fontana and Calvin S. Lo and W. Stanley Czarnecki and Jason Liang and Icko E. T. Iben and Gary M. Decad and Venus A. Hipolito},
   doi = {10.1063/1.5007788},
   issn = {2158-3226},
   issue = {5},
   journal = {AIP Advances},
   keywords = {disc drives,hard discs,magnetic recording,magnetic sensors,magnetic tape storage,magnetoresistive devices,tape recorders,tunnelling magnetoresistance},
   month = {5},
   pages = {056511},
   publisher = {AIP Publishing LLC},
   title = {TMR tape drive for a 15 TB cartridge},
   volume = {8},
   url = {http://aip.scitation.org/doi/10.1063/1.5007788},
   year = {2018},
}
@article{Mining2005,
   author = {Statistical Data Mining},
   issue = {x},
   pages = {1-10},
   title = {STATS784 FC Terms Test Answers Instructions : Surname : First name :},
   year = {2005},
}
@article{Baker1999,
   abstract = {Abstract&nbsp;&nbsp; Using a biopsy dart system, samples of skin tissue were collected from southern right whales (Eubalaena australis) in 1995 on two wintering grounds, southwest Australia (n=20) and the Auckland Islands of New Zealand (n=20); and on offshore feeding grounds at Latitudes 40 to 43, south of Western Australia (n=5). A variable section of the mitochondrial DNA control-region (289 nucleotides) was amplified and sequenced from these 45 individuals (21 males, 20 females and 4 of unknown sex), distinguishing a total of seven unique sequences (i.e. mtDNA haplotypes). Two haplotypes were found on both wintering grounds (including a common type representing 45% of each sample), and five types were unique to only one wintering ground. An analysis of variance adapted for molecular information revealed significant genetic differentiation between the two wintering grounds (p=0.017). The feeding-ground sample was too small for statistical comparison with the wintering grounds, but included two haplotypes found only in the Auckland Islands as well as the common haplotype found on both wintering grounds. The nucleotide diversity and differentiation of mtDNA among the right whales was similar to that among humpback whales (Megaptera novaeangliae) from the same regions (Baker et al. 1998), but haplotype diversity was significantly reduced, perhaps as a result of more intensive hunting during the last century and continued illegal hunting during this century.},
   author = {C S Baker and N. J. Patenaude and J L Bannister and J Robins and H Kato},
   doi = {10.1007/s002270050519},
   issn = {0025-3162},
   issue = {1},
   journal = {Marine Biology},
   keywords = {GENETICS,RIGHT WHALE},
   pages = {1-7},
   title = {Distribution and diversity of mtDNA lineages among southern\nright whales (Eubalaena australis) from Australia and New Zealand},
   volume = {134},
   url = {http://www.springerlink.com/openurl.asp?genre=article&id=doi:10.1007/s002270050519},
   year = {1999},
}
@article{Of2000,
   author = {Analysis Of and Mitochondrial Dna and South Atlantic and Right Whales},
   issue = {July},
   pages = {545-558},
   title = {ANALYSIS OF MITOCHONDRIAL DNA DIVERSITY WITHIN AND BETWEEN NORTH},
   volume = {16},
   year = {2000},
}
@article{Carroll2011a,
   abstract = {During the last 2 centuries, southern right whales Eubalaena australis were hunted to near extinction, and an estimated 150000 were killed by pre-industrial whaling in the 19th century and ille- gal Soviet whaling in the 20th century. Here we focus on the coastal calving grounds of Australia and New Zealand (NZ), where previous work suggests 2 genetically distinct stocks of southern right whales are recovering. Historical migration patterns and spatially variable patterns of recovery suggest each of these stocks are subdivided into 2 stocks: (1) NZ, comprising NZ subantarctic (NZSA) and mainland NZ (MNZ) stocks; and (2) Australia, comprising southwest and southeast stocks. We expand upon previous work to investigate population subdivision by analysing over 1000 samples collected at 6 locations across NZ and Australia, although sample sizes were small from some locations. Mitochondrial DNA (mtDNA) control region haplotypes (500 bp) and microsatellite genotypes (13 loci) were used to identify 707 indi- vidual whales and to test for genetic differentiation. For the first time, we documented the movement of 7 individual whales between the NZSA and MNZ based on the matching of multilocus genotypes. Given the current and historical evidence, we hypothesise that individuals from the NZ subantarctic are slowly recolonising MNZ, where a former calving ground was extirpated. We also suggest that southeast Aus- tralian right whales represent a remnant stock, distinct from the southwest Australian stock, based on significant differentiation in mtDNA haplotype frequencies (FST = 0.15, p < 0.01; ΦST = 0.12, p = 0.02) and contrasting patterns of recovery. In comparison with significant differences in mtDNA haplotype frequencies found between the 3 proposed stocks (overall FST = 0.07, ΦST = 0.12, p < 0.001), we found no significant differentiation in microsatellite loci (overall FST = 0.004, G’ST = 0.019, p = 0.07), suggesting ongoing or recent historical reproductive interchange.},
   author = {E. Carroll and N. J. Patenaude and A. Alexander and Debbie Steel and Robert G. Harcourt and S. Childerhouse and S. Smith and J. Bannister and Rochelle Constantine and Charles Scott Baker},
   doi = {10.3354/meps09145},
   isbn = {0022701117},
   issn = {0171-8630},
   issue = {1998},
   journal = {Marine Ecology Progress Series},
   keywords = {Eubalaena australis,Microsatellite,Population structure,Southern right whale,mtDNA},
   pages = {257-268},
   title = {Population structure and individual movement of southern right whales around New Zealand and Australia},
   volume = {432},
   url = {http://www.int-res.com/prepress/m09145.html},
   year = {2011},
}
@article{Risso2017,
   author = {Davide Risso},
   title = {EDASeq : Exploratory Data Analysis and Nor- malization for RNA-Seq},
   year = {2017},
}
@article{Baker2012,
   abstract = {Ammonia-oxidizing Archaea (AOA) are among the most abundant microorganisms in the oceans and have crucial roles in biogeochemical cycling of nitrogen and carbon. To better understand AOA inhabiting the deep sea, we obtained community genomic and transcriptomic data from ammonium-rich hydrothermal plumes in the Guaymas Basin (GB) and from surrounding deep waters of the Gulf of California. Among the most abundant and active lineages in the sequence data were marine group I (MGI) Archaea related to the cultured autotrophic ammonia-oxidizer, Nitrosopumilus maritimus. Assembly of MGI genomic fragments yielded 2.9 Mb of sequence containing seven 16S rRNA genes (95.4-98.4% similar to N. maritimus), including two near-complete genomes and several lower-abundance variants. Equal copy numbers of MGI 16S rRNA genes and ammonia monooxygenase genes and transcription of ammonia oxidation genes indicates that all of these genotypes actively oxidize ammonia. De novo genomic assembly revealed the functional potential of MGI populations and enhanced interpretation of metatranscriptomic data. Physiological distinction from N. maritimus is evident in the transcription of novel genes, including genes for urea utilization, suggesting an alternative source of ammonia. We were also able to determine which genotypes are most active in the plume. Transcripts involved in nitrification were more prominent in the plume and were among the most abundant transcripts in the community. These unique data sets reveal populations of deep-sea AOA thriving in the ammonium-rich GB that are related to surface types, but with key genomic and physiological differences.},
   author = {Brett J. Baker and Ryan A. Lesniewski and Gregory J. Dick},
   doi = {10.1038/ismej.2012.64},
   isbn = {1751-7370 (Electronic)\n1751-7362 (Linking)},
   issn = {17517362},
   issue = {12},
   journal = {ISME Journal},
   keywords = {ammonia,archaea,metagenomics,metatranscriptomics,nitrification},
   pages = {2269-2279},
   pmid = {22695863},
   title = {Genome-enabled transcriptomics reveals archaeal populations that drive nitrification in a deep-sea hydrothermal plume},
   volume = {6},
   year = {2012},
}
@article{Moritz2004,
   abstract = {Fixed and invariant characteristiclike a on a the general utility of mtDNA across different online; Stoeckle M (2003) Taxonomy, and the},
   author = {Craig Moritz and Carla Cicero},
   doi = {10.1371/journal.pbio.0020354},
   isbn = {1544-9173\r1545-7885},
   issn = {15449173},
   issue = {10},
   journal = {PLoS Biology},
   pmid = {15486587},
   title = {DNA barcoding: Promise and pitfalls},
   volume = {2},
   year = {2004},
}
@article{Essays1994,
   author = {T H Issue Essays},
   pages = {373-375},
   title = {Defining ‘ Evolutionarily Significant Units ’},
   year = {1994},
}
@article{Dunn2003,
   abstract = {In most countries, the scientific community simply is not prepared to allocate significant resources to taxonomic research of poorly known organism groups, not even from funds dedicated to biodiversity research [Statistics on Swedish},
   author = {Christopher P. Dunn},
   doi = {10.1016/S0169-5347(03)00094-6},
   isbn = {0169-5347},
   issn = {01695347},
   issue = {6},
   journal = {Trends in Ecology and Evolution},
   pages = {270-271},
   pmid = {2374},
   title = {Keeping taxonomy based in morphology [3]},
   volume = {18},
   year = {2003},
}
@article{Rosenbaum2000,
   abstract = {Few studies have examined systematic relationships of right whales (Eubalaena spp.) since the original species descriptions, even though they are one of the most endangered large whales. Little morphological evidence exists to support the current species designa-tions for Eubalaena glacialis in the northern hemisphere and E. australis in the southern hemisphere. Differences in migratory behaviour or antitropical distribution between right whales in each hemisphere are considered a barrier to gene flow and maintain the current species distinctions and geographical populations. However, these distinctions between populations have remained controversial and no study has included an analysis of all right whales from the three major ocean basins. To address issues of genetic dif-ferentiation and relationships among right whales, we have compiled a database of mito-chondrial DNA control region sequences from right whales representing populations in all three ocean basins that consist of: western North Atlantic E. glacialis , multiple geographically distributed populations of E. australis and the first molecular analysis of historical and recent samples of E. glacialis from the western and eastern North Pacific Ocean. Diagnostic characters, as well as phylogenetic and phylogeographic analyses, support the possibility that three distinct maternal lineages exist in right whales, with North Pacific E. glacialis being more closely related to E. australis than to North Atlantic E. glacialis . Our genetic results provide unequivocal character support for the two usually recognized species and a third distinct genetic lineage in the North Pacific under the Phylogenetic Species Concept, as well as levels of genetic diversity among right whales world-wide.},
   author = {H. C. Rosenbaum and R. L. Brownell and M. W. Brown and C. Schaeff and V. Portway and B. N. White and S. Malik and L. A. Pastene and N. J. Patenaude and C. S. Baker and M. Goto and P. B. Best and P. J. Clapham and P. Hamilton and M. Moore and R. Payne and V. Rowntree and C. T. Tynan and J. L. Bannister and R. Desalle},
   doi = {10.1046/j.1365-294X.2000.01066.x},
   isbn = {0962-1083},
   issn = {09621083},
   issue = {11},
   journal = {Molecular Ecology},
   keywords = {Cetaceans,Conservation,Phylogeny,Right whales,Taxonomy},
   pages = {1793-1802},
   pmid = {11091315},
   title = {World-wide genetic differentiation of Eubalaena: Questioning the number of right whale species},
   volume = {9},
   year = {2000},
}
@article{Rosenbaum1997,
   abstract = {DNA was isolated from an early twentieth century museum specimen of northern right whale baleen. A system of stringent controls and a novel set of cetacean specific primers eliminated contamination from external sources and ensured the authenticity of the results. Sequence analysis revealed that there were informative nucleotide positions between the museum specimen and extant members of the population and closely related species. The results indicate that museum specimens of baleen can be used to assess historical genetic population structure of the great whales.},
   author = {H. C. Rosenbaum and M. G. Egan and P. J. Clapham and R. L. Brownell and R. Desalle},
   doi = {10.1046/j.1365-294X.1997.00230.x},
   isbn = {0962-1083},
   issn = {09621083},
   issue = {7},
   journal = {Molecular Ecology},
   keywords = {Ancient DNA,Cetacea,Control region,Eubalaena glacialis,Historical population genetics,Mitochondrial DNA,Museum specimens},
   pages = {677-681},
   pmid = {9226948},
   title = {An effective method for isolating DNA from historical specimens of baleen},
   volume = {6},
   year = {1997},
}
@article{Rossberg2013,
   author = {Axel G Rossberg and Tim Rogers and Alan J Mckane and Axel G Rossberg and Tim Rogers and Alan J Mckane},
   issue = {July},
   keywords = {ecology,evolution,theoretical biology},
   title = {Are there species smaller than 1 mm ? Are there species smaller than 1 mm ?},
   year = {2013},
}
@article{Patenaude2007,
   abstract = {The population structure and mitochondrial (mt) DNA diversity of southern right whales (Eubalaena australis) are described from 146 individuals sampled on 4 winter calving grounds (Argentina, South Africa, Western Australia, and the New Zealand sub-Antarctic) and 2 summer feeding grounds (South Georgia and south of Western Australia). Based on a consensus region of 275 base pairs of the mtDNA control region, 37 variable sites defined 37 unique haplotypes, of which only one was shared between regional samples of the Indo-Pacific and South Atlantic Oceans. Phylogenetic reconstruction of the southern right whale haplotypes revealed 2 distinct clades that differed significantly in frequencies between oceans. An analysis of molecular variance confirmed significant overall differentiation among the 4 calving grounds at both the haplotype and the nucleotype levels (FST = 0.159; \{Phi\}ST = 0.238; P < 0.001). Haplotype diversity was significantly lower in the Indo-Pacific (h = 0.701 \{+/-\} 0.037) compared with the South Atlantic (h = 0.948 \{+/-\} 0.013), despite a longer history of exploitation and larger catches in the South Atlantic. In fact, the haplotype diversity in the Indo-Pacific basin was similar to that of the North Atlantic right whale that currently numbers about 300 animals. Multidimensional scaling of genetic differentiation suggests that gene flow occurred primarily between adjacent calving grounds within an ocean basin, with mixing of lineages from different calving grounds occurring on feeding grounds},
   author = {N. J. Patenaude and Vicky A. Portway and Cathy M. Schaeff and John L. Bannister and Peter B. Best and Roger S. Payne and Vicky J. Rowntree and Mariana Rivarola and C. Scott Baker},
   doi = {10.1093/jhered/esm005},
   isbn = {0022-1503},
   issn = {00221503},
   issue = {2},
   journal = {Journal of Heredity},
   pages = {147-157},
   pmid = {1569},
   title = {Mitochondrial DNA diversity and population structure among southern right whales (Eubalaena australis)},
   volume = {98},
   year = {2007},
}
@article{Guide2013,
   author = {Metagenome Assembly Guide},
   title = {Metagenome Assembly Guide},
   year = {2013},
}
@article{Mutschler2015,
   abstract = {The emergence of an RNA catalyst capable of self-replication is considered a key transition in the origin of life. However, how such replicase ribozymes emerged from the pools of short RNA oligomers arising from prebiotic chemistry and non-enzymatic replication is unclear. Here we show that RNA polymerase ribozymes can assemble from simple catalytic networks of RNA oligomers no longer than 30 nucleotides. The entropically disfavoured assembly reaction is driven by iterative freeze-thaw cycles, even in the absence of external activation chemistry. The steep temperature and concentration gradients of such cycles result in an RNA chaperone effect that enhances the otherwise only partially realized catalytic potential of the RNA oligomer pool by an order of magnitude. Our work outlines how cyclic physicochemical processes could have driven an expansion of RNA compositional and phenotypic complexity from simple oligomer pools.},
   author = {Hannes Mutschler and Aniela Wochner and Philipp Holliger},
   doi = {10.1038/nchem.2251},
   isbn = {1755-4330},
   issn = {17554349},
   issue = {6},
   journal = {Nature Chemistry},
   pages = {502-508},
   pmid = {25991529},
   title = {Freeze-thaw cycles as drivers of complex ribozyme assembly},
   volume = {7},
   year = {2015},
}
@article{Lehmann2007,
   abstract = {RNA polymerase (Pol) II catalyses DNA-dependent RNA synthesis during gene transcription. There is, however, evidence that Pol II also possesses RNA-dependent RNA polymerase (RdRP) activity. Pol II can use a homopolymeric RNA template, can extend RNA by several nucleotides in the absence of DNA, and has been implicated in the replication of the RNA genomes of hepatitis delta virus (HDV) and plant viroids. Here we show the intrinsic RdRP activity of Pol II with only pure polymerase, an RNA template-product scaffold and nucleoside triphosphates (NTPs). Crystallography reveals the template-product duplex in the site occupied by the DNA-RNA hybrid during transcription. RdRP activity resides at the active site used during transcription, but it is slower and less processive than DNA-dependent activity. RdRP activity is also obtained with part of the HDV antigenome. The complex of transcription factor IIS (TFIIS) with Pol II can cleave one HDV strand, create a reactive stem-loop in the hybrid site, and extend the new RNA 3' end. Short RNA stem-loops with a 5' extension suffice for activity, but their growth to a critical length apparently impairs processivity. The RdRP activity of Pol II provides a missing link in molecular evolution, because it suggests that Pol II evolved from an ancient replicase that duplicated RNA genomes.},
   author = {Elisabeth Lehmann and Florian Brueckner and Patrick Cramer},
   doi = {10.1038/nature06290},
   isbn = {1476-4687 (Electronic)},
   issn = {14764687},
   issue = {7168},
   journal = {Nature},
   pages = {445-449},
   pmid = {18004386},
   title = {Molecular basis of RNA-dependent RNA polymerase II activity},
   volume = {450},
   year = {2007},
}
@article{Takeuchi2018f,
   author = {Nobuto Takeuchi},
   title = {BIOINF703 : Origin of Genomes},
   year = {2018},
}
@article{Takeuchi2018d,
   author = {Nobuto Takeuchi},
   title = {BIOINF703 : Origin of Genomes},
   year = {2018},
}
@article{Nielsen2013,
   author = {Per H Nielsen},
   title = {This PDF file includes:},
   year = {2013},
}
@article{Albertsen2013,
   abstract = {Reference genomes are required to understand the diverse roles of microorganisms in ecology, evolution, human and animal health, but most species remain uncultured. Here we present a sequence composition-independent approach to recover high-quality microbial genomes from deeply sequenced metagenomes. Multiple metagenomes of the same community, which differ in relative population abundances, were used to assemble 31 bacterial genomes, including rare (<1% relative abundance) species, from an activated sludge bioreactor. Twelve genomes were assembled into complete or near-complete chromosomes. Four belong to the candidate bacterial phylum TM7 and represent the most complete genomes for this phylum to date (relative abundances, 0.06-1.58%). Reanalysis of published metagenomes reveals that differential coverage binning facilitates recovery of more complete and higher fidelity genome bins than other currently used methods, which are primarily based on sequence composition. This approach will be an important addition to the standard metagenome toolbox and greatly improve access to genomes of uncultured microorganisms.},
   author = {Mads Albertsen and Philip Hugenholtz and Adam Skarshewski and Kåre L. Nielsen and Gene W. Tyson and Per H. Nielsen},
   doi = {10.1038/nbt.2579},
   isbn = {1546-1696 (Electronic)\r1087-0156 (Linking)},
   issn = {10870156},
   issue = {6},
   journal = {Nature Biotechnology},
   pages = {533-538},
   pmid = {23707974},
   title = {Genome sequences of rare, uncultured bacteria obtained by differential coverage binning of multiple metagenomes},
   volume = {31},
   year = {2013},
}
@article{Bendall2016,
   abstract = {Multiple models describe the formation and evolution of distinct microbial phylogenetic groups. These evolutionary models make different predictions regarding how adaptive alleles spread through populations and how genetic diversity is maintained. Processes predicted by competing evolutionary models, for example, genome-wide selective sweeps vs gene-specific sweeps, could be captured in natural populations using time-series metagenomics if the approach were applied over a sufficiently long time frame. Direct observations of either process would help resolve how distinct microbial groups evolve. Here, from a 9-year metagenomic study of a freshwater lake (2005–2013), we explore changes in single-nucleotide polymorphism (SNP) frequencies and patterns of gene gain and loss in 30 bacterial populations. SNP analyses revealed substantial genetic heterogeneity within these populations, although the degree of heterogeneity varied by >1000-fold among populations. SNP allele frequencies also changed dramatically over time within some populations. Interestingly, nearly all SNP variants were slowly purged over several years from one population of green sulfur bacteria, while at the same time multiple genes either swept through or were lost from this population. These patterns were consistent with a genome-wide selective sweep in progress, a process predicted by the ‘ecotype model’ of speciation but not previously observed in nature. In contrast, other populations contained large, SNP-free genomic regions that appear to have swept independently through the populations prior to the study without purging diversity elsewhere in the genome. Evidence for both genome-wide and gene-specific sweeps suggests that different models of bacterial speciation may apply to different populations coexisting in the same environment.},
   author = {Matthew L. Bendall and Sarah L.R. Stevens and Leong Keat Chan and Stephanie Malfatti and Patrick Schwientek and Julien Tremblay and Wendy Schackwitz and Joel Martin and Amrita Pati and Brian Bushnell and Jeff Froula and Dongwan Kang and Susannah G. Tringe and Stefan Bertilsson and Mary A. Moran and Ashley Shade and Ryan J. Newton and Katherine D. McMahon and Rex R. Malmstrom},
   doi = {10.1038/ismej.2015.241},
   isbn = {1751-7362},
   issn = {17517370},
   issue = {7},
   journal = {ISME Journal},
   pages = {1589-1601},
   pmid = {26744812},
   publisher = {Nature Publishing Group},
   title = {Genome-wide selective sweeps and gene-specific sweeps in natural bacterial populations},
   volume = {10},
   url = {http://dx.doi.org/10.1038/ismej.2015.241},
   year = {2016},
}
@article{Sharon2013,
   abstract = {The gastrointestinal microbiome undergoes shifts in species and strain abundances, yet dynamics involving closely related microorganisms remain largely unknown because most methods cannot resolve them. We developed new metagenomic methods and utilized them to track species and strain level variations in microbial communities in 11 fecal samples collected from a premature infant during the first month of life. Ninety six percent of the sequencing reads were assembled into scaffolds of >500 bp in length that could be assigned to organisms at the strain level. Six essentially complete (∼99%) and two near-complete genomes were assembled for bacteria that comprised as little as 1% of the community, as well as nine partial genomes of bacteria representing as little as 0.05%. In addition, three viral genomes were assembled and assigned to their hosts. The relative abundance of three Staphylococcus epidermidis strains, as well as three phages that infect them, changed dramatically over time. Genes possibly related to these shifts include those for resistance to antibiotics, heavy metals, and phage. At the species level, we observed the decline of an early-colonizing Propionibacterium acnes strain similar to SK137 and the proliferation of novel Propionibacterium and Peptoniphilus species late in colonization. The Propionibacterium species differed in their ability to metabolize carbon compounds such as inositol and sialic acid, indicating that shifts in species composition likely impact the metabolic potential of the community. These results highlight the benefit of reconstructing complete genomes from metagenomic data and demonstrate methods for achieving this goal.},
   author = {Itai Sharon and Michael J. Morowitz and Brian C. Thomas and Elizabeth K. Costello and David A. Relman and Jillian F. Banfield},
   doi = {10.1101/gr.142315.112},
   isbn = {1549-5469 (Electronic)\n1088-9051 (Linking)},
   issn = {10889051},
   issue = {1},
   journal = {Genome Research},
   pages = {111-120},
   pmid = {22936250},
   title = {Time series community genomics analysis reveals rapid shifts in bacterial species, strains, and phage during infant gut colonization},
   volume = {23},
   year = {2013},
}
@article{Baker2015,
   abstract = {BACKGROUND: Estuaries are among the most productive habitats on the planet. Bacteria in estuary sediments control the turnover of organic carbon and the cycling of nitrogen and sulfur. These communities are complex and primarily made up of uncultured lineages, thus little is known about how ecological and metabolic processes are partitioned in sediments.\n\nRESULTS: De novo assembly and binning resulted in the reconstruction of 82 bacterial genomes from different redox regimes of estuary sediments. These genomes belong to 23 bacterial groups, including uncultured candidate phyla (for example, KSB1, TA06, and KD3-62) and three newly described phyla (White Oak River (WOR)-1, WOR-2, and WOR-3). The uncultured phyla are generally most abundant in the sulfate-methane transition (SMTZ) and methane-rich zones, and genomic data predict that they mediate essential biogeochemical processes of the estuarine environment, including organic carbon degradation and fermentation. Among the most abundant organisms in the sulfate-rich layer are novel Gammaproteobacteria that have genes for the oxidation of sulfur and the reduction of nitrate and nitrite. Interestingly, the terminal steps of denitrification (NO3 to N2O and then N2O to N2) are present in distinct bacterial populations.\n\nCONCLUSIONS: This dataset extends our knowledge of the metabolic potential of several uncultured phyla. Within the sediments, there is redundancy in the genomic potential in different lineages, often distinct phyla, for essential biogeochemical processes. We were able to chart the flow of carbon and nutrients through the multiple geochemical layers of bacterial processing and reveal potential ecological interactions within the communities.},
   author = {Brett J Baker and Cassandre Sara Lazar and Andreas P Teske and Gregory J Dick},
   doi = {10.1186/s40168-015-0077-6},
   isbn = {2049-2618},
   issn = {2049-2618},
   issue = {1},
   journal = {Microbiome},
   keywords = {Estuary,Sediment,Metagenome,Sulfur,Nitrogen,Carbon,anaerobic respiration,candidate phyla,carbon,estuary,metagenome,nitrogen,sediment,sulfur},
   pages = {14},
   pmid = {25922666},
   publisher = {???},
   title = {Genomic resolution of linkages in carbon, nitrogen, and sulfur cycling among widespread estuary sediment bacteria},
   volume = {3},
   url = {http://www.microbiomejournal.com/content/3/1/14},
   year = {2015},
}
@article{Sciences2004,
   author = {Planetary Sciences and Walnut Creek},
   issue = {March},
   pages = {1-6},
   title = {Community structure and metabolism through reconstruction of microbial genomes from the environment Acid Mine Drainage ( AMD ) AMD in Fireway region of Richmond mine at Iron Mountain , CA Pink Biofilm},
   volume = {428},
   year = {2004},
}
@article{Kantor2013,
   abstract = {UNLABELLED: Cultivation-independent surveys of microbial diversity have revealed many bacterial phyla that lack cultured representatives. These lineages, referred to as candidate phyla, have been detected across many environments. Here, we deeply sequenced microbial communities from acetate-stimulated aquifer sediment to recover the complete and essentially complete genomes of single representatives of the candidate phyla SR1, WWE3, TM7, and OD1. All four of these genomes are very small, 0.7 to 1.2 Mbp, and have large inventories of novel proteins. Additionally, all lack identifiable biosynthetic pathways for several key metabolites. The SR1 genome uses the UGA codon to encode glycine, and the same codon is very rare in the OD1 genome, suggesting that the OD1 organism could also transition to alternate coding. Interestingly, the relative abundance of the members of SR1 increased with the appearance of sulfide in groundwater, a pattern mirrored by a member of the phylum Tenericutes. All four genomes encode type IV pili, which may be involved in interorganism interaction. On the basis of these results and other recently published research, metabolic dependence on other organisms may be widely distributed across multiple bacterial candidate phyla.\n\nIMPORTANCE: Few or no genomic sequences exist for members of the numerous bacterial phyla lacking cultivated representatives, making it difficult to assess their roles in the environment. This paper presents three complete and one essentially complete genomes of members of four candidate phyla, documents consistently small genome size, and predicts metabolic capabilities on the basis of gene content. These metagenomic analyses expand our view of a lifestyle apparently common across these candidate phyla.},
   author = {Rose S. Kantor and Kelly C. Wrighton and Kim M. Handley and Itai Sharon and Laura A. Hug and Cindy J. Castelle and Brian C. Thomas and Jillian F. Banfield},
   doi = {10.1128/mBio.00708-13},
   isbn = {2150-7511 (Electronic)},
   issn = {21612129},
   issue = {5},
   journal = {mBio},
   pmid = {24149512},
   title = {Small genomes and sparse metabolisms of sediment-associated bacteria from four candidate phyla},
   volume = {4},
   year = {2013},
}
@article{Robinson2009,
   abstract = {Summary: It is expected that emerging digital gene expression (DGE) technologies will overtake microarray technologies in the near future for many functional genomics applications. One of the fundamental data analysis tasks, especially for gene expression studies, involves determining whether there is evidence that counts for a transcript or exon are significantly different across experimental conditions. edgeR is a Bioconductor software package for examining differential expression of replicated count data. An over-dispersed Poisson model is used to account for both biological and technical variability. Empirical Bayes methods are used to moderate the degree of over-dispersion across transcripts, improving the reliability of inference. The methodology can be used even with the most minimal levels of replication, provided at least one phenotype or experimental condition is replicated. The software may have other applications beyond sequencing data, such as proteome peptide count data. Availability: The package is freely available under the LGPL licence from the Bioconductor web site (http://bioconductor.org). Contact: mrobinson@wehi.edu.au},
   author = {Mark D. Robinson and Davis J. McCarthy and Gordon K. Smyth},
   doi = {10.1093/bioinformatics/btp616},
   isbn = {1367-4811 (Electronic)\n1367-4803 (Linking)},
   issn = {14602059},
   issue = {1},
   journal = {Bioinformatics},
   pages = {139-140},
   pmid = {19910308},
   title = {edgeR: A Bioconductor package for differential expression analysis of digital gene expression data},
   volume = {26},
   year = {2009},
}
@article{Dick2009,
   abstract = {BACKGROUND: Analyses of DNA sequences from cultivated microorganisms have revealed genome-wide, taxa-specific nucleotide compositional characteristics, referred to as genome signatures. These signatures have far-reaching implications for understanding genome evolution and potential application in classification of metagenomic sequence fragments. However, little is known regarding the distribution of genome signatures in natural microbial communities or the extent to which environmental factors shape them.\n\nRESULTS: We analyzed metagenomic sequence data from two acidophilic biofilm communities, including composite genomes reconstructed for nine archaea, three bacteria, and numerous associated viruses, as well as thousands of unassigned fragments from strain variants and low-abundance organisms. Genome signatures, in the form of tetranucleotide frequencies analyzed by emergent self-organizing maps, segregated sequences from all known populations sharing < 50 to 60% average amino acid identity and revealed previously unknown genomic clusters corresponding to low-abundance organisms and a putative plasmid. Signatures were pervasive genome-wide. Clusters were resolved because intra-genome differences resulting from translational selection or protein adaptation to the intracellular (pH approximately 5) versus extracellular (pH approximately 1) environment were small relative to inter-genome differences. We found that these genome signatures stem from multiple influences but are primarily manifested through codon composition, which we propose is the result of genome-specific mutational biases.\n\nCONCLUSIONS: An important conclusion is that shared environmental pressures and interactions among coevolving organisms do not obscure genome signatures in acid mine drainage communities. Thus, genome signatures can be used to assign sequence fragments to populations, an essential prerequisite if metagenomics is to provide ecological and biochemical insights into the functioning of microbial communities.},
   author = {Gregory J. Dick and Anders F. Andersson and Brett J. Baker and Sheri L. Simmons and Brian C. Thomas and A. Pepper Yelton and Jillian F. Banfield},
   doi = {10.1186/gb-2009-10-8-r85},
   isbn = {1474-760X},
   issn = {14747596},
   issue = {8},
   journal = {Genome Biology},
   pmid = {19698104},
   title = {Community-wide analysis of microbial genome sequence signatures},
   volume = {10},
   year = {2009},
}
@article{Genomics,
   author = {Environmental Genomics},
   title = {Using metagenomics to understand complex interactions between organism and environment},
}
@article{Bentley2004,
   abstract = {▪ Abstract Recent advances in DNA-sequencing technologies have made available an enormous resource of data for the study of bacterial genomes. The broad sample of complete genomes currently available allows us to look at variation in the gross features and characteristics of genomes while the detail of the sequences reveal some of the mechanisms by which these genomes evolve. This review aims to describe bacterial genome structures according to current knowledge and proposed hypotheses. We also describe examples where mechanisms of genome evolution have acted in the adaptation of bacterial species to particular niches.},
   author = {Stephen D. Bentley and Julian Parkhill},
   doi = {10.1146/annurev.genet.38.072902.094318},
   isbn = {0066-4197 (Print)\n0066-4197 (Linking)},
   issn = {0066-4197},
   issue = {1},
   journal = {Annual Review of Genetics},
   keywords = {abstract recent advances in,an enormous resource of,available,available allows us to,bacteria,data for the study,dna-sequencing technologies have made,evolution,genome,look at variation in,of bacterial genomes,of complete genomes currently,rearrangement,structure,the broad sample,the gross},
   pages = {771-791},
   pmid = {15568993},
   title = {Comparative Genomic Structure of Prokaryotes},
   volume = {38},
   url = {http://www.annualreviews.org/doi/10.1146/annurev.genet.38.072902.094318},
   year = {2004},
}
@article{Handley2017,
   abstract = {Metabolic and spatio-taxonomic response of uncultivated seafloor bacteria following the Deepwater Horizon oil spill},
   author = {K. M. Handley and Y. M. Piceno and P. Hu and L. M. Tom and O. U. Mason and G. L. Andersen and J. K. Jansson and J. A. Gilbert},
   doi = {10.1038/ismej.2017.110},
   isbn = {1751-7362 1751-7370},
   issn = {17517370},
   issue = {11},
   journal = {ISME Journal},
   pages = {2569-2583},
   pmid = {28777379},
   title = {Metabolic and spatio-taxonomic response of uncultivated seafloor bacteria following the Deepwater Horizon oil spill},
   volume = {11},
   year = {2017},
}
@article{Chen2014,
   author = {Yunshun Chen and Davis Mccarthy and Mark Robinson and Gordon K Smyth},
   issue = {September},
   title = {of digital gene expression data User ’ s Guide},
   year = {2014},
}
@article{huelsenbeck2001mrbayes,
   author = {John P Huelsenbeck and Fredrik Ronquist},
   issue = {8},
   journal = {Bioinformatics},
   pages = {754-755},
   publisher = {Oxford University Press},
   title = {MRBAYES: Bayesian inference of phylogenetic trees},
   volume = {17},
   year = {2001},
}
@article{excoffier2005arlequin,
   author = {Laurent Excoffier and Guillaume Laval and Stefan Schneider},
   journal = {Evolutionary bioinformatics},
   pages = {117693430500100003},
   publisher = {SAGE Publications Sage UK: London, England},
   title = {Arlequin (version 3.0): an integrated software package for population genetics data analysis},
   volume = {1},
   year = {2005},
}
@article{doi:10.1093/sysbio/syy032,
   author = {Andrew Rambaut and Alexei J Drummond and Dong Xie and Guy Baele and Marc A Suchard},
   doi = {10.1093/sysbio/syy032},
   issue = {5},
   journal = {Systematic Biology},
   pages = {901-904},
   title = {Posterior Summarization in Bayesian Phylogenetics Using Tracer 1.7},
   volume = {67},
   url = {http://dx.doi.org/10.1093/sysbio/syy032},
   year = {2018},
}
@article{10.1371/journal.pcbi.1003537,
   abstract = {We present a new open source, extensible and flexible software platform for Bayesian evolutionary analysis called BEAST 2. This software platform is a re-design of the popular BEAST 1 platform to correct structural deficiencies that became evident as the BEAST 1 software evolved. Key among those deficiencies was the lack of post-deployment extensibility. BEAST 2 now has a fully developed package management system that allows third party developers to write additional functionality that can be directly installed to the BEAST 2 analysis platform via a package manager without requiring a new software release of the platform. This package architecture is showcased with a number of recently published new models encompassing birth-death-sampling tree priors, phylodynamics and model averaging for substitution models and site partitioning. A second major improvement is the ability to read/write the entire state of the MCMC chain to/from disk allowing it to be easily shared between multiple instances of the BEAST software. This facilitates checkpointing and better support for multi-processor and high-end computing extensions. Finally, the functionality in new packages can be easily added to the user interface (BEAUti 2) by a simple XML template-based mechanism because BEAST 2 has been re-designed to provide greater integration between the analysis engine and the user interface so that, for example BEAST and BEAUti use exactly the same XML file format.},
   author = {Remco R. Bouckaert and Joseph Heled and Denise Kühnert and Tim Vaughan and Chieh-Hsi Wu and Dong Xie and Marc A Suchard and Andrew Rambaut and Alexei J. Drummond},
   doi = {10.1371/journal.pcbi.1003537},
   issue = {4},
   journal = {PLOS Computational Biology},
   pages = {1-6},
   publisher = {Public Library of Science},
   title = {BEAST 2: A Software Platform for Bayesian Evolutionary Analysis},
   volume = {10},
   url = {https://doi.org/10.1371/journal.pcbi.1003537},
   year = {2014},
}
@misc{rambaut2016tracer,
   author = {Andrew Rambaut and Marc A Suchard and D Xie and Alexei J Drummond},
   title = {Tracer v1. 6. 2014},
   year = {2016},
}
@misc{Guarracino2010,
   author = {Fabio Guarracino and Luca Cabrini and Rubia Baldassarri and Sonia Petronio and Marco De Carlo and Remo Daniel Covello and Giovanni Landoni and Luciano Gabbrielli and Nicolino Ambrosino},
   doi = {10.1053/j.jvca.2010.06.032},
   isbn = {0892-0915 (Print)\r0892-0915 (Linking)},
   issn = {1532-8422},
   issue = {24},
   journal = {Journal of cardiothoracic and vascular anesthesia},
   pages = {3124-3130},
   pmid = {20829068},
   title = {Noninvasive Ventilation for Awake Percutaneous Aortic Valve Implantation in High-Risk Respiratory Patients: A Case Series.},
   volume = {294},
   url = {http://www.ncbi.nlm.nih.gov/pubmed/20857278},
   year = {2010},
}
@article{Takeuchi2018a,
   author = {Nobuto Takeuchi},
   title = {BIOINF703 : Origin of Genomes},
   year = {2018},
}
@article{tamura1993estimation,
   author = {Koichiro Tamura and Masatoshi Nei},
   issue = {3},
   journal = {Molecular biology and evolution},
   pages = {512-526},
   title = {Estimation of the number of nucleotide substitutions in the control region of mitochondrial DNA in humans and chimpanzees.},
   volume = {10},
   year = {1993},
}
@article{Mate2011,
   author = {B. R. Mate and P. B. Best and B. A. Lagerquist and M. H. Winsor},
   doi = {10.1111/j.1748-7692.2010.00412.x},
   issn = {08240469},
   issue = {3},
   journal = {Marine Mammal Science},
   keywords = {Antarctic Polar Front,Argos telemetry,Eubalaena australis,South Africa,Subtropical Convergence,foraging,migration,movements,southern right whale},
   month = {7},
   pages = {455-476},
   publisher = {Wiley/Blackwell (10.1111)},
   title = {Coastal, offshore, and migratory movements of South African right whales revealed by satellite telemetry},
   volume = {27},
   url = {http://doi.wiley.com/10.1111/j.1748-7692.2010.00412.x},
   year = {2011},
}
@article{Bannister2001,
   abstract = {The history of Australian right whaling is briefly reviewed. Most catching took place in the first half of the 19th century, with a peak in the 1830s, involving bay whaling by locals and visiting whaleships in winter and whaling offshore in the summer. In the early 20th century, right whales were regarded as at least very rare, if not extinct. The first published scientific record for Australian waters in the 20th century was a sighting near Albany, Western Australia, in 1955. Increasing sightings close to the coast in winter and spring led to annual aerial surveys off southern Western Australia from 1976. To allow for possible effects of coastwise movements, coverage was extended into South Australian waters from 1993. Evidence from 19th century pelagic catch locations, recent sightings surveys, 1960s Soviet catch data and photographically-identified individuals is beginning to confirm earlier views about likely seasonal movements to and from warm water coastal breeding grounds and colder water feeding grounds. Increase rates of ca 7-13% have been observed since 1983. Some effects of different breeding female cohort strength are now beginning to appear. A minimum population size of ca 700 for the period 1995-97 is suggested for the bulk of the 'Australian' population, i.e. animals approaching the ca 2,000km of coast between Cape Leeuwin, Western Australia and Ceduna, South Australia.},
   author = {John Bannister},
   isbn = {1561-073X},
   journal = {Journal of Cetacean Research & Management Special Issue.},
   keywords = {Balaenidae: Animals, Cetaceans, Chordates, Mammals,Behavior,Biogeography: Population Studies,Eubalaena australis: southern right whale, distrib,Wildlife Management: Conservation,[00512] General biology - Conservation and resourc,[07002] Behavioral biology - General and comparati,[07003] Behavioral biology - Animal behavior,[07502] Ecology: environmental biology - General a,[07508] Ecology: environmental biology - Animal,[07516] Ecology: environmental biology - Wildlife,[62800] Animal distribution,[85805] Balaenidae,[85805] Balaenidae, Cetacea, Mammalia, Vertebrata,,historical whaling,population size,population status,seasonal movements},
   pages = {103-110},
   pmid = {200200312983},
   title = {Status of southern right whales (Eubalaena australis) off Australia},
   volume = {2},
   year = {2001},
}
@article{Takeuchi2018b,
   author = {Nobuto Takeuchi},
   title = {BIOINF703 : Origin of Genomes},
   year = {2018},
}
@article{Takeuchi2018,
   author = {Nobuto Takeuchi},
   title = {BIOINF703 : Origin of Genomes},
   year = {2018},
}
@article{Takeuchi2018g,
   author = {Nobuto Takeuchi},
   title = {BIOINF703 : Origin of Genomes},
   year = {2018},
}
@article{Nature2005,
   abstract = {Nature guide to authors: Summary paragraph for Letters},
   author = {Nature},
   doi = {10.1107/S0365110X59001529},
   isbn = {0365-110X},
   issn = {0365110X},
   issue = {May},
   journal = {Nature},
   pages = {2005},
   pmid = {12891355},
   title = {How to construct a Nature summary paragraph},
   volume = {118},
   year = {2005},
}
@article{Takeuchia,
   author = {Nobuto Takeuchi},
   pages = {1-3},
   title = {Lab 6 Guide 1},
}
@article{Takeuchi2018c,
   author = {Nobuto Takeuchi},
   title = {BIOINF703 : Origin of Genomes},
   year = {2018},
}
@article{Takeuchi,
   author = {Nobuto Takeuchi},
   issue = {1},
   pages = {1-7},
   title = {Lab 6 Guide 2},
}
@article{Carol2015a,
   author = {JW Maciaszek and NA Parada and WW Cruikshank and DM Center and H Kornfeld and GA Viglianti},
   issue = {1},
   journal = {J Immunol.},
   pages = {5-8},
   title = {IL-16 represses HIV-1 promoter activity.},
   volume = {158},
   url = {http://www.ncbi.nlm.nih.gov/pubmed/8977168%5Cnhttp://www.jimmunol.org/content/158/1/5.full.pdf},
   year = {1997},
}
@inproceedings{Heckel2017,
   abstract = {Due to its longevity and enormous information density, DNA is an attractive medium for archival storage. In this work, we study the fundamental limits and tradeoffs of DNA-based storage systems under a simple model, motivated by current technological constraints on DNA synthesis and sequencing. Our model captures two key distinctive aspects of DNA storage systems: (1) the data is written onto many short DNA molecules that are stored in an unordered way and (2) the data is read by randomly sampling from this DNA pool. Under this model, we characterize the storage capacity, and show that a simple index-based coding scheme is optimal.},
   author = {Reinhard Heckel and Ilan Shomorony and Kannan Ramchandran and David N.C. Tse},
   doi = {10.1109/ISIT.2017.8007106},
   isbn = {9781509040964},
   issn = {21578095},
   journal = {IEEE International Symposium on Information Theory - Proceedings},
   title = {Fundamental limits of DNA storage systems},
   year = {2017},
}
@article{Takahashi2019,
   abstract = {We developed a complete end-to-end DNA data storage device. The device enables the encoding of data, which is then written to a DNA oligonucleotide using a custom DNA synthesizer, pooled for liquid storage, and read using a nanopore sequencer and a novel, minimal preparation protocol. We demonstrate an automated 5-byte write, store, and read cycle with the ability to expand as new technology is available.},
   author = {Christopher N. Takahashi and Bichlien H. Nguyen and Karin Strauss and Luis Ceze},
   doi = {10.1038/s41598-019-41228-8},
   issn = {20452322},
   issue = {1},
   journal = {Scientific Reports},
   title = {Demonstration of End-to-End Automation of DNA Data Storage},
   volume = {9},
   year = {2019},
}
@misc{Lu2016,
   abstract = {The revolution of genome sequencing is continuing after the successful second-generation sequencing (SGS) technology. The third-generation sequencing (TGS) technology, led by Pacific Biosciences (PacBio), is progressing rapidly, moving from a technology once only capable of providing data for small genome analysis, or for performing targeted screening, to one that promises high quality de novo assembly and structural variation detection for human-sized genomes. In 2014, the MinION, the first commercial sequencer using nanopore technology, was released by Oxford Nanopore Technologies (ONT). MinION identifies DNA bases by measuring the changes in electrical conductivity generated as DNA strands pass through a biological pore. Its portability, affordability, and speed in data production makes it suitable for real-time applications, the release of the long read sequencer MinION has thus generated much excitement and interest in the genomics community. While de novo genome assemblies can be cheaply produced from SGS data, assembly continuity is often relatively poor, due to the limited ability of short reads to handle long repeats. Assembly quality can be greatly improved by using TGS long reads, since repetitive regions can be easily expanded into using longer sequencing lengths, despite having higher error rates at the base level. The potential of nanopore sequencing has been demonstrated by various studies in genome surveillance at locations where rapid and reliable sequencing is needed, but where resources are limited.},
   author = {Hengyun Lu and Francesca Giordano and Zemin Ning},
   doi = {10.1016/j.gpb.2016.05.004},
   issn = {22103244},
   issue = {5},
   journal = {Genomics, Proteomics and Bioinformatics},
   title = {Oxford Nanopore MinION Sequencing and Genome Assembly},
   volume = {14},
   year = {2016},
}
@article{Newman2019,
   abstract = {© 2019, The Author(s). DNA promises to be a high density data storage medium, but physical storage poses a challenge. To store large amounts of data, pools must be physically isolated so they can share the same addressing scheme. We propose the storage of dehydrated DNA spots on glass as an approach for scalable DNA data storage. The dried spots can then be retrieved by a water droplet using a digital microfluidic device. Here we show that this storage schema works with varying spot organization, spotted masses of DNA, and droplet retrieval dwell times. In all cases, the majority of the DNA was retrieved and successfully sequenced. We demonstrate that the spots can be densely arranged on a microfluidic device without significant contamination of the retrieval. We also demonstrate that 1 TB of data could be stored in a single spot of DNA and successfully retrieved using this method.},
   author = {Sharon Newman and Ashley P. Stephenson and Max Willsey and Bichlien H. Nguyen and Christopher N. Takahashi and Karin Strauss and Luis Ceze},
   doi = {10.1038/s41467-019-09517-y},
   issn = {20411723},
   issue = {1},
   journal = {Nature Communications},
   title = {High density DNA data storage library via dehydration with digital microfluidic retrieval},
   volume = {10},
   year = {2019},
}
@article{HosseinTabatabaeiYazdi2017,
   abstract = {DNA-based data storage is an emerging nonvolatile memory technology of potentially unprecedented density, durability, and replication efficiency. The basic system implementation steps include synthesizing DNA strings that contain user information and subsequently retrieving them via high-throughput sequencing technologies. Existing architectures enable reading and writing but do not offer random-access and error-free data recovery from low-cost, portable devices, which is crucial for making the storage technology competitive with classical recorders. Here we show for the first time that a portable, random-access platform may be implemented in practice using nanopore sequencers. The novelty of our approach is to design an integrated processing pipeline that encodes data to avoid costly synthesis and sequencing errors, enables random access through addressing, and leverages efficient portable sequencing via new iterative alignment and deletion error-correcting codes. Our work represents the only known random access DNA-based data storage system that uses error-prone nanopore sequencers, while still producing error-free readouts with the highest reported information rate/density. As such, it represents a crucial step towards practical employment of DNA molecules as storage media.},
   author = {S. M. Hossein TabatabaeiYazdi and Ryan Gabrys and Olgica Milenkovic},
   doi = {10.1038/s41598-017-05188-1},
   issn = {20452322},
   issue = {1},
   journal = {Scientific Reports},
   title = {Portable and Error-Free DNA-Based Data Storage},
   volume = {7},
   year = {2017},
}
@inproceedings{Blawat2016,
   abstract = {We report on a strong capacity boost in storing digital data in synthetic DNA. In principle, synthetic DNA is an ideal media to archive digital data for very long times because the achievable data density and longevity outperforms today's digital data storage media by far. On the other hand, neither the synthesis, nor the amplification and the sequencing of DNA strands can be performed error-free today and in the foreseeable future. In order to make synthetic DNA available as digital data storage media, specifically tailored forward error correction schemes have to be applied. For the purpose of realizing a DNA data storage, we have developed an efficient and robust forward-error-correcting scheme adapted to the DNA channel. We based the design of the needed DNA channel model on data from a proof-of-concept conducted 2012 by a team from the Harvard Medical School [1]. Our forward error correction scheme is able to cope with all error types of today's DNA synthesis, amplification and sequencing processes, e.g. insertion, deletion, and swap errors. In a successful experiment, we were able to store and retrieve error-free 22 MByte of digital data in synthetic DNA recently. The found residual error probability is already in the same order as it is in hard disk drives and can be easily improved further. This proves the feasibility to use synthetic DNA as longterm digital data storage media.},
   author = {Meinolf Blawat and Klaus Gaedke and Ingo Hütter and Xiao Ming Chen and Brian Turczyk and Samuel Inverso and Benjamin W. Pruitt and George M. Church},
   doi = {10.1016/j.procs.2016.05.398},
   issn = {18770509},
   journal = {Procedia Computer Science},
   title = {Forward error correction for DNA data storage},
   volume = {80},
   year = {2016},
}
@article{Choi2019,
   abstract = {DNA-based data storage has emerged as a promising method to satisfy the exponentially increasing demand for information storage. However, practical implementation of DNA-based data storage remains a challenge because of the high cost of data writing through DNA synthesis. Here, we propose the use of degenerate bases as encoding characters in addition to A, C, G, and t, which augments the amount of data that can be stored per length of DNA sequence designed (information capacity) and lowering the amount of DNA synthesis per storing unit data. Using the proposed method, we experimentally achieved an information capacity of 3.37 bits/character. The demonstrated information capacity is more than twice when compared to the highest information capacity previously achieved. the proposed method can be integrated with synthetic technologies in the future to reduce the cost of DNA-based data storage by 50%. The annual demand for digital data storage is expected to surpass the supply of silicon in 2040, assuming that all data are stored in flash memory for instant access 1. Considering the massive accumulation of digital data, the development of alternative storage methods is essential. One alternative is DNA-based data storage, which converts the binary digital data of 0 and 1 into the quaternary encoding nucleotides A, C, G, and T, synthesizes the sequence, and stores the data 2,3. This concept 2-10 is attractive due to two main advantages: the high physical information density of petabytes of data per gram, and the durability as the storage lasts for centuries without energy input. Due to these advantages, DNA-based data storage is expected to supplement the increasing demand for digital data storage, especially for archival data that are not frequently accessed. Since DNA-based data storage was proposed, the major goal was to improve data to DNA encoding algorithms 9,10 or error correction algorithms 4,6,7,9,10 to reduce data error or loss considering the biochemical properties while handling DNA. These previous studies on encoding algorithms showed 100% reconstruction of the data from DNA while using library of 100 to 200nt length oligonucleotides. To correct the synthesis errors and recover the dropped data fragments during DNA amplification, the library of oligonucleotides that contains 1300 copies of each designed sequences were required 10 , with the developed algorithms. The next step towards the practical use of DNA-based data storage is to reduce the cost of storing the data. The cost of DNA-based data storage is categorized into the cost of data writing through DNA synthesis and the cost of data reading through DNA sequencing. Among these two costs, the cost of data writing is predominant because it is tens of thousands times more expensive per unit DNA than that of reading. However, previous studies have shown that DNA can be put to practical use as a backup storage medium only when the cost of the data writing is},
   author = {Yeongjae Choi and Taehoon Ryu and Amos C. Lee and Hansol Choi and Hansaem Lee and Jaejun Park and Suk Heung Song and Seojoo Kim and Hyeli Kim and Wook Park and Sunghoon Kwon},
   doi = {10.1038/s41598-019-43105-w},
   issn = {20452322},
   issue = {1},
   journal = {Scientific Reports},
   title = {High information capacity DNA-based data storage with augmented encoding characters using degenerate bases},
   volume = {9},
   year = {2019},
}
@inproceedings{Stewart2018,
   abstract = {© Springer Nature Switzerland AG 2018. We present strand and codeword design schemes for a DNA database capable of approximate similarity search over a multidimensional dataset of content-rich media. Our strand designs address cross-talk in associative DNA databases, and we demonstrate a novel method for learning DNA sequence encodings from data, applying it to a dataset of tens of thousands of images. We test our design in the wetlab using one hundred target images and ten query images, and show that our database is capable of performing similarity-based enrichment: on average, visually similar images account for 30% of the sequencing reads for each query, despite making up only 10% of the database.},
   author = {Kendall Stewart and Yuan Jyue Chen and David Ward and Xiaomeng Liu and Georg Seelig and Karin Strauss and Luis Ceze},
   doi = {10.1007/978-3-030-00030-1_4},
   isbn = {9783030000295},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   title = {A content-addressable DNA database with learned sequence encodings},
   volume = {11145 LNCS},
   year = {2018},
}
@inproceedings{Limbachiya2016,
   abstract = {DNA based storage systems received attention by many researchers. This includes archival and re-writable random access DNA based storage systems. In this work, we have developed an efficient technique to encode the data into DNA sequence by using non-linear families of ternary codes. In particular, we proposes an algorithm to encode data into DNA with high information storage density and better error correction using a sub code of Golay code. Theoretically, 115 exabytes (EB) data can be stored in one gram of DNA by our method.},
   author = {Dixita Limbachiya and Vijay Dhameliya and Madhav Khakhar and Manish K. Gupta},
   doi = {10.1109/IWSDA.2015.7458386},
   isbn = {9781467383080},
   journal = {7th International Workshop on Signal Design and Its Applications in Communications, IWSDA 2015},
   title = {On optimal family of codes for archival DNA storage},
   year = {2016},
}
@misc{Rang2018,
   abstract = {Nanopore sequencing is a rapidly maturing technology delivering long reads in real time on a portable instrument at low cost. Not surprisingly, the community has rapidly taken up this new way of sequencing and has used it successfully for a variety of research applications. A major limitation of nanopore sequencing is its high error rate, which despite recent improvements to the nanopore chemistry and computational tools still ranges between 5% and 15%. Here, we review computational approaches determining the nanopore sequencing error rate. Furthermore, we outline strategies for translation of raw sequencing data into base calls for detection of base modifications and for obtaining consensus sequences.},
   author = {Franka J. Rang and Wigard P. Kloosterman and Jeroen de Ridder},
   doi = {10.1186/s13059-018-1462-9},
   issn = {1474760X},
   issue = {1},
   journal = {Genome Biology},
   title = {From squiggle to basepair: Computational approaches for improving nanopore sequencing read accuracy},
   volume = {19},
   year = {2018},
}
@article{Shipman2016,
   abstract = {Copyright 2016 by the American Association for the Advancement of Science. All rights reserved. The ability to write a stable record of identified molecular events into a specific genomic locus would enable the examination of long cellular histories and have many applications, ranging from developmental biology to synthetic devices.We show that the type I-E CRISPR (clustered regularly interspaced short palindromic repeats)-Cas  system of Escherichia coli can mediate acquisition of defined pieces of synthetic DNA.We harnessed this feature to generate records of specific DNA sequences into a population of bacterial genomes.We then applied directed evolution so as to alter the recognition of a protospacer adjacent motif by the Cas1-Cas2 complex, which enabled recording in two modes simultaneously. We used this system to reveal aspects of spacer acquisition, fundamental to the CRISPR-Cas adaptation process. These results lay the foundations of a multimodal intracellular recording device.},
   author = {Seth L. Shipman and Jeff Nivala and Jeffrey D. Macklis and George M. Church},
   doi = {10.1126/science.aaf1175},
   issn = {10959203},
   issue = {6298},
   journal = {Science},
   title = {Molecular recordings by directed CRISPR spacer acquisition},
   volume = {353},
   year = {2016},
}
@article{Gabrys2017,
   abstract = {We consider a new family of codes, termed asymmetric Lee distance codes, that arise in the design and implementation of DNA-based storage systems and systems with parallel string transmission protocols. The codewords are defined over a quaternary alphabet, although the results carry over to other alphabet sizes; furthermore, symbol confusability is dictated by their underlying binary representation. Our contributions are two-fold. First, we demonstrate that the new distance represents a linear combination of the Lee and Hamming distance and derive upper bounds on the size of the codes under this metric based on linear programming techniques. Second, we propose a number of code constructions which imply lower bounds.},
   author = {Ryan Gabrys and Han Mao Kiah and Olgica Milenkovic},
   doi = {10.1109/TIT.2017.2700847},
   issn = {00189448},
   issue = {8},
   journal = {IEEE Transactions on Information Theory},
   title = {Asymmetric Lee Distance Codes for DNA-Based Storage},
   volume = {63},
   year = {2017},
}
@article{Lee2019,
   abstract = {DNA is an emerging medium for digital data and its adoption can be accelerated by synthesis processes specialized for storage applications. Here, we describe a de novo enzymatic synthesis strategy designed for data storage which harnesses the template-independent polymerase terminal deoxynucleotidyl transferase (TdT) in kinetically controlled conditions. Information is stored in transitions between non-identical nucleotides of DNA strands. To produce strands representing user-defined content, nucleotide substrates are added iteratively, yielding short homopolymeric extensions whose lengths are controlled by apyrase-mediated substrate degradation. With this scheme, we synthesize DNA strands carrying 144 bits, including addressing, and demonstrate retrieval with streaming nanopore sequencing. We further devise a digital codec to reduce requirements for synthesis accuracy and sequencing coverage, and experimentally show robust data retrieval from imperfectly synthesized strands. This work provides distributive enzymatic synthesis and information-theoretic approaches to advance digital information storage in DNA.},
   author = {Henry H. Lee and Reza Kalhor and Naveen Goela and Jean Bolot and George M. Church},
   doi = {10.1038/s41467-019-10258-1},
   issn = {20411723},
   issue = {1},
   journal = {Nature Communications},
   title = {Terminator-free template-independent enzymatic DNA synthesis for digital information storage},
   volume = {10},
   year = {2019},
}
@article{Carmean2019,
   abstract = {Moore's law may be slowing, but our ability to manipulate molecules is improving faster than ever. DNA could provide alternative substrates for computing and storage as existing ones approach physical limits. In this paper, we explore the implications of this trend in computer architecture. We present a computer systems perspective on molecular processing and storage, positing a hybrid molecular-electronic architecture that plays to the strengths of both domains. We cover the design and implementation of all stages of the pipeline: encoding, DNA synthesis, system integration with digital microfluidics, DNA sequencing (including emerging technologies such as nanopores), and decoding. We first draw on our experience designing a DNA-based archival storage system, which includes the largest demonstration to date of DNA digital data storage of over three billion nucleotides encoding over 400 MB of data. We then propose a more ambitious hybrid-electronic design that uses a molecular form of near-data processing for massive parallelism. We present a model that demonstrates the feasibility of these systems in the near future. We think the time is ripe to consider molecular storage seriously and explore system designs and architectural implications.},
   author = {Douglas Carmean and Luis Ceze and Georg Seelig and Kendall Stewart and Karin Strauss and Max Willsey},
   doi = {10.1109/JPROC.2018.2875386},
   issn = {00189219},
   issue = {1},
   journal = {Proceedings of the IEEE},
   title = {DNA Data Storage and Hybrid Molecular-Electronic Computing},
   volume = {107},
   year = {2019},
}
@inproceedings{Goela2017,
   abstract = {Focusing on error-correction methods and codes, a systems level design is presented for encoding movies and digital information in DNA storage. A source of data (e.g., movies, audio) is compressed, efficiently encoded with redundant information, modulated, and stored in multiple DNA oligonucleotide strands. The goal is to decode the source from the DNA reliably even in the presence of diverse errors introduced by DNA synthesis, PCR amplification, and DNA sequencing processes.},
   author = {Naveen Goela and Jean Bolot},
   doi = {10.1109/ITA.2016.7888163},
   isbn = {9781509025299},
   journal = {2016 Information Theory and Applications Workshop, ITA 2016},
   title = {Encoding movies and data in DNA storage},
   year = {2017},
}
@article{Mehta2018,
   abstract = {Almost five decades ago Crick, Orgel, and others proposed the RNA world hypothesis. Subsequent studies have raised the possibility that RNA might be able to support both genotype and phenotype, and the function of RNA templates has been studied in terms of evolution, replication, and catalysis. Recently, we engineered strains of E. coli in which a large fraction of 2′-deoxycytidine in the genome is substituted with the modified base 5-hydroxymethyl-2′-deoxycytidine. We now report the generation of mutant strains derived from these engineered bacteria that show significant (∼40−50%) ribonu-cleotide content in their genome. We have begun to characterize the properties of these chimeric genomes and the corresponding strains to determine the circumstances under which E. coli can incorporate ribonucleotides into its genome and herein report our initial observations.},
   author = {Angad P. Mehta and Yiyang Wang and Sean A. Reed and Lubica Supekova and Tsotne Javahishvili and John C. Chaput and Peter G. Schultz},
   doi = {10.1021/jacs.8b07046},
   issn = {15205126},
   issue = {36},
   journal = {Journal of the American Chemical Society},
   pages = {11464-11473},
   title = {Bacterial Genome Containing Chimeric DNA-RNA Sequences},
   volume = {140},
   year = {2018},
}
@article{Poole2006,
   abstract = {Our understanding of the early steps in the evolution of life is hampered by a Catch-22: Darwinian selection leading to longer genomes requires as prerequisite increased replicative fidelity. Yet a genome at capacity cannot increase in size; it will be catastrophically mutated out of existence if fidelity has not already increased. Traditionally the problem has been considered for genotypes but can be down-sized if multiple genotypes specify the same phenotype. Kun and colleagues put empirical meat on theoretical bone by analysing ribozyme mutagenesis data, concluding that modest replication fidelities could permit a primordial genome with up to 100 genes.},
   author = {Anthony M. Poole},
   doi = {10.1002/bies.20367},
   isbn = {0265-9247 (Print)\r0265-9247 (Linking)},
   issn = {02659247},
   issue = {2},
   journal = {BioEssays},
   pages = {105-108},
   pmid = {16435297},
   title = {Getting from an RNA world to modern cells just got a little easier},
   volume = {28},
   year = {2006},
}
@article{Zenkin2006,
   abstract = {Fidelity of template-dependent nucleic acid synthesis is the main determinant of stable heredity and error-free gene expression. The mechanism (or mechanisms) ensuring fidelity of transcription by DNA-dependent RNA polymerases (RNAPs) is not fully understood. Here, we show that the 3' end-proximal nucleotide of the nascent transcript stimulates hydrolysis of the penultimate phosphodiester bond by providing active groups and coordination bonds to the RNAP active center. This stimulation is much higher in the case of misincorporated nucleotide. We show that during transcription elongation, the hydrolytic reaction stimulated by misincorporated nucleotides proofreads most of the misincorporation events and thus serves as an intrinsic mechanism of transcription fidelity.},
   author = {Nikolay Zenkin and Yulia Yuzenkova and Konstantin Severinov},
   doi = {10.1126/science.1127422},
   isbn = {1095-9203 (Electronic)},
   issn = {00368075},
   issue = {5786},
   journal = {Science},
   pages = {518-520},
   pmid = {16873663},
   title = {Transcript-assisted transcriptional proofreading},
   volume = {313},
   year = {2006},
}
@article{Beerli2007,
   author = {Peter Beerli},
   issue = {December},
   pages = {2-4},
   title = {Comment on "Population Size Does Not Influence Mitochondrial Genetic Diversity in Animals" -- Mulligan et al. 314 (5804): 1390a -- Science},
   volume = {314},
   url = {papers3://publication/uuid/2DF4C5A0-2C73-4953-8993-ACE86A678C2E},
   year = {2007},
}
@article{Koshkin1998a,
   abstract = {... Also shown is the locked N-type conformation of an LNA nucleoside. ... The Danish Natural Science Research Council, The Danish Technical Research Council, and Exiqon A/S, Denmark are thanked for financial support. ... (17) Egli, M. Antisense Nucleic  Acid Drug DeV. ... \n},
   author = {A A Koshkin and P Nielsen and M Meldgaard},
   journal = {Journal of the …},
   pages = {1-2},
   title = {The University of Auckland - Loading...},
   url = {http://pubs.acs.org/doi/pdf/10.1021/ja9822862%5Cnpapers2://publication/uuid/286361C2-53C8-418C-B206-A7434CE769CE},
   year = {1998},
}
@article{Takeuchi2018e,
   author = {Nobuto Takeuchi},
   title = {BIOINF703 : Origin of Genomes},
   year = {2018},
}
@article{Koshkin1998,
   abstract = {... Also shown is the locked N-type conformation of an LNA nucleoside. ... The Danish Natural Science Research Council, The Danish Technical Research Council, and Exiqon A/S, Denmark are thanked for financial support. ... (17) Egli, M. Antisense Nucleic  Acid Drug DeV. ... \n},
   author = {A A Koshkin and P Nielsen and M Meldgaard},
   journal = {Journal of the …},
   pages = {1-2},
   title = {The University of Auckland - Loading...},
   url = {http://pubs.acs.org/doi/pdf/10.1021/ja9822862%5Cnpapers2://publication/uuid/286361C2-53C8-418C-B206-A7434CE769CE},
   year = {1998},
}
@article{Neigel2002,
   author = {Joseph E Neigel},
   issue = {Slatkin 1985},
   keywords = {dispersal,f st,gene flow,nm,population structure},
   pages = {167-173},
   title = {Is F ST obsolete ?},
   year = {2002},
}
@article{Hilbert2011a,
   abstract = {We estimate the world's technological capacity to store, communicate, and compute information, tracking 60 analog and digital technologies during the period from 1986 to 2007. In 2007, humankind was able to store 2.9 x 10 20 optimally compressed bytes, communicated almost 2 x 10 21 bytes, and carry out 6.4 x 10 18 instructions per second on general-purpose computers. General-purpose computing capacity grew at an annual rate of 58%. The world's capacity for bidirectional telecommunication grew at 28% per year, closely followed by the increase in globally stored information (23%). Humankind's capacity for unidirectional information diffusion through broadcasting channels has experienced comparatively modest annual growth (6%). Telecommunication has been dominated by digital technologies since 1990 (99.9% in digital format in 2007) and the majority of our technological memory has been in digital format since the early 2000s (94% digital in 2007). Leading social scientists have recognized that we are living through an age in which "the generation of wealth, the exercise of power, and the creation of cultural codes came to depend on the technological capacity of societies and individuals, with information technologies as the core of this capacity" (1). Despite this insight, most evaluations of society's technological capacity to handle information are based on either qualitative assessments or indirect approximations, such as the stock of installed devices or the economic value of related products and services (2-9). Previous work Some pioneering studies have taken a more direct approach to quantify the amount of information that society processes with its information and communication technologies (ICT). Following pioneering work in Japan (10), Pool (11) estimated the growth trends of the "amount of words" transmitted by 17 major communications media in the United States from 1960 to 1977. This study was the first to show empirically the declining relevance of print media with respect to electronic media. In 1997, Lesk (12) asked "how much information is there in the world?" and presented a brief outline on how to go about estimating the global information storage capacity. A group of researchers at the University of California, at Berkeley, took up the measurement challenge between 2000 and 2003 (13). Their focus on "uniquely created" information resulted in the conclusion that "most of the total volume of new information flows is derived from the volume of voice telephone traffic, most of which is unique content" (97%); as broadcasted television and most information storage mainly consists of duplicate information, these omnipresent categories contributed relatively little. A storage company hired a private sector research firm (International Data Corporation, IDC) to estimate the global hardware capacity of digital ICT for the years 2007-2008 (14). For digital storage, IDC estimates that in 2007 "all the empty or usable space on hard drives, tapes, CDs, DVDs, and memory (volatile and nonvolatile) in the market equaled 264 exabytes" (14). During 2008, an industry and university collaboration explicitly focused on information consumption (15), measured in hardware capacity, words, and hours. The results are highly reliant on media time-budget studies, which estimate how many hours people interact with a media device. Not surprisingly, the result obtained with this methodology was that computer games and movies represent 99.2% of the total amount of data "consumed". Scope of our exercise To reconcile these different results, we focus on the world's technological capacity to handle information. We do not account for uniqueness of information, since it is very difficult to differentiate between truly new and merely recombined, duplicate information. Instead we assume that all information has some relevance for some individual. Aside from the traditional focus on the transmission through space (communication) and time (storage), we also consider the computation of information. We define storage as the},
   author = {Martin Hilbert and Priscila López},
   doi = {10.1126/science.1200970},
   journal = {Science},
   month = {2},
   title = {The World's Technological Capacity to Store, Communicate, and Compute Information},
   url = {http://science.sciencemag.org/content/early/2011/02/09/science.1200970.abstract http://science.sciencemag.org/},
   year = {2011},
}
@article{Eddy2004d,
   abstract = {Statistical models called hidden Markov models are a recurring theme in computational biology. What are hidden Markov models, and why are they so useful for so many different problems?},
   author = {Sean R. Eddy},
   doi = {10.1038/nbt1004-1315},
   isbn = {1087-0156},
   issn = {1087-0156},
   issue = {10},
   journal = {Nature Biotechnology},
   keywords = {Markov model hidden Markov Model chaien de Markov},
   pages = {1315-1316},
   pmid = {15470472},
   title = {What is a hidden Markov model?},
   volume = {22},
   url = {papers2://publication/uuid/9BFE5458-7C45-4E6E-9F95-48D5CE4CF032 http://dx.doi.org/10.1038/nbt1004-1315},
   year = {2004},
}
@article{Mortazavi2008,
   abstract = {We have mapped and quantified mouse transcriptomes by deeply sequencing them and recording how frequently each gene is represented in the sequence sample (RNA-Seq). This provides a digital measure of the presence and prevalence of transcripts from known and previously unknown genes. We report reference measurements composed of 41-52 million mapped 25-base-pair reads for poly(A)-selected RNA from adult mouse brain, liver and skeletal muscle tissues. We used RNA standards to quantify transcript prevalence and to test the linear range of transcript detection, which spanned five orders of magnitude. Although >90% of uniquely mapped reads fell within known exons, the remaining data suggest new and revised gene models, including changed or additional promoters, exons and 3' untranscribed regions, as well as new candidate microRNA precursors. RNA splice events, which are not readily measured by standard gene expression microarray or serial analysis of gene expression methods, were detected directly by mapping splice-crossing sequence reads. We observed 1.45 x 10(5) distinct splices, and alternative splices were prominent, with 3,500 different genes expressing one or more alternate internal splices.},
   author = {Ali Mortazavi and Brian A. Williams and Kenneth McCue and Lorian Schaeffer and Barbara Wold},
   doi = {10.1038/nmeth.1226},
   isbn = {1548-7105 (Electronic)\r1548-7091 (Linking)},
   issn = {15487091},
   issue = {7},
   journal = {Nature Methods},
   pages = {621-628},
   pmid = {18516045},
   title = {Mapping and quantifying mammalian transcriptomes by RNA-Seq},
   volume = {5},
   year = {2008},
}
@article{Jackson2018,
   abstract = {The multi-state Markov model is a useful way of describing a process in which an individual moves through a series of states in continuous time. The msm package for R allows a general multi-state model to be fitted to longitudinal data. Data often consist of observations of the process at arbitrary times, so that the exact times when the state changes are unobserved. For example, the progression of chronic diseases is often described by stages of severity, and the state of the patient may only be known at doctor or hospital visits. Features of msm include the ability to model transition rates and hidden Markov output models in terms of covariates, and the ability to model data with a variety of observation schemes, including censored states. Hidden Markov models, in which the true path through states is only observed through some error-prone marker, can also be fitted. The observation is generated, conditionally on the underly- ing states, via some distribution. An example is a screening misclassification model in which states are observed with error. More generally, hidden Markov models can have a continuous response, with some arbitrary distribution, conditionally on the underlying state. This manual introduces the theory behind multi-state Markov and hidden Markov models, and gives a tutorial in the typical use of the msm package, illustrated by some typical applications to modelling chronic diseases.},
   author = {Christopher H Jackson},
   journal = {Journal of Statistical Software},
   pmid = {18286417},
   title = {Multi-state modelling with R: the msm package (long vignette)},
   year = {2018},
}
@misc{Yachie2008,
   abstract = {Data-encoding synthetic DNA, inserted into the genome of a living organism, is thought to be more robust than the current media. Because the living genome is duplicated and copied into new generations, one of the merits of using DNA material is long-term data storage within heritable media. A disadvantage of this approach is that encoded data can be unexpectedly broken by mutation, deletion, and insertion of DNA, which occurs naturally during evolution and prolongation, or laboratory experiments. For this reason, several information theory-based approaches have been developed as an error check of broken DNA data in order to achieve data durability. These approaches cannot efficiently recover badly damaged data- encoding DNA. We recently developed a DNA data-storage approach based on the multiple sequence alignment method to achieve a high level of data durability. In this paper, we overview this technology and discuss strategies for optimal application of this approach.},
   author = {Nozomu Yachie and Yoshiaki Ohashi and Masaru Tomita},
   doi = {10.1007/s11693-008-9020-5},
   issn = {18725325},
   journal = {Systems and Synthetic Biology},
   keywords = {DNa data storage,Error check,Error correction,Genetically modified organism (GMO),Polymerase chain reaction (PCR),Sequence alignment},
   title = {Stabilizing synthetic data in the DNA of living organisms},
   year = {2008},
}
@article{Borkowski2016,
   abstract = {Synthetic biology uses DNA programs to add new functions into living cells. By expressing transcription factors, microbes such as Escherichia coli can be made to perform complex computational logic (1) and pass “memories” of selected events between generations (2). But DNA is more than just the source code for protein expression programs; DNA can be used as the storage medium for information. In synthetic biology, the use of DNA for in vivo information storage was first realized in 2009 with a synthetic genetic program that enabled E. coli to count events by using a recombinase to rearrange DNA in response to an input (3). With multiple recombinases, this technology could be used to store 1.375 bytes of information in a living E. coli (4). As reported by Shipman et al. (5) on page 463 of this issue, and by Roquet et al. (6), in vivo encoding of information into DNA is pushed even further, using either genome editing to store dozens of bytes of data, or employing multiple recombinases to realize “state machines” inside living cells.},
   author = {Olivier Borkowski and Charlie Gilbert and Tom Ellis},
   doi = {10.1126/science.aah4438},
   issn = {0036-8075},
   journal = {Science},
   title = { On the record with E. coli DNA },
   year = {2016},
}
@inproceedings{Debata2012,
   abstract = {A major problem in communication engineering system is the transmitting of information from source to receiver over a noisy channel. To check the error in information digits many error detecting and correcting codes have been developed. The main aim of these error correcting codes is to encode the information digits and decode these digits to detect and correct the common errors in transmission. This information theory concept helps to study the information transmission in biological systems and extend the field of coding theory into the biological domain. In the cellular level, the information in DNA is transformed into proteins. The sequence of bases like Adenine (A), Thymine (T), Guanine (G) and Cytosine (C) in DNA may be considered as digital codes which transmit genetic information. This paper shows the existence of any form error detecting code in the DNA structure, by encoding the DNA sequences using Hamming code. © 2012 Published by Elsevier Ltd.},
   author = {Prajna Paramita Debata and Debahuti Mishra and Kailash Shaw and Sashikala Mishra},
   doi = {10.1016/j.proeng.2012.06.216},
   issn = {18777058},
   journal = {Procedia Engineering},
   keywords = {Biological systems,Coding theory,Encoding DNA sequences,Error detecting and correcting codes,Hamming code,Information transmission},
   title = {A coding theoretic model for error-detecting in DNA sequences},
   year = {2012},
}
@article{Limbachiya2018,
   abstract = {DNA-based data storage systems have evolved as a solution to accommodate data explosion. In this letter, some properties of DNA codewords that are essential for an archival DNA storage are considered for the design of codes. Constraint-based DNA codes, which avoid runs of nucleotides, have fixed GC-weight, and a specific minimum distance is presented. An altruistic algorithm that enumerates DNA codewords with the above constraints is provided. A theoretical bound on such DNA codewords is obtained. This bound is tight when there is no minimum distance constraint.},
   author = {Dixita Limbachiya and Manish K. Gupta and Vaneet Aggarwal},
   doi = {10.1109/LCOMM.2018.2861867},
   issn = {15582558},
   journal = {IEEE Communications Letters},
   keywords = {DNA codes,DNA storage,constrained codes},
   title = {Family of Constrained Codes for Archival DNA Data Storage},
   year = {2018},
}
@article{Bornholt2017,
   abstract = {Storing data in DNA molecules offers extreme density and durability advantages that can mitigate exponential growth in data storage needs. This article presents a DNA-based archival storage system, performs wet lab experiments to show its feasibility, and identifies technology trends that point to increasing practicality.},
   author = {James Bornholt and Randolph Lopez and Douglas M. Carmean and Luis Ceze and Georg Seelig and Karin Strauss},
   doi = {10.1109/MM.2017.70},
   issn = {02721732},
   journal = {IEEE Micro},
   keywords = {Archival storage,DNA,molecular storage},
   title = {Toward a DNA-Based Archival Storage System},
   year = {2017},
}
@book{Xu2013,
   abstract = {Statistical genomics is a rapidly developing field, with more and more people involved in this area. However, a lack of synthetic reference books and textbooks in statistical genomics has become a major hurdle on the development of the field. Although many books have been published recently in bioinformatics, most of them emphasize DNA sequence analysis under a deterministic approach. Principles of Statistical Genomics synthesizes the state-of-the-art statistical methodologies (stochastic approaches) applied to genome study. It facilitates understanding of the statistical models and methods behind the major bioinformatics software packages, which will help researchers choose the optimal algorithm to analyze their data and better interpret the results of their analyses. Understanding existing statistical models and algorithms assists researchers to develop improved statistical methods to extract maximum information from their data. Resourceful and easy to use, Principles of Statistical Genomics is a comprehensive reference for researchers and graduate students studying statistical genomics.},
   author = {Shizhong Xu},
   doi = {10.1007/978-0-387-70807-2},
   isbn = {9780387708072},
   journal = {Principles of Statistical Genomics},
   title = {Principles of statistical genomics},
   year = {2013},
}
@article{Iterson2012,
   abstract = {Bioinformatics is the field where computational methods from various domains have come together for analysis of biological data. Each domain has introduced its own specific jargon. However, in closely related domains, e.g. machine learning and statistics, concordant and discordant terminology occurs, the later can lead to confusion. This article aims to help solve the confusion of tongues arising from these two closely related domains, which are frequently used in bioinformatics. We provide a short summary of the most commonly applied machine learning and statistical approaches to data analysis in bioinformatics, i.e. classification and statistical hypothesis testing. We explain differences and similarities in common terminology used in various domains, such as precision, recall, sensitivity and true positive rate. This primer can serve as a guide to the terminology used in these fields. © 2012 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim.},
   author = {Maarten Van Iterson and Herman H.H.B.M. Van Haagen and Jelle J. Goeman},
   doi = {10.1002/pmic.201100395},
   issn = {16159853},
   journal = {Proteomics},
   keywords = {Bioinformatics,Classification,Contingency table,False discovery rate,Hypothesis testing,Sensitivity and specificity},
   title = {Resolving confusion of tongues in statistics and machine learning: A primer for biologists and bioinformaticians},
   year = {2012},
}
@article{Liu2020,
   abstract = {© 2019 Elsevier B.V. We present a machine learning approach to short tandem repeat (STR) sequence detection and extraction from massively parallel sequencing data called Fragsifier. Using this approach, STRs are detected on each read by first locating the longest repeat stretches followed by locus prediction using k-mers in a machine learning sequence model. This is followed by reference flanking sequence alignment to determine precise STR boundaries. We show that Fragsifier produces genotypes that are concordant with profiles obtained using capillary electrophoresis (CE), and also compared the results with that of STRait Razor and the ForenSeq UAS. The data pre-processing and training of the sequence classifier is readily scripted, allowing the analyst to experiment with different thresholds, datasets and loci of interest, and different machine learning models.},
   author = {Y.-Y. Liu and D. Welch and R. England and J. Stacey and S. Harbison},
   doi = {10.1016/j.fsigen.2019.102194},
   issn = {18780326},
   journal = {Forensic Science International: Genetics},
   keywords = {Bioinformatics,Machine learning,Massively parallel sequencing,STR extraction},
   title = {Forensic STR allele extraction using a machine learning paradigm},
   volume = {44},
   year = {2020},
}
@article{England2015,
   abstract = {© 2015 Elsevier Ireland Ltd In the last few years the cost of massively parallel sequencing has reduced dramatically to the point that it can now be practically considered as a tool in forensic casework. An important consideration for the implementation of any new approach is the ability to remain compatible with previous technology. With this is mind we conducted two sets of experiments to evaluate sequencing the previously amplified products of two commercial forensic STR multiplexes. Samples amplified with AmpFlSTR® Identifiler® and PowerPlex® Y were sequenced on the Illumina© MiSeq and Ion PGM™ Sequencer (Life Technologies). We found it is possible to sequence such amplified DNA and to accurately determine the STR genotype of forensic samples using both platforms. Sequencing these STR loci provided extra information in the form of sequence variation, something that is not possible measuring amplicon length alone. And from these results we begin to characterise the sequence data from a forensic perspective, by looking at sequence variation within the repeats, stutter and heterozygote imbalance.},
   author = {R. England and N. Curnow and A. Liu and J. Stacey and S. Harbison},
   doi = {10.1016/j.fsigss.2015.09.084},
   issn = {1875175X},
   journal = {Forensic Science International: Genetics Supplement Series},
   keywords = {Massively parallel sequencing,Short tandem repeats},
   title = {Massively parallel sequencing of Identifiler and PowerPlex<sup>®</sup> Y amplified forensic samples},
   volume = {5},
   year = {2015},
}
@article{Knight2010,
   abstract = {The results of an indoor hydroponic Cannabis growth study are presented. It is intended that this work will be of assistance to those with an interest in determining an estimation of yield and value of Cannabis crops. Three cycles of six plants were grown over a period of 1 year in order to ascertain the potential yield of female flowering head material from such an operation. The cultivation methods used were selected to replicate typical indoor hydroponic Cannabis growing operations, such as are commonly encountered by the New Zealand Police. The plants were also tested to ascertain the percentage of the psychoactive chemical Δ-9 tetrahydrocannabinol (THC) present in the flowering head material, and were genetically profiled by STR analysis. Phenotypic observations are related to the data collected. The inexperience of the growers was evidenced by different problems encountered in each of the three cycles, each of which would be expected to negatively impact the yield and THC data obtained. These data are therefore considered to be conservative. The most successful cycle yielded an average of 881. g (31.1. oz) of dry, groomed female flowering head per plant, and over the whole study the 18 plants yielded a total of 12,360. g (436.0. oz), or an average of 687. g (24.2. oz) of dry head per plant. THC data shows significant intra-plant variation and also demonstrates inter-varietal variation. THC values for individual plants ranged from 4.3 to 25.2%. The findings of this study and a separate ESR research project illustrate that the potency of Cannabis grown in New Zealand has dramatically increased in recent years. DNA analysis distinguished distinct groups in general agreement with the phenotypic variation observed. One plant however, exhibiting a unique triallelic pattern at two of the five loci tested, while remaining phenotypically indistinguishable from three other plants within the same grow. © 2010 Elsevier Ireland Ltd.},
   author = {G. Knight and S. Hansen and M. Connor and H. Poulsen and C. McGovern and J. Stacey},
   doi = {10.1016/j.forsciint.2010.04.022},
   issn = {03790738},
   issue = {1-3},
   journal = {Forensic Science International},
   keywords = {Cannabis,DNA,Hydroponic cultivation,Screen of Green (ScrOG),Tetrahydrocannabinol (THC),Yield},
   title = {The results of an experimental indoor hydroponic Cannabis growing study, using the 'Screen of Green' (ScrOG) method-Yield, tetrahydrocannabinol (THC) and DNA analysis},
   volume = {202},
   year = {2010},
}
@article{Mayer2016,
   abstract = {Biopolymers are an attractive alternative to store and circulate information. DNA, for example, combines remarkable longevity with high data storage densities and has been demonstrated as a means for preserving digital information. Inspired by the dynamic, biological regulation of (epi)genetic information, we herein present how binary data can undergo controlled changes when encoded in synthetic DNA strands. By exploiting differential kinetics of hydrolytic deamination reactions of cytosine and its naturally occurring derivatives, we demonstrate how multiple layers of information can be stored in a single DNA template. Moreover, we show that controlled redox reactions allow for interconversion of these DNA-encoded layers of information. Overall, such interlacing of multiple messages on synthetic DNA libraries showcases the potential of chemical reactions to manipulate digital information on (bio)polymers.},
   author = {Clemens Mayer and Gordon R. McInroy and Pierre Murat and Pieter Van Delft and Shankar Balasubramanian},
   doi = {10.1002/anie.201605531},
   issn = {15213773},
   issue = {37},
   journal = {Angewandte Chemie - International Edition},
   title = {An Epigenetics-Inspired DNA-Based Data Storage System},
   volume = {55},
   year = {2016},
}
@misc{Akram2018,
   abstract = {There has been an ascending growth in the capacity of information being generated. The increased production of data in turn has put forward other challenges as well thus, and there is the need to store this information and not only to store it but also to retain it for a prolonged time period. The reliance on DNA as a dense storage medium with high storage capacity and its ability to withstand extreme environmental conditions has increased over the past few years. There have been developments in reading and writing different forms of data on DNA, codes for encrypting data and using DNA as a way of secret writing leading towards new styles like stenography and cryptography. The article outlines different methods adopted for storing digital data on DNA with pros and cons of each method that has been applied plus the advantages and limitations of using DNA as a storage medium.},
   author = {Fatima Akram and Ikram ul Haq and Haider Ali and Aiman Tahir Laghari},
   doi = {10.1007/s11033-018-4280-y},
   issn = {15734978},
   issue = {5},
   journal = {Molecular Biology Reports},
   title = {Trends to store digital data in DNA: an overview},
   volume = {45},
   year = {2018},
}
@article{Vlek2015,
   abstract = {Bayesian networks have gained popularity as a probabilistic tool for reasoning with legal evidence. However, two common difficulties are (1) the construction and (2) the understanding of a network. In previous work, we proposed to use narrative tools and in particular scenario schemes to assist the construction and the understanding of Bayesian networks for legal cases. We proposed a construction method and a reporting format for explaining or understanding the network. The quality of a scenario, which plays an important role in the narrative approach to evidential reasoning, was not yet included in this method. In this paper, we provide a discussion of what constitutes the quality of a scenario, in terms of the narrative concepts of completeness, consistency and plausibility. We propose a probabilistic interpretation of these concepts, and show how they can be incorporated in our previously proposed method. We also illustrate with an example how these concepts concerning scenario quality can be used to explain or understand a Bayesian network.},
   author = {Charlotte S. Vlek and Henry Prakken and Silja Renooij and Bart Verheij},
   doi = {10.3233/978-1-61499-609-5-131},
   isbn = {9781614996088},
   issn = {18798314},
   journal = {Frontiers in Artificial Intelligence and Applications},
   keywords = {Bayesian networks,Narrative,Reasoning about legal evidence},
   pages = {131-140},
   publisher = {IOS Press},
   title = {Representing the Quality of Crime Scenarios in a Bayesian Network},
   volume = {279},
   url = {https://ebooks.iospress.nl/doi/10.3233/978-1-61499-609-5-131},
   year = {2015},
}
@article{Taylor2024,
   abstract = {Background: Household settings are high risk for COVID-19 transmission. Understanding transmission factors associated with environmental dwelling characteristics is important in informing public health and building design recommendations. We aimed to develop a directed acyclic graph (DAG) to inform a novel analytical study examining the effect of dwelling environmental characteristics on household transmission of COVID-19. Methods: Key demographic, behavioural and environmental dwelling characteristics were identified by a multidisciplinary team. Using the DAG to visually display risk factors, and using expert knowledge of available datasets we reached a consensus on the factors included and directionality of relationships to build the final conceptual framework. Factors were displayed as nodes and relationships as pathways. Results: Of 34 potential factors, 16 were included in the DAG, with 13 causal and three biasing pathways. Three variables were not measurable using retrospective datasets. The DAG enabled us to select data sources for the pilot study period and to inform the analysis plan. Key exposure nodes were energy efficiency or dwelling age; dwelling type or number of storeys; and dwelling size. We determined direct and proxy confounders which we could adjust for, potential interactions terms we could test in model building, and co-linear variables to omit in the same model. Conclusions: The DAG helped identify key variables and datasets. It prioritised key nodes and pathways to formalise complex relationships between variables. It was pivotal in identifying unobserved variables, confounders, co-linearity and potential interactions. It has supported data selection and design of a retrospective pilot study analysis plan.},
   author = {Hannah Taylor and Helen Crabbe and Clare Humphreys and Gavin Dabrera and Anna Mavrogianni and Neville Q. Verlander and Giovanni S. Leonardi},
   doi = {10.1016/J.BUILDENV.2023.111145},
   issn = {0360-1323},
   journal = {Building and Environment},
   keywords = {COVID-19,DAG,Directed Acyclic Graphs,Dwelling characteristics,Environmental exposures,Epidemiology,Housing,Public Health,Study design},
   month = {2},
   pages = {111145},
   publisher = {Pergamon},
   title = {Development and use of a directed acyclic graph (DAG) for conceptual framework and study protocol development exploring relationships between dwelling characteristics and household transmission of COVID-19 – England, 2020},
   volume = {250},
   year = {2024},
}
@inbook{BenGal2007,
   abstract = {Adaptive management is an iterative process of gathering new knowledge regarding a system's behavior and monitoring the ecological consequences of management actions to improve management decisions. Although the concept originated in the 1970s, it is rarely actively incorporated into ecological restoration. Bayesian networks (BNs) are emerging as efficient ecological decision-support tools well suited to adaptive management, but examples of their application in this capacity are few. We developed a BN within an adaptive-management framework that focuses on managing the effects of feral grazing and prescribed burning regimes on avian diversity within woodlands of subtropical eastern Australia. We constructed the BN with baseline data to predict bird abundance as a function of habitat structure, grazing pressure, and prescribed burning. Results of sensitivity analyses suggested that grazing pressure increased the abundance of aggressive honeyeaters, which in turn had a strong negative effect on small passerines. Management interventions to reduce pressure of feral grazing and prescribed burning were then conducted, after which we collected a second set of field data to test the response of small passerines to these measures. We used these data, which incorporated ecological changes that may have resulted from the management interventions, to validate and update the BN. The network predictions of small passerine abundance under the new habitat and management conditions were very accurate. The updated BN concluded the first iteration of adaptive management and will be used in planning the next round of management interventions. The unique belief-updating feature of BNs provides land managers with the flexibility to predict outcomes and evaluate the effectiveness of management interventions.},
   author = {Irad Ben‐Gal},
   doi = {10.1002/9780470061572.eqr089},
   journal = {Encyclopedia of Statistics in Quality and Reliability},
   month = {12},
   publisher = {Wiley},
   title = {Bayesian Networks},
   year = {2007},
}
@misc{BiedermanTaroni2012,
   abstract = {Almost 30 years ago, Bayesian networks (BNs) were developed in the field of artificial intelligence as a framework that should assist researchers and practitioners in applying the theory of probability to inference problems of more substantive size and, thus, to more realistic and practical problems. Since the late 1980s, Bayesian networks have also attracted researchers in forensic science and this tendency has considerably intensified throughout the last decade. This review article provides an overview of the scientific literature that describes research on Bayesian networks as a tool that can be used to study, develop and implement probabilistic procedures for evaluating the probative value of particular items of scientific evidence in forensic science. Primary attention is drawn here to evaluative issues that pertain to forensic DNA profiling evidence because this is one of the main categories of evidence whose assessment has been studied through Bayesian networks. The scope of topics is large and includes almost any aspect that relates to forensic DNA profiling. Typical examples are inference of source (or, 'criminal identification'), relatedness testing, database searching and special trace evidence evaluation (such as mixed DNA stains or stains with low quantities of DNA). The perspective of the review presented here is not exclusively restricted to DNA evidence, but also includes relevant references and discussion on both, the concept of Bayesian networks as well as its general usage in legal sciences as one among several different graphical approaches to evidence evaluation. © 2011 Elsevier Ireland Ltd. All rights reserved.},
   author = {A. Biedermann and F. Taroni},
   doi = {10.1016/j.fsigen.2011.06.009},
   issn = {18724973},
   issue = {2},
   journal = {Forensic Science International: Genetics},
   keywords = {Bayesian networks,DNA evidence,Object-oriented Bayesian networks},
   month = {3},
   pages = {147-157},
   pmid = {21775236},
   title = {Bayesian networks for evaluating forensic DNA profiling evidence: A review and guide to literature},
   volume = {6},
   year = {2012},
}
@misc{HuginWP2016,
   abstract = {This white paper illustrates how to use HUGIN Bayesian network software for forensic identification problems using DNA profiles. Bayesian networks are ideal for expressing, manipulating, and resolving identity questions in a variety of forensic contexts, including identification based on DNA evidence. Using HUGIN software forensic science service organizations can develop and integrate advanced probabilistic evidence calculations in their own systems for analyzing DNA and calculating probabilities of identity. Computations are made quickly, reliably, and efficiently-even in complex cases involving several and possibly mixed traces of DNA, combining evidence from alternative sources.},
   author = {Hugin Expert A/S},
   title = {The use of HUGIN software for forensic identification problems involving DNA evidence},
   url = {www.hugin.com},
   year = {2016},
}
@article{Chen2012,
   abstract = {Bayesian networks (BNs) are increasingly being used to model environmental systems, in order to: integrate multiple issues and system components; utilise information from different sources; and handle missing data and uncertainty. BNs also have a modular architecture that facilitates iterative model development. For a model to be of value in generating and sharing knowledge or providing decision support, it must be built using good modelling practice. This paper provides guidelines to developing and evaluating Bayesian network models of environmental systems, and presents a case study habitat suitability model for juvenile Astacopsis gouldi, the giant freshwater crayfish of Tasmania. The guidelines entail clearly defining the model objectives and scope, and using a conceptual model of the system to form the structure of the BN, which should be parsimonious yet capture all key components and processes. After the states and conditional probabilities of all variables are defined, the BN should be assessed by a suite of quantitative and qualitative forms of model evaluation. All the assumptions, uncertainties, descriptions and reasoning for each node and linkage, data and information sources, and evaluation results must be clearly documented. Following these standards will enable the modelling process and the model itself to be transparent, credible and robust, within its given limitations. © 2012 Elsevier Ltd.},
   author = {Serena H. Chen and Carmel A. Pollino},
   doi = {10.1016/j.envsoft.2012.03.012},
   issn = {13648152},
   journal = {Environmental Modelling and Software},
   keywords = {Bayes network,Bayesian belief network,Ecological models,Good modelling practice,Integration,Model evaluation},
   month = {11},
   pages = {134-145},
   title = {Good practice in Bayesian network modelling},
   volume = {37},
   year = {2012},
}
@article{Koeijer2020,
   abstract = {Activity level evaluations, although still a major challenge for many disciplines, bring a wealth of possibilities for a more formal approach to the evaluation of interdisciplinary forensic evidence. This paper proposes a practical methodology for combining evidence from different disciplines within the likelihood ratio framework. Evidence schemes introduced in this paper make the process of combining evidence more insightful and intuitive thereby assisting experts in their interdisciplinairy evaluation and in explaining this process to the courts. When confronted with two opposing scenarios and multiple types of evidence, the likelihood ratio approach allows experts to combine this evidence in a probabilistic manner. Parts of the prosecution and defence scenarios for which forensic science is expected to be informative are identified. For these so called core elements, activity level propositions are formulated. Afterwards evidence schemes are introduced to assist the expert in combining the evidence in a logical manner. Two types of evidence relations are identified: serial and parallel evidence. Practical guidelines are given on how to deal with both types of evidence relations when combining the evidence.},
   author = {Jan A. de Koeijer and Marjan J. Sjerps and Peter Vergeer and Charles E.H. Berger},
   doi = {10.1016/j.scijus.2019.09.001},
   issn = {18764452},
   issue = {1},
   journal = {Science and Justice},
   keywords = {Activity level,Combining evidence,Evidence scheme,Parallel evidence,Scenarios,Serial evidence},
   month = {1},
   pages = {20-29},
   pmid = {31924285},
   publisher = {Forensic Science Society},
   title = {Combining evidence in complex cases - a practical approach to interdisciplinary casework},
   volume = {60},
   year = {2020},
}
@article{Zoete2015,
   abstract = {When two or more crimes show specific similarities, such as a very distinct modus operandi, the probability that they were committed by the same offender becomes of interest. This probability depends on the degree of similarity and distinctiveness. We show how Bayesian networks can be used to model different evidential structures that can occur when linking crimes, and how they assist in understanding the complex underlying dependencies. That is, how evidence that is obtained in one case can be used in another and vice versa. The flip side of this is that the intuitive decision to "unlink" a case in which exculpatory evidence is obtained leads to serious overestimation of the strength of the remaining cases.},
   author = {Jacob de Zoete and Marjan Sjerps and David Lagnado and Norman Fenton},
   doi = {10.1016/j.scijus.2014.11.005},
   issn = {18764452},
   issue = {3},
   journal = {Science and Justice},
   keywords = {Bayesian networks,Case linkage,Combining evidence,Crime linkage,Serial crime},
   month = {5},
   pages = {209-217},
   publisher = {Forensic Science Society},
   title = {Modelling crime linkage with Bayesian networks},
   volume = {55},
   year = {2015},
}
@article{Taylor2016,
   abstract = {Bayesian networks are being increasingly used to address complex questions of forensic interest. Like all probabilities, those that underlie the nodes within a network rely on structured data and knowledge. Obviously, the more structured data we have, the better. But, in real life, the numbers of experiments that can be carried out are limited. It is thus important to know if/when our knowledge is sufficient and when one needs to perform further experiments to be in a position to report the value of the observations made. To explore the impact of the amount of data that are available for assessing results, we have constructed Bayesian Networks and explored the sensitivity of the likelihood ratios to changes to the data that underlie each node. Bayesian networks are constructed and sensitivity analyses performed using freely available R libraries (gRain and BNlearn). We demonstrate how the analyses can be used to yield information about the robustness provided by the data used to inform the conditional probability table, and also how they can be used to direct further research for maximum effect. By maximum effect, we mean to contribute with the least investment to an increased robustness. In addition, the paper investigates the consequences of the sensitivity analysis to the discussion on how the evidence shall be reported for a given state of knowledge in terms of underpinning data.},
   author = {Duncan Taylor and Tacha Hicks and Christophe Champod},
   doi = {10.1016/j.scijus.2016.06.010},
   issn = {18764452},
   issue = {5},
   journal = {Science and Justice},
   keywords = {Bayesian networks,Data,Likelihood ratio,Sensitivity analysis,Source level propositions},
   month = {9},
   pages = {402-410},
   publisher = {Forensic Science Society},
   title = {Using sensitivity analyses in Bayesian Networks to highlight the impact of data paucity and direct future analyses: a contribution to the debate on measuring and reporting the precision of likelihood ratios},
   volume = {56},
   year = {2016},
}
@article{Schaapveld2019,
   abstract = {In court, it is typical for biological evidence to be reported at a level that only addresses how likely the DNA evidence is if it originated from a particular individual, or individuals. However, there are other questions that could be considered that would be of value in enabling the court, including the jury, to make better informed decisions. For example, although answers to specific questions such as: “Which type of bodily fluid has the DNA originated from?” or, “How was the DNA deposited at the scene?” would be probabilistic in nature, they can be crucial to the outcome of a case. The relationship between the DNA evidence, the source of the DNA and the activity that took place is described in a term called the “hierarchy of propositions.” Currently, such questions are usually answered by scientists subjectively with little to no logical framework to assist them. Bayesian networks have proven to be beneficial in providing logical reasoning by way of a likelihood ratio to help combine subjective, yet, experience‐based, opinions of experts with experimental data when answering questions which can be both complex and uncertain. These networks offer a framework that provides balance, transparency, and robustness in the evaluation of evidence. A current limitation of the use of Bayesian networks includes a lack of understanding of the underlying concepts from both forensic scientists and the courts and consequently a reduced recognition of the potential strengths. This article is categorized under:   Forensic Biology > Interpretation of Biological Evidence   },
   author = {Tayla E. M. Schaapveld and Stephanie L. Opperman and SallyAnn Harbison},
   doi = {10.1002/wfs2.1325},
   issn = {2573-9468},
   issue = {3},
   journal = {WIREs Forensic Science},
   month = {5},
   publisher = {Wiley},
   title = {Bayesian networks for the interpretation of biological evidence},
   volume = {1},
   year = {2019},
}
@article{Aguilera2010,
   abstract = {Bayesian networks are one of the most powerful tools in the design of expert systems located in an uncertainty framework. However, normally their application is determined by the discretization of the continuous variables. In this paper the naïve Bayes (NB) and tree augmented naïve Bayes (TAN) models are developed. They are based on Mixtures of Truncated Exponentials (MTE) designed to deal with discrete and continuous variables in the same network simultaneously without any restriction. The aim is to characterize the habitat of the spur-thighed tortoise (Testudo graeca graeca), using several continuous environmental variables, and one discrete (binary) variable representing the presence or absence of the tortoise. These models are compared with the full discrete models and the results show a better classification rate for the continuous one. Therefore, the application of continuous models instead of discrete ones avoids loss of statistical information due to the discretization. Moreover, the results of the TAN continuous model show a more spatially accurate distribution of the tortoise. The species is located in the Doñana Natural Park, and in semiarid habitats. The proposed continuous models based on MTEs are valid for the study of species predictive distribution modelling. © 2010 Elsevier Ltd.},
   author = {P. A. Aguilera and A. Fernández and F. Reche and R. Rumí},
   doi = {10.1016/j.envsoft.2010.04.016},
   issn = {13648152},
   issue = {12},
   journal = {Environmental Modelling and Software},
   keywords = {Classification,Conservation planning,Hybrid Bayesian networks,Mixtures of truncated exponentials},
   month = {12},
   pages = {1630-1639},
   title = {Hybrid Bayesian network classifiers: Application to species distribution models},
   volume = {25},
   year = {2010},
}
@article{Taylor2018,
   abstract = {The hierarchy of propositions has been accepted amongst the forensic science community for some time. It is also accepted that the higher up the hierarchy the propositions are, against which the scientist are competent to evaluate their results, the more directly useful the testimony will be to the court. Because each case represents a unique set of circumstances and findings, it is difficult to come up with a standard structure for evaluation. One common tool that assists in this task is Bayesian networks (BNs). There is much diversity in the way that BN can be constructed. In this work, we develop a template for BN construction that allows sufficient flexibility to address most cases, but enough commonality and structure that the flow of information in the BN is readily recognised at a glance. We provide seven steps that can be used to construct BNs within this structure and demonstrate how they can be applied, using a case example.},
   author = {Duncan Taylor and Alex Biedermann and Tacha Hicks and Christophe Champod},
   doi = {10.1016/j.fsigen.2017.12.006},
   issn = {18780326},
   journal = {Forensic Science International: Genetics},
   keywords = {Activity level propositions,Bayesian networks,DNA,Data,Evidence evaluation,Likelihood ratio},
   month = {3},
   pages = {136-146},
   pmid = {29275089},
   publisher = {Elsevier Ireland Ltd},
   title = {A template for constructing Bayesian networks in forensic biology cases when considering activity level propositions},
   volume = {33},
   year = {2018},
}
@misc{HuginAPI1990,
   author = {Hugin Expert A/S},
   title = {HUGIN API REFERENCE MANUAL},
   year = {1990},
}
@misc{Koller2013,
   abstract = {Bayesian networks provide a modeling language and associated inference algorithm for stochastic domains. They have been successfully applied in a variety of medium-scale applications. However, when faced with a large complex domain, the task of modeling using Bayesian networks begins to resemble the task of pro­ gramming using logical circuits. In this paper, we de­ scribe an object-oriented Bayesian network (OOBN) lan­ guage, which allows complex domains to be described in terms of interrelated objects. We use a Bayesian net­ work fragment to describe the probabilistic relations be­ tween the attributes of an object. These attributes can themsel ves be objects, providing a natural framework for encoding part-of hierarchies. Classes are used to pro­ vide a reusable probabilistic model which can be applied to multiple similar objects. Classes also support inher­ itance of model fragments from a class to a subclass, allowing the common aspects of related classes to be defined only once. Our language has clear declarative semantics: an OOBN can be interpreted as a stochas­ tic functional program, so that it uniquely specifies a probabilistic model. We provide an inference algorithm for OOBNs, and show that much of the structural infor­ mation encoded by an OOBN-particularly the encap­ sulation of variables within an object and the reuse of model fragments in different contexts-can also be used to speed up the inference process.},
   author = {Daphne Koller},
   title = {Object-Oriented Bayesian Networks},
   year = {2013},
}
@article{Uitdehaag2022,
   abstract = {Forensic soil comparisons can be of high evidential value in a forensic case, but become complex when multiple methods and factors are considered. Bayesian networks are well suited to support forensic practitioners in complex casework. This study discusses the structure of a Bayesian network, elaborates on the in- and output data and evaluates two examples, one using source level propositions and one using activity level propositions. These examples can be applied as a template to construct a case specific network and can be used to assess sensitivity of the target output to different factors and identify avenues for research.},
   author = {S. C.A. Uitdehaag and T. H. Donders and I. Kuiper and F. Wagner-Cremer and M. J. Sjerps},
   doi = {10.1016/j.scijus.2022.02.005},
   issn = {18764452},
   issue = {2},
   journal = {Science and Justice},
   keywords = {Activity level,Bayesian network,Distance measure,Elemental composition,Evaluate propositions,Palynology,Soil comparison,Source level},
   month = {3},
   pages = {229-238},
   pmid = {35277237},
   publisher = {Forensic Science Society},
   title = {Use of Bayesian networks in forensic soil casework},
   volume = {62},
   year = {2022},
}
@article{MarchTaroni2020,
   abstract = {The assessment of different items of evidence is a challenging process in forensic science, particularly when the relevant elements support different inferential directions. In this study, a model is developed to assess the joint probative value of three different analyses related to some biological material retrieved on an object of interest in a criminal case. The study shows the ability of probabilistic graphical models, say Bayesian networks, to deal with complex situations, those that one expects to face in real cases. The results obtained by the model show the importance of a conflict measure as an indication of inconsistencies in the model itself. A contamination event alleged by the defense is also introduced in the model to explain and solve the conflict. The study aims to give an insight in the application of a probabilistic model to real criminal cases.},
   author = {Ilaria De March and Franco Taroni},
   doi = {10.1016/j.fsigen.2019.102172},
   issn = {18780326},
   journal = {Forensic Science International: Genetics},
   keywords = {Activity level interpretation,Bayesian networks,Conflict measure,DNA evidence},
   month = {1},
   pmid = {31629186},
   publisher = {Elsevier Ireland Ltd},
   title = {Bayesian networks and dissonant items of evidence: A case study},
   volume = {44},
   year = {2020},
}
@article{Samie2022,
   abstract = {Chemical and staining methods, immunochromatography, spectroscopy, RNA expression or methylation patterns, do not allow to determine the nature of the biological material with certainty. However, to our knowledge, there are few forensic scientists that assess the value of such test results using a probabilistic approach. This is surprising as it would allow account for false positives and false negatives and avoid misleading conclusions. In this paper, we developed three Bayesian Networks (BNs) to assess the presence of blood, saliva and sperm in the recovered material and combine potentially contradictory observations. The approach was successfully tested using 188 traces from proficiency tests. We have implemented an online user-friendly application (https://forensic-genetic.shinyapps.io/BodyFluidsApp/) that allows forensic scientists to assess the value of their results without having to build Bayesian Networks themselves. They can also input their own data, use the application to identify a potential lack of knowledge and report their conclusions regarding the presence of sperm, blood or/and saliva considering uncertainty.},
   author = {Lydie Samie and Christophe Champod and Séverine Delémont and Patrick Basset and Tacha Hicks and Vincent Castella},
   doi = {10.1016/j.forsciint.2022.111174},
   issn = {18726283},
   journal = {Forensic Science International},
   keywords = {Body Fluid detection,Christmas Tree staining,Likelihood ratio,OBTI,PSA,RSID Saliva},
   month = {2},
   pmid = {34999364},
   publisher = {Elsevier Ireland Ltd},
   title = {Use of Bayesian Networks for the investigation of the nature of biological material in casework},
   volume = {331},
   year = {2022},
}
@article{Cale2016,
   abstract = {The occurrence of secondary DNA transfer has been previously established. However, the transfer of DNA through an intermediary has not been revisited with more sensitive current technologies implemented to increase the likelihood of obtaining results from low-template/low-quality samples. This study evaluated whether this increased sensitivity could lead to the detection of interpretable secondary DNA transfer profiles. After two minutes of hand to hand contact, participants immediately handled assigned knives. Swabbings of the knives with detectable amounts of DNA were amplified with the Identifiler® Plus Amplification Kit and injected on a 3130xl. DNA typing results indicated that secondary DNA transfer was detected in 85% of the samples. In five samples, the secondary contributor was either the only contributor or the major contributor identified despite never coming into direct contact with the knife. This study demonstrates the risk of assuming that DNA recovered from an object resulted from direct contact.},
   author = {Cynthia M. Cale and Madison E. Earll and Krista E. Latham and Gay L. Bush},
   doi = {10.1111/1556-4029.12894},
   issn = {15564029},
   issue = {1},
   journal = {Journal of Forensic Sciences},
   keywords = {Criminalistics,DNA analysis,Forensic casework,Forensic science,Identifiler® Plus,Secondary transfer},
   month = {1},
   pages = {196-203},
   pmid = {26331369},
   publisher = {Blackwell Publishing Inc.},
   title = {Could Secondary DNA Transfer Falsely Place Someone at the Scene of a Crime?},
   volume = {61},
   year = {2016},
}
@inproceedings{Andersen1989,
   abstract = {Causal probabilistic networks have proved to be a useful knowledge representation tool for modelling domains where causal relations in a broad sense are a natural way of relating domain objects and where uncertainty is inherited in these relations. This paper outlines an implementation the HUGIN shell-for handling a domain model expressed by a causal probabilistic network. The only topological restriction imposed on the network is that, it must not contain any directed loops. The approach is illustrated step by step by solving a. genetic breeding problem. A graph representation of the domain model is interactively created by using instances of the basic network components-nodes and arcs-as building blocks. This structure, together with the quantitative relations between nodes and their immediate causes expressed as conditional probabilities, are automatically transformed into a tree structure, a junction tree. Here a computationally efficient and conceptually simple algebra of Bayesian belief universes supports incorporation of new evidence, propagation of information , and calculation of revised beliefs in the states of the nodes in the network. Finally, as an exam ple of a real world application, MUN1N an expert system for electromyography is discussed.},
   author = {Stig K Andersen and Kristian G Olesen and Finn V Jensen and Frank Jensen},
   journal = {Conference: Proceedings of the 11th international joint conference on Artificial intelligence - Volume 2},
   title = {HUGIN*-a Shell for Building Bayesian Belief Universes for Expert Systems},
   year = {1989},
}
@article{Belyi2024,
   abstract = {Retriever Augmented Generation (RAG) systems have become pivotal in enhancing the capabilities of language models by incorporating external knowledge retrieval mechanisms. However, a significant challenge in deploying these systems in industry applications is the detection and mitigation of hallucinations: instances where the model generates information that is not grounded in the retrieved context. Addressing this issue is crucial for ensuring the reliability and accuracy of responses generated by large language models (LLMs) in diverse industry settings. Current hallucination detection techniques fail to deliver accuracy, low latency, and low cost simultaneously. We introduce Luna: a DeBERTA-large (440M) encoder, finetuned for hallucination detection in RAG settings. We demonstrate that Luna outperforms GPT-3.5 and commercial evaluation frameworks on the hallucination detection task, with 97% and 91% reduction in cost and latency, respectively. Luna is lightweight and generalizes across multiple industry verticals and out-of-domain data, making it an ideal candidate for industry LLM applications.},
   author = {Masha Belyi and Robert Friel and Shuai Shao and Atindriyo Sanyal},
   month = {6},
   title = {Luna: An Evaluation Foundation Model to Catch Language Model Hallucinations with High Accuracy and Low Cost},
   url = {http://arxiv.org/abs/2406.00975},
   year = {2024},
}
@article{Singh2023,
   abstract = {While advancing rapidly, Artificial Intelligence still falls short of human intelligence in several key aspects due to inherent limitations in current AI technologies and our understanding of cognition. Humans have an innate ability to understand context, nuances, and subtle cues in communication, which allows us to comprehend jokes, sarcasm, and metaphors. Machines struggle to interpret such contextual information accurately. Humans possess a vast repository of common-sense knowledge that helps us make logical inferences and predictions about the world. Machines lack this innate understanding and often struggle with making sense of situations that humans find trivial. In this article, we review the prospective Machine Intelligence candidates, a review from Prof. Yann LeCun, and other work that can help close this gap between human and machine intelligence. Specifically, we talk about what's lacking with the current AI techniques such as supervised learning, reinforcement learning, self-supervised learning, etc. Then we show how Hierarchical planning-based approaches can help us close that gap and deep-dive into energy-based, latent-variable methods and Joint embedding predictive architecture methods.},
   author = {Apoorv Singh},
   month = {8},
   title = {A Review on Objective-Driven Artificial Intelligence},
   url = {http://arxiv.org/abs/2308.10135},
   year = {2023},
}
@article{Taylor2019,
   abstract = {Trace DNA and the manner in which it is transferred from item to item is a common topic arising in forensic science, both in case evaluations, and in Court testimony. In order to assign the probability of obtaining DNA findings, given competing propositions that specify transfer mechanisms, consideration must be given to a number of factors. Previous work by the authors developed a simple Object-Oriented Bayesian Network (OOBN) that pooled numerous published studies in order to attempt evaluation of trace DNA results given such propositions. In this work, we expand on the previously published OOBN and formalise a number of class networks that can be used together in a logical way to consider DNA movement through complex chains of transfer events. Specifically, we develop an OOBN that considers the two-way transfer of DNA that occurs when two items contact, and allows for the sampling of intermediary items involved in the chain of transfers. The aim is to show that adopting an approach involving basic building blocks, we offer the possibility to tackle complex and various cases for which the OOBN will be obtained by combining their elementary blocks. We conclude with a demonstration of applying the OOBN being applied to a chain of transfers in a case scenario.},
   author = {Duncan Taylor and Lydie Samie and Christophe Champod},
   doi = {10.1016/j.fsigen.2019.06.006},
   issn = {18780326},
   journal = {Forensic Science International: Genetics},
   keywords = {Activity level propositions,Background DNA,Contact DNA,DNA persistence,DNA transfer,Object-oriented Bayesian networks},
   month = {9},
   pages = {69-80},
   pmid = {31234042},
   publisher = {Elsevier Ireland Ltd},
   title = {Using Bayesian networks to track DNA movement through complex transfer scenarios},
   volume = {42},
   year = {2019},
}
@article{Kokshoorn2018,
   abstract = {Sharing data between forensic scientists on DNA transfer, persistence, prevalence and recovery (TPPR) is crucial to advance the understanding of these issues in the criminal justice community. We present the results of a collaborative exercise on reporting forensic genetics findings given activity level propositions. This exercise outlined differences in the methodology that was applied by the participating laboratories, as well as limitations to the use of published data on DNA TPPR. We demonstrate how publication of experimental results in scientific journals can be further improved to allow for an adequate use of these data. Steps that can be taken to share and use these data for research and casework purposes are outlined, and the prospects for future sharing of data through publicly accessible databases are discussed. This paper also explores potential avenues to proceed with implementation and is intended to fuel the discussion on sharing data pertaining to DNA TPPR issues. It is further suggested that international standardization and harmonization on these topics will benefit the forensic DNA community as it has been achieved in the past with the harmonization of STR typing systems.},
   author = {Bas Kokshoorn and Lambertus H.J. Aarts and Ricky Ansell and Edward Connolly and Weine Drotz and Ate D. Kloosterman and Louise G. McKenna and Bianca Szkuta and Roland A.H. van Oorschot},
   doi = {10.1016/j.fsigen.2018.09.006},
   issn = {18780326},
   journal = {Forensic Science International: Genetics},
   keywords = {Activity level inference,Criminalistics,DNA persistence,DNA prevalence,DNA recovery,DNA transfer,Data exchange,Database,Forensic biology,Trace DNA},
   month = {11},
   pages = {260-269},
   pmid = {30273824},
   publisher = {Elsevier Ireland Ltd},
   title = {Sharing data on DNA transfer, persistence, prevalence and recovery: Arguments for harmonization and standardization},
   volume = {37},
   year = {2018},
}
@article{Lewis2020,
   abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
   author = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
   month = {5},
   title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
   url = {http://arxiv.org/abs/2005.11401},
   year = {2020},
}
@article{Tay2020,
   abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
   author = {Yi Tay and Mostafa Dehghani and Dara Bahri and Donald Metzler},
   month = {9},
   title = {Efficient Transformers: A Survey},
   url = {http://arxiv.org/abs/2009.06732},
   year = {2020},
}
@article{Szkuta2018,
   abstract = {Questions relating to how DNA from an individual got to where it was recovered from and the activities associated with its pickup, retention and deposition are increasingly relevant to criminal investigations and judicial considerations. To address activity level propositions, investigators are typically required to assess the likelihood that DNA was transferred indirectly and not deposited through direct contact with an item or surface. By constructing a series of Bayesian networks, we demonstrate their use in assessing activity level propositions derived from a recent legal case involving the alleged secondary transfer of DNA to a surface following a handshaking event. In the absence of data required to perform the assessment, a set of handshaking simulations were performed to obtain probabilities on the persistence of non-self DNA on the hands following a 40 min, 5 h or 8 h delay between the handshake and contact with the final surface (an axe handle). Variables such as time elapsed, and the activities performed and objects contacted between the handshake and contact with the axe handle, were also considered when assessing the DNA results. DNA from a known contributor was transferred to the right hand of an opposing hand-shaker (as a depositor), and could be subsequently transferred to, and detected on, a surface contacted by the depositor 40 min to 5 h post-handshake. No non-self DNA from the known contributor was detected in deposits made 8 h post-handshake. DNA from the depositor was generally detected as the major or only contributor in the profiles generated. Contributions from the known contributor were minor, decreasing in presence and in the strength of support for inclusion as the time between the handshake and transfer event increased. The construction of a series of Bayesian networks based on the case circumstances provided empirical estimations of the likelihood of direct or indirect deposition. The analyses and conclusions presented demonstrate both the complexity of activity level assessments concerning DNA evidence, and the power of Bayesian networks to visualise and explore the issues of interest for a given case.},
   author = {Bianca Szkuta and Kaye N. Ballantyne and Bas Kokshoorn and Roland A.H. van Oorschot},
   doi = {10.1016/j.fsigen.2017.11.017},
   issn = {18780326},
   journal = {Forensic Science International: Genetics},
   keywords = {Activity level propositions,Bayesian network,DNA transfer,Handshaking},
   month = {3},
   pages = {84-97},
   pmid = {29216581},
   publisher = {Elsevier Ireland Ltd},
   title = {Transfer and persistence of non-self DNA on hands over time: Using empirical data to evaluate DNA evidence given activity level propositions},
   volume = {33},
   year = {2018},
}
@misc{Gosch2019,
   abstract = {Since DNA from touched items and surfaces (“touch DNA”) can successfully and reliably be analyzed, the question as to how a particular DNA containing sample came to be from where it was recovered is of increasing forensic interest and expert witnesses in court are increasingly challenged to assess for instance whether an incriminatory DNA sample matching to a suspect could have been transferred to the crime scene in an innocent manner and to guess at the probability of such an occurrence. The latter however will frequently entail expressing a subjective probability i.e. simply making a best guess from experience. There is, to the present date, an extensive and complex body of literature on primary, secondary, tertiary and even higher order DNA transfer, its possibility, plausibility, dependency on an array of variables and factors and vast numbers of permutations thereof. However, from our point of view there is a lack of systematic data on DNA transfer with existing research widely varying in quality and relevance. Our aim was, starting from a comprehensive survey of the status quo and appreciating its increasing importance, to in the first part of our review raise consciousness towards the underestimated and insufficiently accounted for complexity of DNA transfer and thus appendant research of forensic scientists serving as expert witnesses in court but also acting in the role of a journal referee to point them to areas of criticism when reviewing a manuscript on DNA transfer. In the second part, we present propositions how to systematize and integrate future research efforts concerning DNA transfer. Also, we present a searchable database providing an extensive overview of the current state of knowledge on DNA transfer, intended to facilitate the identification of relevant studies adding knowledge to a specific question and thus help forensic experts to base their opinion on a broader, more complete and more reproducible selection of studies.},
   author = {Annica Gosch and Cornelius Courts},
   doi = {10.1016/j.fsigen.2019.01.012},
   issn = {18780326},
   journal = {Forensic Science International: Genetics},
   keywords = {DNA transfer,Forensic genetics,Touch DNA,Trace DNA},
   month = {5},
   pages = {24-36},
   pmid = {30731249},
   publisher = {Elsevier Ireland Ltd},
   title = {On DNA transfer: The lack and difficulty of systematic research and how to do it better},
   volume = {40},
   year = {2019},
}
@article{Samie2016,
   abstract = {Technical developments have made it possible to analyze very low amounts of DNA. This has many advantages, but the drawback of this technological progress is that interpretation of the results becomes increasingly complex: the number of mixed DNA profiles increased relatively to single source DNA profiles and stochastic effects in the DNA profile, such as drop-in and drop-out, are more frequently observed. Moreover, the relevance of low template DNA material regarding the activities alleged is not as straightforward as it was a few years ago, when for example large quantities of blood were recovered. The possibility of secondary and tertiary transfer is now becoming an issue. The purpose of this research is twofold: first, to study the transfer of DNA from the handler and secondly, to observe if handlers would transfer DNA from persons closely connected to them. We chose to mimic cases where the offender would attack a person with a knife. As a first approach, we envisaged that the defense would not give an alternative explanation for the origin of the DNA. In our transfer experiments (4 donors, 16 experiments each, 64 traces), 3% of the traces were single DNA profiles. Most of the time, the DNA profile of the person handling the knife was present as the major profile: in 83% of the traces the major contributor profile corresponded to the stabber's DNA profile (in single stains and mixtures). Mixture with no clear major/minor fraction (12%) were observed. 5% of the traces were considered of insufficient quality (more than 3 contributors, presence of a few minor peaks). In that case, we considered that the stabber's DNA was absent. In our experiments, no traces allowed excluding the stabber, however it must be noted that precautions were taken to minimize background DNA as knives were cleaned before the experiments. DNA profiles of the stabber's colleagues were not observed. We hope that this study will allow for a better understanding of the transfer mechanism and of how to assess and describe results given activity level propositions. In this preliminary research, we have focused on the transfer of DNA on the hand of the person. Besides, more research is needed to assign the probability of the results given an alternative activity proposed by the defense, for instance when the source of the DNA is not contested, but that the activities are.},
   author = {Lydie Samie and Tacha Hicks and Vincent Castella and Franco Taroni},
   doi = {10.1016/j.fsigen.2016.02.001},
   issn = {18780326},
   journal = {Forensic Science International: Genetics},
   keywords = {Activity level propositions,Bayesian framework,DNA,Knives,Likelihood ratio,STRmix,Transfer},
   month = {5},
   pages = {73-80},
   pmid = {26875110},
   publisher = {Elsevier Ireland Ltd},
   title = {Stabbing simulations and DNA transfer},
   volume = {22},
   year = {2016},
}
@article{Lehmann2015,
   abstract = {Abstract DNA transfer is of increasing importance in crime scene situations, partly due to analytical techniques detecting profiles in ever declining amounts of DNA. Whereas the focus has previously been DNA transfer of target sources, the effects of background DNA on transfer and detection of DNA after multiple contact situations have been much less investigated. This study measured the transfer and detection rates of a specific DNA source in the presence of background DNA sources. The presence of background DNA influenced the transfer of DNA differently depending on the combination of biological material and surface type. The detection of a profile from the target DNA decreased after multiple contact situations, due to the reduced total and relative quantity of target DNA, and the increasing complexity of the mixture. The results of this study contribute to a greater understanding of the effects of background DNA sources on DNA transfer and detection.},
   author = {V. J. Lehmann and R. J. Mitchell and K. N. Ballantyne and R. A.H. Van Oorschot},
   doi = {10.1016/j.fsigen.2015.05.002},
   issn = {18780326},
   journal = {Forensic Science International: Genetics},
   keywords = {Background DNA,Contamination,DNA,Forensic science,Trace,Transfer},
   month = {7},
   pages = {68-75},
   pmid = {26143222},
   publisher = {Elsevier Ireland Ltd},
   title = {Following the transfer of DNA: How does the presence of background DNA affect the transfer and detection of a target source of DNA?},
   volume = {19},
   year = {2015},
}
@article{Gill2021,
   abstract = {Bayesian logistic regression is used to model the probability of DNA recovery following direct and secondary transfer and persistence over a 24 h period between deposition and sample collection. Sub-source level likelihood ratios provided the raw data for activity-level analysis. Probabilities of secondary transfer are typically low, and there are challenges with small data-sets with low numbers of positive observations. However, the persistence of DNA over time can be modelled by a single logistic regression for both direct and secondary transfer, except that the time since deposition must be compensated by an offset value for the latter. This simplifies the analysis. Probabilities are used to inform an activity-level Bayesian Network that takes account of alternative propositions e.g. time of assault and time of social activities. The model is extended in order to take account of multiple contacts between person of interest and ’victim’. Variables taken into account include probabilities of direct and secondary transfer, along with background DNA from unknown individuals. The logistic regression analysis is Bayesian — for each analysis, 4000 separate simulations were carried out. Quantile assignments enable calculation of a plausible range of probabilities and sensitivity analysis is used to describe the corresponding variation of LRs that occur when modelled by the Bayesian network. It is noted that there is need for consistent experimental design, and analysis, to facilitate inter-laboratory comparisons. Appropriate recommendations are made. The open-source program written in R-code ALTRaP (Activity Level, Transfer, Recovery and Persistence) enables analysis of complex multiple transfer propositions that are commonplace in cases-work e.g. between those who cohabit. A number of case examples are provided. ALTRaP can be used to replicate the results and can easily be modified to incorporate different sets of data and variables.},
   author = {Peter Gill and Øyvind Bleka and Arne Roseth and Ane Elida Fonneløp},
   doi = {10.1016/j.fsigen.2021.102509},
   issn = {18780326},
   journal = {Forensic Science International: Genetics},
   keywords = {ALTRaP,Bayesian network,Direct transfer,Evidence evaluation,Likelihood ratio (LR),Mixtures,Secondary transfer},
   month = {7},
   pmid = {33930816},
   publisher = {Elsevier Ireland Ltd},
   title = {An LR framework incorporating sensitivity analysis to model multiple direct and secondary transfer events on skin surface},
   volume = {53},
   year = {2021},
}
@article{Szkuta2017,
   abstract = {During the evaluation of forensic DNA evidence in court proceedings, the emphasis previously placed on the source of the DNA is progressively shifting to the consideration of the activities resulting in its deposition. While direct contact and deposition may be a likely explanation, alternative scenarios involving DNA transfer through a secondary person or medium are important to consider. Here we assessed whether non-self DNA, indirectly transferred via a handshake, could be detected on surfaces contacted by the opposing hand-shaker after 15 min, and considered the variables affecting its persistence in subsequent contacts. In general, the depositor of the handprint was the major contributor to DNA profiles collected from handprints placed on glass plates. Minor contributions from the opposing hand-shaker (as a known contributor) were detected at a lower rate, decreasing as the number of contacted items increased post-handshake. Delays in deposition also affected the detection of the opposing hand-shaker, with a 15 min delay between handshaking and contact resulting in the reduced presence, and corresponding LRs, of the known contributor. The handprint depositor was excluded from their own handprint on several occasions, including instances where the opposing hand-shaker was not excluded from the same profile. Several factors appeared to strongly influence the detection of both the depositor and contributing individual involved in the handshake. The relative shedding ability of the pair had the largest effect, where good shedders (whether depositor or contributor) could swamp poor to moderate shedders, while the pairing of two moderate or two poor shedders could result in the detection of both individuals. When the deposition of a handprint was delayed, the activities performed by the individual had a substantial effect on the resultant detection of the contributing profile – multiple contacts with the same items increased the likelihood that the known contributor's DNA would be retained and subsequently detected, through the parking and re-transfer of DNA on used items.},
   author = {Bianca Szkuta and Kaye N. Ballantyne and Roland A.H. van Oorschot},
   doi = {10.1016/j.fsigen.2017.01.006},
   issn = {18780326},
   journal = {Forensic Science International: Genetics},
   keywords = {DNA,activity,forensic,handshake,transfer},
   month = {5},
   pages = {10-20},
   pmid = {28126692},
   publisher = {Elsevier Ireland Ltd},
   title = {Transfer and persistence of DNA on the hands and the influence of activities performed},
   volume = {28},
   year = {2017},
}
@article{Szkuta2020,
   abstract = {Cellular material derived from contact traces can be transferred via many direct and indirect routes, with the manner of contact and the time of transfer (in relation to the alleged crime-event) having an impact on whether DNA is recovered from the surface and a reportable profile generated. In an effort to acquire information on the transfer and recovery of DNA traces from clothing items worn during scenarios commonly encountered in casework, upper garments were worn during a normal working day before individuals were paired to embrace one another (‘contact’), go on an outing together (‘close proximity’), or individually asked to spend a day in another person's environment (‘physical absence’). Each prescribed activity was repeated by sixteen individuals across four countries, and was the last activity performed before the garment was removed. Samples were collected from several areas of the upper garments and processed from DNA extraction through to profiling within the laboratory of the country in which the individual resided. Activities relating to the garment prior to and during wearing, including the prescribed activity, were recorded by the participant and considered during the interpretation of results. In addition to obtaining reference profiles from the wearer and their activity partner, DNA profiles from the wearers’ close associates identified in the questionnaire were obtained to assess the impact of background DNA transferred prior to the prescribed activity. The wearer was typically, but not always, observed as the major contributor to the profiles obtained. DNA from the activity partner was observed on several areas of the garment following the embrace and after temporarily occupying another person's space. Particular areas of the garment were more prone to acquiring the hugging partner or office owner's DNA than others, and whether they were observed as the major or minor component was activity dependent. For each of the pairs, no DNA from the activity partner was acquired by the garments during the outing, even though both participants were in close proximity. This study provides empirical data on the transfer, persistence, prevalence and recovery of DNA from clothing items, and enables a better understanding of the mechanisms which lead to the transfer and detectability of DNA traces in different scenarios.},
   author = {Bianca Szkuta and Ricky Ansell and Lina Boiso and Edward Connolly and Ate D. Kloosterman and Bas Kokshoorn and Louise G. McKenna and Kristy Steensma and Roland A.H. van Oorschot},
   doi = {10.1016/j.fsigen.2020.102268},
   issn = {18780326},
   journal = {Forensic Science International: Genetics},
   keywords = {Activity level,Clothing,DNA persistence,DNA prevalence,DNA recovery,DNA transfer},
   month = {5},
   pmid = {32172221},
   publisher = {Elsevier Ireland Ltd},
   title = {DNA transfer to worn upper garments during different activities and contacts: An inter-laboratory study},
   volume = {46},
   year = {2020},
}
@article{Lang2023,
   abstract = {This paper describes the construction and use of a machine-learning model to provide objective support for a physical fit examination of duct tapes. We present the ForensicFit package that can preprocess and database raw tape images. Using the processed tape image, we trained a convolutional neural network to compare tape edges and predict membership scores (i.e., fit or non-fit category). A dataset of nearly 2000 tapes and 4000 images was evaluated, including various quality grades: low, medium, and high, as well as two separation methods, scissor-cut and hand-torn. The model predicts medium-quality and high-quality scissor-cut tape more accurately than hand-torn, whereas for low-quality tape predicts the hand-torn tapes more accurately. These results are consistent with previous studies performed on the same datasets by analyst examinations. A method of pixel importance was also implemented to show which pixels are used to make the decision. This method can confirm some fit features that correspond with analyst-identified features, like edge morphology and backing pattern. This pilot study demonstrates the feasibility of computational algorithms to build physical fit databases and automated comparisons using deep neural networks, which can be used as a model for other materials.},
   author = {Logan Lang and Pedram Tavadze and Meghan Prusinowski and Zachary Andrews and Cedric Neumann and Tatiana Trejos and Aldo H. Romero},
   doi = {10.1016/j.forsciint.2023.111884},
   issn = {18726283},
   journal = {Forensic Science International},
   keywords = {Convolutional neural networks,Decision tree,Duct tape,Machine learning,Physical fit},
   month = {12},
   pmid = {37989070},
   publisher = {Elsevier Ireland Ltd},
   title = {Using convolutional neural networks to support examiners in duct tape physical fit comparisons},
   volume = {353},
   year = {2023},
}
@book{Bolstad2016,
   author = {William M. Bolstad and James M. Curran},
   edition = {3rd},
   publisher = {John Wiley & Sons, Inc.},
   title = {Introduction to Bayesian Statistics},
   year = {2016},
}
@article{Spaulding2023,
   abstract = {During the examination of trace evidence, often a realignment along the edges of known and questioned items are made to determine if a physical fit is present and if those objects were once one continuous piece or object. Duct tape is an evidence type in which the evaluation of physical fits is often conducted and is regarded as conclusive evidence of an association between the items. The examination and conclusion of a physical fit between edges relies heavily on examiner discretion to identify distinctive features across the edges since there are no statistical approaches or objective methodologies for the comparison. This study developed an automated image processing and comparison method to quantify tape end matches using cross-correlation scores and an empirical approach to the assessment. Characterization of 150 hand torn duct tape end pair physical fits were also conducted where matching and non-matching sample distributions were created. This study also evaluated partial duct tape edges and the influence this has on a comparison. Given the strength associated with a physical fit and the presence of stretching or deformation along the fractured edge, an understanding of the value these samples have is paramount. Furthermore, random match probabilities were calculated based on the correlation scores from the inter-comparisons to model the weight of evidence or strength of association between the edges. Finally, the study demonstrated that not every true match holds the same association strength through score distributions, but the approach is able to distinguish matching and non-matching samples at edge widths greater than 27 %.},
   author = {Jamie S. Spaulding and Gina M. Picconatto},
   doi = {10.1016/j.forsciint.2022.111519},
   issn = {18726283},
   journal = {Forensic Science International},
   keywords = {Algorithm,Edge similarity score,End matching,Physical fit,Trace evidence},
   month = {1},
   pmid = {36423360},
   publisher = {Elsevier Ireland Ltd},
   title = {Characterization of fracture match associations with automated image processing},
   volume = {342},
   year = {2023},
}
@article{Edge2024,
   abstract = {The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as "What are the main themes in the dataset?", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose a Graph RAG approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed. Our approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that Graph RAG leads to substantial improvements over a na\"ive RAG baseline for both the comprehensiveness and diversity of generated answers. An open-source, Python-based implementation of both global and local Graph RAG approaches is forthcoming at https://aka.ms/graphrag.},
   author = {Darren Edge and Ha Trinh and Newman Cheng and Joshua Bradley and Alex Chao and Apurva Mody and Steven Truitt and Jonathan Larson},
   month = {4},
   title = {From Local to Global: A Graph RAG Approach to Query-Focused Summarization},
   url = {http://arxiv.org/abs/2404.16130},
   year = {2024},
}
@misc{Astronomer2024,
   author = {Ben Gregory and Julia Wrzosińska},
   title = {What is a DAG? (Directed Acyclic Graph) | Astronomer},
   url = {https://www.astronomer.io/blog/what-exactly-is-a-dag/},
}
@book{Sridhar2024,
   city = {Cham},
   doi = {10.1007/978-3-031-56128-3},
   editor = {Narasi Sridhar},
   isbn = {978-3-031-56127-6},
   publisher = {Springer International Publishing},
   title = {Bayesian Network Modeling of Corrosion},
   url = {https://link.springer.com/10.1007/978-3-031-56128-3},
   year = {2024},
}
