
@article{soleymani_stats_2017,
	title = {Stats 707},
	author = {Soleymani, Mehdi},
	year = {2017},
}

@article{uoa_stats_2017,
	title = {{STATS} 784 {Statistical} {Data} {Mining} {Assignment} 4},
	number = {1},
	author = {UOA, Department of Statistics},
	year = {2017},
	pages = {6--7},
}

@article{jonge_introduction_2013,
	title = {An introduction to data cleaning with {R}},
	issn = {1572-0314},
	url = {http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf},
	doi = {60083 201313- X-10-13},
	abstract = {Data cleaning, or data preparation is an essential part of statistical analysis. In fact, in practice it is often more time-consuming than the statistical analysis itself. These lecture notes describe a range of techniques, implemented in the R statistical environment, that allow the reader to build data cleaning scripts for data suffering from a wide range of errors and inconsistencies, in textual format. These notes cover technical as well as subject-matter related aspects of data cleaning. Technical aspects include data reading, type conversion and string matching and manipulation. Subject-matter related aspects include topics like data checking, error localization and an introduction to imputation methods in R. References to relevant literature and R packages are provided throughout.},
	journal = {Statistics Netherlands},
	author = {Jonge, Edwin de and Loo, Mark van der},
	year = {2013},
	note = {ISBN: 1572-0314},
	keywords = {data editing, methodology, statistical software},
	pages = {53},
}

@misc{ict_comparison_nodate,
	title = {Comparison of {Storage} {Media}},
	url = {https://www.ictlounge.com/html/comparison_of_storage_media.htm},
	author = {ICT, IGCSE},
}

@misc{biosciences_home_2018,
	title = {Home},
	url = {https://www.pacb.com/},
	author = {Biosciences, Pacific},
	year = {2018},
}

@misc{scientific_home_2018,
	title = {Home},
	url = {https://www.thermofisher.com/us/en/home.html},
	author = {Scientific, ThermoFisher},
	year = {2018},
}

@article{takeuchi_bioinf703_2018,
	title = {{BIOINF703} : {Origin} of {Genomes}},
	author = {Takeuchi, Nobuto},
	year = {2018},
}

@article{beerli_comment_2007,
	title = {Comment on "{Population} {Size} {Does} {Not} {Influence} {Mitochondrial} {Genetic} {Diversity} in {Animals}" – {Mulligan} et al. 314 (5804): 1390a – {Science}},
	volume = {314},
	url = {papers3://publication/uuid/2DF4C5A0-2C73-4953-8993-ACE86A678C2E},
	number = {December},
	author = {Beerli, Peter},
	year = {2007},
	pages = {2--4},
}

@book{Orengo,
	title = {Bioinformatics: genes, proteins and computers},
	author = {Orengo, Christine and Jones, David and Thornton, Janet},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\79EM93W7\\(Advanced text) Christine Orengo_ David Jones_ Janet M Thornton -Bioinformatics _ genes, proteins, and computers-TF (2003).pdf:application/pdf},
}

@incollection{Frenkel2017,
	title = {Chapter 4. {Molecular} {Dynamics} {Simulations}},
	booktitle = {Understanding molecular simulation : from algorithms to applications.},
	author = {Frenkel, D. and Smit, B. and Smit, B.},
	year = {2017},
	note = {Issue: 2001},
	pages = {84--104},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ZDUTT8G3\\Understanding_Molecular_Simulation_From_Algorithms_Pg107-127.pdf:application/pdf},
}

@article{Park2015,
	title = {Stretching {Deca}-alanine},
	number = {September},
	author = {Park, Sanghyun},
	year = {2015},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\JFBNU36X\\10Ala-tutorial.pdf:application/pdf},
}

@article{Leimkuhler2015,
	title = {Molecular {Dynamics}},
	volume = {39},
	issn = {1520-5207},
	url = {http://link.springer.com/10.1007/978-3-319-16375-8},
	doi = {10.1007/978-3-319-16375-8},
	number = {2001},
	author = {Leimkuhler, Ben and Matthews, Charles},
	year = {2015},
	pmid = {20455590},
	note = {ISBN: 978-3-319-16374-1},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\XW26XT9Z\\Understanding_Molecular_Simulation_From_Algorithms_Pg86-100.pdf:application/pdf},
}

@article{Schneider,
	title = {Introduction to {Molecular} {Dynamics}},
	url = {http://link.springer.com/10.1007/978-3-540-74686-7_1},
	doi = {10.1007/978-3-540-74686-7_1},
	journal = {Computational Many-Particle Physics},
	author = {Schneider, Ralf and Sharma, Amit Raj and Rai, Abha},
	pages = {3--40},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\P7H4KID4\\Python1D-MD.pdf:application/pdf},
}

@article{So2017,
	title = {Page 1 of 4},
	author = {So, S and Amos, S},
	year = {2017},
	pages = {0--2},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\I8RI37T5\\1dmd.pdf:application/pdf},
}

@article{Welch2018,
	title = {Bioinf 703 {Networks} lab},
	number = {August},
	author = {Welch, David},
	year = {2018},
	pages = {1--2},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\KYLN6QBE\\networkLab.pdf:application/pdf},
}

@article{Merico2009,
	title = {How to visually interpret biological data using networks},
	volume = {27},
	issn = {10870156},
	url = {http://dx.doi.org/10.1038/nbt.1567},
	doi = {10.1038/nbt.1567},
	abstract = {Networks in biology can appear complex and difficult to decipher. We illustrate how to interpret biological networks with the help of frequently used visualization and analysis patterns.},
	number = {10},
	journal = {Nature Biotechnology},
	author = {Merico, Daniele and Gfeller, David and Bader, Gary D.},
	year = {2009},
	pmid = {19816451},
	note = {arXiv: NIHMS150003
Publisher: Nature Publishing Group
ISBN: 1546-1696 (Electronic){\textbackslash}r1087-0156 (Linking)},
	pages = {921--924},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\CPC7X2AS\\merico 2009 How to visually interpret biological data using networks.pdf:application/pdf},
}

@article{Taipale2018,
	title = {Informational limits of biological organisms},
	volume = {37},
	issn = {0261-4189},
	url = {http://emboj.embopress.org/lookup/doi/10.15252/embj.201696114},
	doi = {10.15252/embj.201696114},
	abstract = {Genomic analyses have revealed that free‐living biological organisms carry between 107 and 1011 bits of information in their genomes. In large organisms with relatively small population sizes, such as humans, only in the order of 1\% of the genomic information is shaped by the environment via natural selection. A much larger amount of information than this is routinely being generated by biomedical researchers, and the rapidly accumulating data is often interpreted to mean that biological systems are extremely complex. However, as the genome is finite in length, it cannot define precisely optimal values for the quantitative parameters of the experimentally identified molecular phenotypes. Furthermore, because the genomic sequences orchestrate a biochemical system that is much more information‐rich than the genome, the vast majority of the measured molecular phenotypes must represent “molecular spandrels”, that is phenotypes that are not independent of each other, and instead co‐determined by the same genomic sequences. These considerations are important in interpreting the results of individual experiments. In addition, they indicate that full understanding of biological systems requires a genome‐centric model that does not abstract away the information contained in the genome, and instead explicitly maps all phenotypic data back to specific genomic sequences.},
	number = {10},
	journal = {The EMBO Journal},
	author = {Taipale, Jussi},
	year = {2018},
	pmid = {29669861},
	pages = {e96114},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\GWJY7TCV\\Taipale18.pdf:application/pdf},
}

@article{Singer2004,
	title = {From "junk" to gene: {Curriculum} vitae of a primate receptor isoform gene},
	volume = {341},
	issn = {00222836},
	doi = {10.1016/j.jmb.2004.06.070},
	abstract = {Exonization of Alu retroposons awakens public opinion, particularly when causing genetic diseases. However, often neglected, alternative "Alu-exons" also carry the potential to greatly enhance genetic diversity by increasing the transcriptome of primates chiefly via alternative splicing. Here, we report a 5′ exon generated from one of the two alternative transcripts in human tumor necrosis factor receptor gene type 2 (p75TNFR) that contains an ancient Alu-SINE, which provides an alternative N-terminal protein-coding domain. We follow the primate evolution over the past 63 million years to reconstruct the key events that gave rise to a novel receptor isoform. The Alu integration and start codon formation occurred between 58 and 40 million years ago (MYA) in the common ancestor of anthropoid primates. Yet a functional gene product could not be generated until a novel splice site and an open reading frame were introduced between 40 and 25 MYA on the catarrhine lineage (Old World monkeys including apes). © 2004 Elsevier Ltd.},
	number = {4},
	journal = {Journal of Molecular Biology},
	author = {Singer, Silke S. and Männel, Daniela N. and Hehlgans, Thomas and Brosius, Jürgen and Schmitz, Jürgen},
	year = {2004},
	pmid = {15328599},
	note = {ISBN: 0022-2836 (Print){\textbackslash}n0022-2836 (Linking)},
	keywords = {Alu exonization, differentially spliced transcripts, gene evolution, primates, tumor necrosis factor receptor gene},
	pages = {883--886},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\PV5R5VUE\\Singer-Brosius-Schmitz04.pdf:application/pdf},
}

@article{Sep2013,
	title = {Tape rescues big data},
	url = {https://www.economist.com/blogs/babbage/2013/09/information-storage},
	journal = {The Economist},
	author = {Sep, Babbage and Collider, Large Hadron},
	year = {2013},
	pages = {5--9},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\A5RI5D3M\\ProQuestDocuments-2018-08-30.pdf:application/pdf},
}

@article{Seitz2001,
	title = {A {Novel} {p75TNF} {Receptor} {Isoform} {Mediating} {NFκB} {Activation}},
	volume = {276},
	issn = {00219258},
	doi = {10.1074/jbc.M101336200},
	abstract = {We report the identification of a novel p75TNF receptor isoform termed icp75TNFR, which is generated by the use of an alternative transcriptional start site within the p75TNFR gene and characterized by regulated intracellular expression. The icp75TNFR protein has an apparent molecular mass of approximately 50 kDa and is recognized by antibodies generated against the transmembrane form of p75TNFR. The icp75TNFR binds the tumor necrosis factor(TNF) and mediates intracellular signaling. Overexpression of the icp75TNFR cDNA results in TNF-induced activation of NFkappaB in a TNF receptor-associated factor 2 (TRAF2)-dependent manner. Thus, our results provide an example for intracellular cytokine receptor activation.},
	number = {22},
	journal = {Journal of Biological Chemistry},
	author = {Seitz, Carola and Müller, Peter and Krieg, René C. and Männel, Daniela N. and Hehlgans, Thomas},
	year = {2001},
	pmid = {11279196},
	note = {ISBN: 0021-9258 (Print){\textbackslash}r0021-9258 (Linking)},
	pages = {19390--19395},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\KG6MCV9I\\Seitz etal 2001.pdf:application/pdf},
}

@article{Poole2018,
	title = {Evaluating a published paper (7\%)},
	author = {Poole, Ant},
	year = {2018},
	pages = {1--2},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\Z3XD2PMB\\BIOSCI703_Lab1.pdf:application/pdf},
}

@article{Zimmermann2008,
	title = {{DNA} damage in preserved specimens and tissue samples: {A} molecular assessment},
	volume = {5},
	issn = {17429994},
	doi = {10.1186/1742-9994-5-18},
	abstract = {The extraction of genetic information from preserved tissue samples or museum specimens is a fundamental component of many fields of research, including the Barcode of Life initiative, forensic investigations, biological studies using scat sample analysis, and cancer research utilizing formaldehyde-fixed, paraffin-embedded tissue. Efforts to obtain genetic information from these sources are often hampered by an inability to amplify the desired DNA as a consequence of DNA damage.Previous studies have described techniques for improved DNA extraction from such samples or focused on the effect of damaging agents - such as light, oxygen or formaldehyde - on free nucleotides.We present ongoing work to characterize lesions in DNA samples extracted from preserved specimens. The extracted DNA is digested to single nucleosides with a combination of DNase I, Snake Venom Phosphodiesterase, and Antarctic Phosphatase and then analyzed by HPLC-ESI-TOF-MS.We present data for moth specimens that were preserved dried and pinned with no additional preservative and for frog tissue samples that were preserved in either ethanol, or formaldehyde, or fixed in formaldehyde and then preserved in ethanol. These preservation methods represent the most common methods of preserving animal specimens in museum collections. We observe changes in the nucleoside content of these samples over time, especially a loss of deoxyguanosine. We characterize the fragmentation state of the DNA and aim to identify abundant nucleoside lesions. Finally, simple models are introduced to describe the DNA fragmentation based on nicks and double-strand breaks.},
	journal = {Frontiers in Zoology},
	author = {Zimmermann, Juergen and Hajibabaei, Mehrdad and Blackburn, David C. and Hanken, James and Cantin, Elizabeth and Posfai, Janos and Evans, Thomas C.},
	year = {2008},
	pmid = {18947416},
	note = {ISBN: 1742-9994},
	pages = {1--13},
	file = {Zimmermann et al 2008.PDF:C\:\\Users\\jstacey\\Zotero\\storage\\RJC5LQPG\\Zimmermann et al 2008.PDF:application/pdf},
}

@inproceedings{Kazansky2016,
	address = {San Francisco, California},
	title = {Eternal {5D} data storage via ultrafast-laser writing in glass},
	isbn = {978-1-62841-971-9},
	url = {http://www.spie.org/x117492.xml},
	doi = {10.1117/2.1201603.006365},
	abstract = {Securely storing large amounts of information over relatively short timescales of 100 years, comparable to the span of the human memory, is a challenging problem. Conventional optical data storage technology used in CDs and DVDs has reached capacities of hundreds of gigabits per square inch, but its lifetime is limited to a decade. DNA based data storage can hold hundreds of terabytes per gram, but the durability is limited. The major challenge is the lack of appropriate combination of storage technology and medium possessing the advantages of both high capacity and long lifetime. The recording and retrieval of the digital data with a nearly unlimited lifetime was implemented by femtosecond laser nanostructuring of fused quartz. The storage allows unprecedented properties including hundreds of terabytes per disc data capacity, thermal stability up to 1000 °C, and virtually unlimited lifetime at room temperature opening a new era of eternal data archiving.},
	booktitle = {The {International} {Society} of {Optics} and {Photonics} - {LASE}},
	author = {Kazansky, Peter and Cerkauskaite, Ausra and Beresna, Martynas and Drevinskas, Rokas and Patel, Aabid and Zhang, Jingyu and Gecevicius, Mindaugas},
	year = {2016},
	note = {Issue: March 2016
ISSN: 18182259},
	keywords = {form birefringence, material processing, optical data multiplexing, ultrafast phenomena},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\G6MEI4IC\\Zhang 2016.pdf:application/pdf},
}

@article{TabatabaeiYazdi2015,
	title = {A {Rewritable}, {Random}-{Access} {DNA}-{Based} {Storage} {System}},
	volume = {5},
	issn = {20452322},
	url = {http://dx.doi.org/10.1038/srep14138},
	doi = {10.1038/srep14138},
	abstract = {We describe the first DNA-based storage architecture that enables random access to data blocks and rewriting of information stored at arbitrary locations within the blocks. The newly developed architecture overcomes drawbacks of existing read-only methods that require decoding the whole file in order to read one data fragment. Our system is based on new constrained coding techniques and accompanying DNA editing methods that ensure data reliability, specificity and sensitivity of access, and at the same time provide exceptionally high data storage capacity. As a proof of concept, we encoded parts of the Wikipedia pages of six universities in the USA, and selected and edited parts of the text written in DNA corresponding to three of these schools. The results suggest that DNA is a versatile media suitable for both ultrahigh density archival and rewritable storage applications.},
	journal = {Nature Scientific Reports},
	author = {Tabatabaei Yazdi, S. M.Hossein and Yuan, Yongbo and Ma, Jian and Zhao, Huimin and Milenkovic, Olgica},
	year = {2015},
	pmid = {26382652},
	note = {arXiv: 1505.02199
Publisher: Nature Publishing Group
ISBN: 2045-2322 (Electronic){\textbackslash}r2045-2322 (Linking)},
	pages = {1--10},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\JYVN9VVG\\Yazdi et al 2015.pdf:application/pdf},
}

@article{Lynch2006,
	title = {Streamlining and {Simplification} of {Microbial} {Genome} {Architecture}},
	volume = {60},
	issn = {0066-4227},
	url = {http://www.annualreviews.org/doi/10.1146/annurev.micro.60.080805.142300},
	doi = {10.1146/annurev.micro.60.080805.142300},
	abstract = {The genomes of unicellular species, particularly prokaryotes, are greatly reduced in size and simplified in terms of gene structure relative to those of multicellular eukaryotes. Arguments proposed to explain this disparity include selection for metabolic efficiency and elevated rates of deletion in microbes, but the evidence in support of these hypotheses is at best equivocal. An alternative explanation based on fundamental population-genetic principles is proposed here. By increasing the mutational target sizes of associated genes, most forms of nonfunctional DNA are opposed by weak selection. Free-living microbial species have elevated effective population sizes, and the consequent reduction in the power of random genetic drift appears to be sufficient to enable natural selection to inhibit the accumulation of excess DNA. This hypothesis provides a potentially unifying explanation for the continuity in genomic scaling from prokaryotes to multicellular eukaryotes, the divergent patterns of mitochondrial evolution in animals and land plants, and various aspects of genomic modification in microbial endosymbionts},
	number = {1},
	journal = {Annual Review of Microbiology},
	author = {Lynch, Michael},
	year = {2006},
	pmid = {16824010},
	note = {ISBN: 0066-4227 (Print)},
	keywords = {genome evolution, genomic streamlining, mutation, prokaryotes, random genetic drift, recombination},
	pages = {327--349},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\5A52QTLF\\Lynch06.pdf:application/pdf},
}

@article{Corley2016,
	title = {Selecting against accidental {RNA} interactions},
	volume = {5},
	issn = {2050084X},
	doi = {10.7554/eLife.13479},
	abstract = {Many genes carry information for making proteins. To make a protein, a working copy of the information stored in DNA is first copied into a molecule of messenger RNA. These RNA messages are then interpreted by the ribosome, the molecular machine that makes proteins. Many messages are produced from each gene, and each message can be read multiple times. Thus, it should follow that the number of messages produced dictates the number of proteins made. However, this is not the case and the number of proteins produced cannot be completely predicted from knowing the number of messenger RNAs. Cells control how much of a given protein they produce through interactions between the messenger RNAs and other regulatory RNAs. The regulatory RNAs bind directly to a message and impede protein production. Because there are millions of RNAs in a cell, these interactions have evolved to be highly specific. Nevertheless, it seems inevitable that messenger RNAs would encounter other RNAs too, which could short-circuit gene regulation and lead to less protein being produced. Umu et al. have now asked if such short-circuit events are selected against during evolution. Computational tools were used to predict the strength of binding between the RNAs found in the dominant forms of microbial life on Earth: the bacteria and the archaea. This approach revealed that the majority of messenger RNAs bind more weakly to the most common RNA molecules found in cells than would be expected by chance. Weakened binding should prevent the RNA molecules from becoming tangled with each other and ensure that protein levels are not perturbed by unintended interactions between highly expressed messages and other RNAs. To test this hypothesis further, Umu et al. generated versions of the gene for a green fluorescent protein that differed only in how well their messenger RNAs could avoid interacting with the most abundant RNAs in E. coli cells. Those messengers that were designed to avoid interacting with other RNAs yielded far more protein than those that were not. The findings show that taking this kind of avoidance into account can improve predictions about how much protein will be produced and should therefore make it easier to control protein production in experimental systems. Finally, the messenger RNAs of some bacteria do not show such clear avoidance. However, these bacteria have a more complex internal cell structure. This finding hints at an alternative means for avoiding short-circuiting events that could be used by more complicated cells, such of those of animals and plants, which also contain much larger numbers of RNAs.},
	number = {September},
	journal = {eLife},
	author = {Corley, M and Laederach, A},
	year = {2016},
	pmid = {27642845},
	note = {ISBN: 1044071060},
	pages = {3--5},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\RXUM4PRC\\Corley2016.pdf:application/pdf;Umu-etal16.pdf:C\:\\Users\\jstacey\\Zotero\\storage\\WWM5LHG6\\Umu-etal16.pdf:application/pdf},
}

@article{Gillings2016,
	title = {Information in the {Biosphere}: {Biological} and {Digital} {Worlds}},
	volume = {31},
	issn = {01695347},
	url = {http://dx.doi.org/10.1016/j.tree.2015.12.013},
	doi = {10.1016/j.tree.2015.12.013},
	abstract = {Evolution has transformed life through key innovations in information storage and replication, including RNA, DNA, multicellularity, and culture and language. We argue that the carbon-based biosphere has generated a cognitive system (humans) capable of creating technology that will result in a comparable evolutionary transition. Digital information has reached a similar magnitude to information in the biosphere. It increases exponentially, exhibits high-fidelity replication, evolves through differential fitness, is expressed through artificial intelligence (AI), and has facility for virtually limitless recombination. Like previous evolutionary transitions, the potential symbiosis between biological and digital information will reach a critical point where these codes could compete via natural selection. Alternatively, this fusion could create a higher-level superorganism employing a low-conflict division of labor in performing informational tasks. Digital information is accumulating at an exponential rate and could exceed the quantity of DNA-based information. There are biological and social implications arising from our growing fusion with the digital world.The parallels between evolution in the biological and digital worlds need to be explored.},
	number = {3},
	journal = {Trends in Ecology and Evolution},
	author = {Gillings, Michael R. and Hilbert, Martin and Kemp, Darrell J.},
	year = {2016},
	pmid = {26777788},
	note = {Publisher: Elsevier Ltd
ISBN: 1872-8383 (Electronic){\textbackslash}r0169-5347 (Linking)},
	keywords = {Artificial intelligence, Big data, Digital, Evolutionary transition, Information, Moore's law, Replicator, Singularity, Synthetic biology},
	pages = {180--189},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\RU24MCDT\\Gillings-etal16.pdf:application/pdf},
}

@article{Lehman2018,
	title = {The {Surprising} {Creativity} of {Digital} {Evolution}: {A} {Collection} of {Anecdotes} from the {Evolutionary} {Computation} and {Artificial} {Life} {Research} {Communities}},
	url = {http://arxiv.org/abs/1803.03453},
	abstract = {Biological evolution provides a creative fount of complex and subtle adaptations, often surprising the scientists who discover them. However, because evolution is an algorithmic process that transcends the substrate in which it occurs, evolution's creativity is not limited to nature. Indeed, many researchers in the field of digital evolution have observed their evolving algorithms and organisms subverting their intentions, exposing unrecognized bugs in their code, producing unexpected adaptations, or exhibiting outcomes uncannily convergent with ones in nature. Such stories routinely reveal creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.},
	author = {Lehman, Joel and Clune, Jeff and Misevic, Dusan and Adami, Christoph and Altenberg, Lee and Beaulieu, Julie and Bentley, Peter J. and Bernard, Samuel and Beslon, Guillaume and Bryson, David M. and Chrabaszcz, Patryk and Cheney, Nick and Cully, Antoine and Doncieux, Stephane and Dyer, Fred C. and Ellefsen, Kai Olav and Feldt, Robert and Fischer, Stephan and Forrest, Stephanie and Frénoy, Antoine and Gagné, Christian and Goff, Leni Le and Grabowski, Laura M. and Hodjat, Babak and Hutter, Frank and Keller, Laurent and Knibbe, Carole and Krcah, Peter and Lenski, Richard E. and Lipson, Hod and MacCurdy, Robert and Maestre, Carlos and Miikkulainen, Risto and Mitri, Sara and Moriarty, David E. and Mouret, Jean-Baptiste and Nguyen, Anh and Ofria, Charles and Parizeau, Marc and Parsons, David and Pennock, Robert T. and Punch, William F. and Ray, Thomas S. and Schoenauer, Marc and Shulte, Eric and Sims, Karl and Stanley, Kenneth O. and Taddei, François and Tarapore, Danesh and Thibault, Simon and Weimer, Westley and Watson, Richard and Yosinski, Jason},
	year = {2018},
	note = {arXiv: 1803.03453},
	pages = {1--31},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\97F3XBGI\\Lehman_etal18.pdf:application/pdf},
}

@article{Shipman2017,
	title = {{CRISPR}-{Cas} encoding of a digital movie into the genomes of a population of living bacteria},
	volume = {547},
	issn = {14764687},
	url = {http://dx.doi.org/10.1038/nature23017},
	doi = {10.1038/nature23017},
	abstract = {DNA is an excellent medium for data archival. Recent efforts have illustrated the potential for information storage in DNA using synthesized oligonucleotides assembled in vitro 1-6. A relatively unexplored avenue of information storage in DNA is the ability to write information into the genome of a living cell by the addition of nucleotides over time. Using the Cas1-Cas2 integrase, the CRISPR-Cas microbial immune system stores the nucleotide content of invading viruses to confer adaptive immunity 7. Harnessed, this system has the potential to write arbitrary information into the genome 8. Here, we use the CRISPR-Cas system to encode images and a short movie into the genomes of a population of living bacteria. In doing so, we push the technical limits of this information storage system and optimize strategies to minimize those limitations. We additionally uncover underlying principles of the CRISPR-Cas adaptation system, including sequence determinants of spacer acquisition relevant for understanding both the basic biology of bacterial adaptation as well as its technological applications. This work demonstrates that this system can capture and stably store practical amounts of real data within the genomes of populations of living cells. By combining the principles of information storage in DNA with DNA capture systems capable of functioning in living cells, one can create living organisms that capture, store, and},
	number = {7663},
	journal = {Nature},
	author = {Shipman, Seth L. and Nivala, Jeff and Macklis, Jeffrey D. and Church, George M.},
	year = {2017},
	pmid = {28700573},
	note = {Publisher: Nature Publishing Group
ISBN: 1476-4687 (Electronic) 0028-0836 (Linking)},
	pages = {345--349},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\SVGQ8GVW\\Shipman2018.pdf:application/pdf},
}

@article{Shapiro2013,
	title = {How life changes itself: {The} {Read}-{Write} ({RW}) genome},
	volume = {10},
	issn = {15710645},
	url = {http://dx.doi.org/10.1016/j.plrev.2013.07.001},
	doi = {10.1016/j.plrev.2013.07.001},
	abstract = {The genome has traditionally been treated as a Read-Only Memory (ROM) subject to change by copying errors and accidents. In this review, I propose that we need to change that perspective and understand the genome as an intricately formatted Read-Write (RW) data storage system constantly subject to cellular modifications and inscriptions. Cells operate under changing conditions and are continually modifying themselves by genome inscriptions. These inscriptions occur over three distinct time-scales (cell reproduction, multicellular development and evolutionary change) and involve a variety of different processes at each time scale (forming nucleoprotein complexes, epigenetic formatting and changes in DNA sequence structure). Research dating back to the 1930s has shown that genetic change is the result of cell-mediated processes, not simply accidents or damage to the DNA. This cell-active view of genome change applies to all scales of DNA sequence variation, from point mutations to large-scale genome rearrangements and whole genome duplications (WGDs). This conceptual change to active cell inscriptions controlling RW genome functions has profound implications for all areas of the life sciences. © 2013 Elsevier B.V.},
	number = {3},
	journal = {Physics of Life Reviews},
	author = {Shapiro, James A.},
	year = {2013},
	pmid = {23876611},
	note = {Publisher: Elsevier B.V.
ISBN: 1571-0645},
	keywords = {Epigenetics, Genome inscriptions, Mobile genetic elements (MGEs), Natural genetic engineering (NGE)},
	pages = {287--323},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\6PIRGG2B\\Shapiro 2013.pdf:application/pdf},
}

@article{Lee2005,
	title = {Shannon information in complete genomes},
	volume = {3},
	number = {3},
	author = {Lee, Hoong-chien},
	year = {2005},
	note = {ISBN: 0769521940},
	keywords = {genomics, molecular evolution, shannon information, statistical analysis},
	pages = {587--608},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\SNA9DX7I\\Shannon et al 2005.pdf:application/pdf},
}

@article{Rubio2013,
	title = {Time-dependent changes in {DNA} stability in decomposing teeth over 18 months},
	volume = {71},
	issn = {00016357},
	doi = {10.3109/00016357.2012.700068},
	abstract = {OBJECTIVE: The objective was to use a dual quantitative and qualitative approach to analyze the dental DNA degradation produced by the passage of time since tooth death under controlled environmental conditions.{\textbackslash}n{\textbackslash}nMATERIALS AND METHODS: Sixty human teeth were stored at room temperature for 0, 1, 3, 6, 12 or 18 months post-extraction. DNA quantification was determined by real-time quantitative PCR using a Quantifiler(TM) kit. DNA quality was assessed by the allelic dropout ratio between the smallest and largest loci obtained after STR genotyping and using an AmpFlSTR® Identifiler™ PCR kit. We also evaluated differences of DNA concentration related to gender and tooth position.{\textbackslash}n{\textbackslash}nRESULTS: DNA concentration significantly reduced in 1 month post-extraction, stabilized between 1-12 months post-extraction, but decreased again at 18 months post-extraction. Interestingly, a significant reduction of the allelic dropout ratio (DNA quality) was only detected at 18 months post-extraction.{\textbackslash}n{\textbackslash}nCONCLUSIONS: Stability of dental DNA decreased over time, differently affecting the amount and quality of the DNA in a time-dependent process over the first 18 months post-extraction. These results have a potential use in post-mortem intervals in human teeth in controlled environmental conditions.},
	number = {3-4},
	journal = {Acta Odontologica Scandinavica},
	author = {Rubio, Leticia and Santos, Ignacio and Gaitan, Maria Jesus and Martin De- Las Heras, Stella},
	year = {2013},
	pmid = {22783923},
	note = {ISBN: 10.3109/00016357.2012.700068},
	keywords = {DNA degradation, Forensic dentistry, Post-extraction interval, Time since death, Tooth},
	pages = {638--643},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ZNM52IF7\\Rubio et al 2013.pdf:application/pdf},
}

@article{Yachie2007,
	title = {Alignment-based approach for durable data storage into living organisms},
	volume = {23},
	issn = {87567938},
	doi = {10.1021/bp060261y},
	abstract = {The practical realization of DNA data storage is a major scientific goal. Here we introduce a simple, flexible, and robust data storage and retrieval method based on sequence alignment of the genomic DNA of living organisms. Duplicated data encoded by different oligonucleotide sequences was inserted redundantly into multiple loci of the Bacillus subtilis genome. Multiple alignment of the bit data sequences decoded by B. subtilis genome sequences enabled the retrieval of stable and compact data without the need for template DNA, parity checks, or error-correcting algorithms. Combined with the computational simulation of data retrieval from mutated message DNA, a practical use of this alignment-based method is discussed.},
	number = {2},
	journal = {Biotechnology Progress},
	author = {Yachie, Nozomu and Sekiyama, Kazuhide and Sugahara, Junichi and Ohashi, Yoshiaki and Tomita, Masaru},
	year = {2007},
	pmid = {17253725},
	note = {ISBN: 8756-7938 (Print){\textbackslash}n1520-6033 (Linking)},
	pages = {501--505},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\6GIJBLVW\\Yachie_et_al-2007-Biotechnology_Progress.pdf:application/pdf},
}

@article{Bishop2017,
	title = {Technology {Working} {Group} {Meeting} on future {DNA} synthesis technologies},
	author = {Bishop, Bryan and Mccorkle, Nathan and Zhirnov, Victor},
	year = {2017},
	pages = {1--39},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\XDHJWVMQ\\m-api-079e3ca7-4649-accd-5275-e93dbbaa4735.pdf:application/pdf},
}

@article{Tabor2003,
	title = {Playing to win at {DNA} computation},
	volume = {21},
	issn = {10870156},
	doi = {10.1038/nbt0903-1013},
	abstract = {An automaton built with DNA enzymes plays tic-tac-toe against human players.},
	number = {9},
	journal = {Nature Biotechnology},
	author = {Tabor, Jeffrey J. and Ellington, Andrew D.},
	year = {2003},
	pmid = {12949563},
	note = {ISBN: 1087-0156},
	pages = {1013--1015},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\DREE3UJY\\Tabor article 2013.pdf:application/pdf},
}

@inproceedings{Markowitz2016,
	address = {Arlington, VA},
	title = {Workshop {Results}},
	booktitle = {{SRC}/{IARPA} {Workshop} on {DNA}-based {Massive} {Information} {Storage}},
	publisher = {The Intelligence Advanced Research Projects Activity and Semiconductor Research Corporation},
	author = {Markowitz, David},
	year = {2016},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\NUPQCXGI\\SRC_IARPA Workshop 2016.pdf:application/pdf},
}

@article{Organick2017,
	title = {Scaling up {DNA} data storage and random access retrieval},
	url = {https://www.biorxiv.org/content/early/2017/03/07/114553},
	doi = {10.1101/114553},
	abstract = {Current storage technologies can no longer keep pace with exponentially growing amounts of data. Synthetic DNA offers an attractive alternative due to its potential information density of {\textasciitilde} 1018B/mm3, 107 times denser than magnetic tape, and potential durability of thousands of years. Recent advances in DNA data storage have highlighted technical challenges, in particular, coding and random access, but have stored only modest amounts of data in synthetic DNA. This paper demonstrates an end-to-end approach toward the viability of DNA data storage with large-scale random access. We encoded and stored 35 distinct files, totaling 200MB of data, in more than 13 million DNA oligonucleotides (about 2 billion nucleotides in total) and fully recovered the data with no bit errors, representing an advance of almost an order of magnitude compared to prior work. Our data curation focused on technologically advanced data types and historical relevance, including the Universal Declaration of Human Rights in over 100 languages, a high-definition music video of the band OK Go, and a CropTrust database of the seeds stored in the Svalbard Global Seed Vault. We developed a random access methodology based on selective amplification, for which we designed and validated a large library of primers, and successfully retrieved arbitrarily chosen items from a subset of our pool containing 10.3 million DNA sequences. Moreover, we developed a novel coding scheme that dramatically reduces the physical redundancy (sequencing read coverage) required for error-free decoding to a median of 5x, while maintaining levels of logical redundancy comparable to the best prior codes. We further stress-tested our coding approach by successfully decoding a file using the more error-prone nanopore-based sequencing. We provide a detailed analysis of errors in the process of writing, storing, and reading data from synthetic DNA at a large scale, which helps characterize DNA as a storage medium and justify our coding approach. Thus, we have demonstrated a significant improvement in data volume, random access, and encoding/decoding schemes that contribute to a whole-system vision for DNA data storage.},
	journal = {bioRxiv},
	author = {Organick, Lee and Ang, Siena Dumas and Chen, Yuan-Jyue and Lopez, Randolph and Yekhanin, Sergey and Makarychev, Konstantin and Racz, Miklos Z. and Kamath, Govinda and Gopalan, Parikshit and Nguyen, Bichlien and Takahashi, Christopher and Newman, Sharon and Parker, Hsing-Yeh and Rashtchian, Cyrus and Stewart, Kendall and Gupta, Gagan and Carlson, Robert and Mulligan, John and Carmean, Douglas and Seelig, Georg and Ceze, Luis and Strauss, Karin},
	year = {2017},
	note = {arXiv: 114553
ISBN: 9781510856738},
	pages = {114553},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\AGLIXR5Z\\Organick et al 2017.pdf:application/pdf},
}

@article{Olalde2014,
	title = {Derived immune and ancestral pigmentation alleles in a 7,000-year-old {Mesolithic} {European}},
	volume = {507},
	issn = {14764687},
	doi = {10.1038/nature12960},
	abstract = {Ancient genomic sequences have started to reveal the origin and the demographic impact of farmers from the Neolithic period spreading into Europe. The adoption of farming, stock breeding and sedentary societies during the Neolithic may have resulted in adaptive changes in genes associated with immunity and diet. However, the limited data available from earlier hunter-gatherers preclude an understanding of the selective processes associated with this crucial transition to agriculture in recent human evolution. Here we sequence an approximately 7,000-year-old Mesolithic skeleton discovered at the La Braña-Arintero site in León, Spain, to retrieve a complete pre-agricultural European human genome. Analysis of this genome in the context of other ancient samples suggests the existence of a common ancient genomic signature across western and central Eurasia from the Upper Paleolithic to the Mesolithic. The La Braña individual carries ancestral alleles in several skin pigmentation genes, suggesting that the light skin of modern Europeans was not yet ubiquitous in Mesolithic times. Moreover, we provide evidence that a significant number of derived, putatively adaptive variants associated with pathogen resistance in modern Europeans were already present in this hunter-gatherer.},
	number = {7491},
	journal = {Nature},
	author = {Olalde, Iñigo and Allentoft, Morten E. and Sánchez-Quinto, Federico and Santpere, Gabriel and Chiang, Charleston W K and DeGiorgio, Michael and Prado-Martinez, Javier and Rodríguez, Juan Antonio and Rasmussen, Simon and Quilez, Javier and Ramírez, Oscar and Marigorta, Urko M. and Fernández-Callejo, Marcos and Prada, María Encina and Encinas, Julio Manuel Vidal and Nielsen, Rasmus and Netea, Mihai G. and Novembre, John and Sturm, Richard A. and Sabeti, Pardis and Marquès-Bonet, Tomàs and Navarro, Arcadi and Willerslev, Eske and Lalueza-Fox, Carles},
	year = {2014},
	pmid = {24463515},
	note = {arXiv: NIHMS150003
ISBN: 1476-4687 (Electronic){\textbackslash}r0028-0836 (Linking)},
	pages = {225--228},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\NA5ATF6H\\Olalde et al 2014.pdf:application/pdf},
}

@article{Porter2013,
	title = {Success in {Introductory} {Programming}: {What} {Works}?},
	volume = {56},
	issn = {00010782},
	url = {10.1145/2492007.2492017%5Cnhttps://proxy.tamuc.edu:2048/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=89594060&site=eds-live},
	doi = {10.1145/2492007},
	abstract = {The article discusses education business models, focusing on massive open online courses (MOOCs) as of August 2013 and arguing that MOOCs can become profitable. Potential sources of revenue such as state education subsidies, student tuition, employment recruiting services, syndication, and sponsors are discussed. Companies such as the recruitment company Udacity are mentioned.},
	number = {8},
	journal = {Communications of the ACM},
	author = {Porter, Leo and Guzdial, Mark and McDowell, Charlie and Simon, Beth},
	year = {2013},
	pmid = {89594063},
	note = {ISBN: 0001-0782},
	keywords = {BUSINESS models, BUSINESS revenue, EMPLOYEE recruitment, MASSIVE open online courses, TUITION},
	pages = {25--28},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\JVZK7IAL\\New approach to info storage 2013.pdf:application/pdf},
}

@article{Minsky2004,
	title = {Information {Content} and {Complexity} in the {High}-{Order} {Organization} of {DNA}},
	volume = {33},
	issn = {1056-8700},
	url = {http://www.annualreviews.org/doi/10.1146/annurev.biophys.33.110502.133328},
	doi = {10.1146/annurev.biophys.33.110502.133328},
	abstract = {Nucleic acids are characterized by a vast structural variability. Secondary structural conformations include the main polymorphs A, B, and Z, cruciforms, intrinsic curvature, and multistranded motifs. DNA secondary motifs are stabilized and regulated by the primary base sequence, contextual effects, environmental factors, as well as by high-order DNA packaging modes. The high-order modes are, in turn, affected by secondary structures and by the environment. This review is concerned with the flow of structural information among the hierarchical structural levels of DNA molecules, the intricate interplay between the various factors that affect these levels, and the regulation and physiological significance of DNA high-order structures.},
	number = {1},
	journal = {Annual Review of Biophysics and Biomolecular Structure},
	author = {Minsky, Abraham},
	year = {2004},
	pmid = {15139816},
	note = {ISBN: \%(},
	keywords = {and z, are characterized by a, b, cruci-, dna condensation, dna microheterogeneity, dna repair, electrostatic collapse, ion correlation, restricted diffusion, s abstract nucleic acids, secondary structural conformations include, the main polymorphs a, vast structural variability},
	pages = {317--342},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\GJBAHKXR\\Minsky2004.pdf:application/pdf},
}

@inproceedings{Rashtchian2017,
	title = {Clustering {Billions} of {Reads} for {DNA} {Data} {Storage}},
	abstract = {Storing data in synthetic DNA offers the possibility of improving information density and durability by several orders of magnitude compared to current storage technologies. However, DNA data storage requires a computationally intensive process to retrieve the data. In particular, a crucial step in the data retrieval pipeline involves clustering billions of strings with respect to edit distance. Datasets in this domain have many notable properties, such as containing a very large number of small clusters that are well-separated in the edit distance metric space. In this regime, existing algorithms are unsuitable because of either their long running time or low accuracy. To address this issue, we present a novel distributed algorithm for approximately computing the underlying clusters. Our algorithm converges efficiently on any dataset that satisfies certain separability properties, such as those coming from DNA data storage systems. We also prove that, under these assumptions, our algorithm is robust to outliers and high levels of noise. We provide empirical justification of the accuracy, scalability, and convergence of our algorithm on real and synthetic data. Compared to the state-of-the-art algorithm for clustering DNA sequences, our algorithm simultaneously achieves higher accuracy and a 1000x speedup on three real datasets.},
	booktitle = {Conf. {Neural} {Information} {Processing} {Systems}},
	author = {Rashtchian, Cyrus and Makarychev, Konstantin and Racz, Miklos Z. and Jevdjic, Djordje and Yekhanin, Sergey and Ang, Siena Dumas and Strauss, Karin and Ceze, Luis},
	year = {2017},
	note = {Issue: Nips
ISSN: 10495258},
	pages = {1--12},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\4DZSJ2S2\\Rashtchian et al.pdf:application/pdf},
}

@article{Panda2018,
	title = {{DNA} as a digital information storage device: hope or hype?},
	volume = {8},
	issn = {21905738},
	url = {https://doi.org/10.1007/s13205-018-1246-7},
	doi = {10.1007/s13205-018-1246-7},
	number = {5},
	journal = {3 Biotech},
	author = {Panda, Darshan and Molla, Kutubuddin Ali and Baig, Mirza Jainul and Swain, Alaka and Behera, Deeptirekha and Dash, Manaswini},
	year = {2018},
	note = {Publisher: Springer Berlin Heidelberg
ISBN: 0123456789},
	keywords = {Data crunch, Data longevity, Digital data, DNA hard drive, DNA steganography, DNA storage, Silicone pollution},
	pages = {1--9},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\J8NBWI88\\Panda2017.pdf:application/pdf},
}

@article{Orlando2013,
	title = {Recalibrating equus evolution using the genome sequence of an early {Middle} {Pleistocene} horse},
	volume = {499},
	issn = {00280836},
	doi = {10.1038/nature12323},
	abstract = {The rich fossil record of equids has made them a model for evolutionary processes. Here we present a 1.12-times coverage draft genome from a horse bone recovered from permafrost dated to approximately 560-780 thousand years before present (kyr BP). Our data represent the oldest full genome sequence determined so far by almost an order of magnitude. For comparison, we sequenced the genome of a Late Pleistocene horse (43 kyr BP), and modern genomes of five domestic horse breeds (Equus ferus caballus), a Przewalski's horse (E. f. przewalskii) and a donkey (E. asinus). Our analyses suggest that the Equus lineage giving rise to all contemporary horses, zebras and donkeys originated 4.0-4.5 million years before present (Myr BP), twice the conventionally accepted time to the most recent common ancestor of the genus Equus. We also find that horse population size fluctuated multiple times over the past 2 Myr, particularly during periods of severe climatic changes. We estimate that the Przewalski's and domestic horse populations diverged 38-72 kyr BP, and find no evidence of recent admixture between the domestic horse breeds and the Przewalski's horse investigated. This supports the contention that Przewalski's horses represent the last surviving wild horse population. We find similar levels of genetic variation among Przewalski's and domestic populations, indicating that the former are genetically viable and worthy of conservation efforts. We also find evidence for continuous selection on the immune system and olfaction throughout horse evolution. Finally, we identify 29 genomic regions among horse breeds that deviate from neutrality and show low levels of genetic variation compared to the Przewalski's horse. Such regions could correspond to loci selected early during domestication.},
	number = {7456},
	journal = {Nature},
	author = {Orlando, Ludovic and Ginolhac, Aurélien and Zhang, Guojie and Froese, Duane and Albrechtsen, Anders and Stiller, Mathias and Schubert, Mikkel and Cappellini, Enrico and Petersen, Bent and Moltke, Ida and Johnson, Philip L.F. and Fumagalli, Matteo and Vilstrup, Julia T. and Raghavan, Maanasa and Korneliussen, Thorfinn S. and Malaspinas, Anna Sapfo and Vogt, Josef and Szklarczyk, Damian and Kelstrup, Christian D. and Vinther, Jakob and Dolocan, Andrei and Stenderup, Jesper and Velazquez, Amhed M.V. and Cahill, James and Rasmussen, Morten and Wang, Xiaoli and Min, Jiumeng and Zazula, Grant D. and Seguin-Orlando, Andaine and Mortensen, Cecilie and Magnussen, Kim and Thompson, John F. and Weinstock, Jacobo and Gregersen, Kristian and Røed, Knut H. and Eisenmann, Véra and Rubin, Carl J. and Miller, Donald C. and Antczak, Douglas F. and Bertelsen, Mads F. and Brunak, Søren and Al-Rasheid, Khaled A.S. and Ryder, Oliver and Andersson, Leif and Mundy, John and Krogh, Anders and Gilbert, M. Thomas P. and Kjær, Kurt and Sicheritz-Ponten, Thomas and Jensen, Lars Juhl and Olsen, Jesper V. and Hofreiter, Michael and Nielsen, Rasmus and Shapiro, Beth and Wang, Jun and Willerslev, Eske},
	year = {2013},
	pmid = {23803765},
	note = {ISBN: 1476-4687 (Electronic){\textbackslash}n0028-0836 (Linking)},
	pages = {74--78},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\98TLVXBD\\Orlando et al 2013 .pdf:application/pdf},
}

@article{Organick2018,
	title = {Random access in large-scale {DNA} data storage},
	volume = {36},
	issn = {1087-0156},
	url = {http://www.nature.com/doifinder/10.1038/nbt.4079},
	doi = {10.1038/nbt.4079},
	abstract = {200 MB of digital data is stored in DNA, randomly accessed and recovered using an error-free approach.},
	number = {3},
	journal = {Nature Biotechnology},
	author = {Organick, Lee and Ang, Siena Dumas and Chen, Yuan-Jyue and Lopez, Randolph and Yekhanin, Sergey and Makarychev, Konstantin and Racz, Miklos Z and Kamath, Govinda and Gopalan, Parikshit and Nguyen, Bichlien and Takahashi, Christopher N and Newman, Sharon and Parker, Hsing-Yeh and Rashtchian, Cyrus and Stewart, Kendall and Gupta, Gagan and Carlson, Robert and Mulligan, John and Carmean, Douglas and Seelig, Georg and Ceze, Luis and Strauss, Karin},
	year = {2018},
	pmid = {29457795},
	note = {ISBN: 1364298980840},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ITJVNKXX\\Organick-etal2018.pdf:application/pdf},
}

@article{Kalff2016,
	title = {A kilobyte rewritable atomic memory},
	volume = {11},
	issn = {17483395},
	url = {http://dx.doi.org/10.1038/nnano.2016.131},
	doi = {10.1038/nnano.2016.131},
	abstract = {The advent of devices based on single dopants, such as the single atom transistor, the single spin magnetometer and the single atom memory, motivates the quest for strategies that permit to control matter with atomic precision. Manipulation of individual atoms by means of low-temperature scanning tunnelling microscopy provides ways to store data in atoms, encoded either into their charge state, magnetization state or lattice position. A defining challenge at this stage is the controlled integration of these individual functional atoms into extended, scalable atomic circuits. Here we present a robust digital atomic scale memory of up to 1 kilobyte (8,000 bits) using an array of individual surface vacancies in a chlorine terminated Cu(100) surface. The memory can be read and rewritten automatically by means of atomic scale markers, and offers an areal density of 502 Terabits per square inch, outperforming state-of-the-art hard disk drives by three orders of magnitude. Furthermore, the chlorine vacancies are found to be stable at temperatures up to 77 K, offering prospects for expanding large-scale atomic assembly towards ambient conditions.},
	number = {11},
	journal = {Nature Nanotechnology},
	author = {Kalff, F. E. and Rebergen, M. P. and Fahrenfort, E. and Girovsky, J. and Toskovic, R. and Lado, J. L. and Fernández-Rossier, J. and Otte, A. F.},
	year = {2016},
	pmid = {27428273},
	note = {arXiv: 1604.02265
Publisher: Nature Publishing Group
ISBN: 1748-3395},
	pages = {926--929},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\8M6E5ZEI\\Kalff et al 2016.pdf:application/pdf},
}

@article{Kaiser2008,
	title = {Molecular study of time dependent changes in {DNA} stability in soil buried skeletal residues},
	volume = {177},
	issn = {03790738},
	doi = {10.1016/j.forsciint.2007.10.005},
	abstract = {In the past years, many publications about identification and sex-determination of dry human bones by means of DNA analysis have been published. However, few studies exist that investigate the potential use of DNA technique to determine the postmortem interval (PMI). In the present study we analyzed the rate of increasingly smaller fragments of chromosomal DNA and PMI. We examined DNA degradation in human bones with postmortem intervals ranging between 1 and more than 200 years that had been kept under comparable conditions concerning weather and soil. Following bone separation into the three different zones of interest of inner/middle/outer segments the quantity of total DNA was determined in each region. Subsequently, the degree of DNA fragmentation was estimated by searching for PCR products of defined size (150, 507 and 763 bp) with primers of the human-specific multicopy β-actin-gene. Concerning DNA quantity we detected a significant correlation between the zone of interest and the amount of DNA. However, there was no correlation between the amount of DNA and PMI. In contrast to this, analyzing DNA using PCR showed a significant inverse correlation between fragment length and PMI. Thus, postmortem DNA degradation into increasingly smaller fragments reveals a time-dependent process. It has the potential to be used as a predictor of PMI in human bone findings, provided that environmental conditions are known. © 2007 Elsevier Ireland Ltd. All rights reserved.},
	number = {1},
	journal = {Forensic Science International},
	author = {Kaiser, Christina and Bachmeier, Beatrice and Conrad, Claudius and Nerlich, Andreas and Bratzke, Hansjürgen and Eisenmenger, Wolfgang and Peschel, Oliver},
	year = {2008},
	pmid = {18063334},
	note = {ISBN: 1872-6283 (Electronic){\textbackslash}n0379-0738 (Linking)},
	keywords = {DNA degradation, Time since death, Bone, Forensic, Postmortem interval (PMI)},
	pages = {32--36},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\RA66HF63\\Kaiser et al 2007.pdf:application/pdf},
}

@article{Jerome2002,
	title = {Quantitative {Stability} of {DNA} after {Extended} {Storage} of {Clinical} {Specimens} as {Determined} by {Real}-{Time} {PCR} {Quantitative} {Stability} of {DNA} after {Extended} {Storage} of {Clinical} {Specimens} as {Determined} by {Real}-{Time} {PCR}},
	volume = {40},
	issn = {0095-1137},
	doi = {10.1128/JCM.40.7.2609},
	number = {7},
	journal = {Journal of clinical microbiology},
	author = {Jerome, Keith R and Huang, Meei-li and Wald, Anna and Selke, Stacy and Corey, Lawrence},
	year = {2002},
	pages = {2609--2611},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\HUYME6T2\\Jerome et al 2001.pdf:application/pdf},
}

@article{Houlihan2017,
	title = {Exploring the {Chemistry} of {Genetic} {Information} {Storage} and {Propagation} through {Polymerase} {Engineering}},
	volume = {50},
	issn = {15204898},
	doi = {10.1021/acs.accounts.7b00056},
	abstract = {Nucleic acids are a distinct form of sequence-defined biopolymer. What sets them apart from other biopolymers such as polypeptides or polysaccharides is their unique capacity to encode, store, and propagate genetic information (molecular heredity). In nature, just two closely related nucleic acids, DNA and RNA, function as repositories and carriers of genetic information. They therefore are the molecular embodiment of biological information. This naturally leads to questions regarding the degree of variation from this seemingly ideal "Goldilocks" chemistry that would still be compatible with the fundamental property of molecular heredity. To address this question, chemists have created a panoply of synthetic nucleic acids comprising unnatural sugar ring congeners, backbone linkages, and nucleobases in order to establish the molecular parameters for encoding genetic information and its emergence at the origin of life. A deeper analysis of the potential of these synthetic genetic polymers for molecular heredity requires a means of replication and a determination of the fidelity of information transfer. While non-enzymatic synthesis is an increasingly powerful method, it currently remains restricted to short polymers. Here we discuss efforts toward establishing enzymatic synthesis, replication, and evolution of synthetic genetic polymers through the engineering of polymerase enzymes found in nature. To endow natural polymerases with the ability to efficiently utilize non-cognate nucleotide substrates, novel strategies for the screening and directed evolution of polymerase function have been realized. High throughput plate-based screens, phage display, and water-in-oil emulsion technology based methods have yielded a number of engineered polymerases, some of which can synthesize and reverse transcribe synthetic genetic polymers with good efficiency and fidelity. The inception of such polymerases demonstrates that, at a basic level at least, molecular heredity is not restricted to the natural nucleic acids DNA and RNA, but may be found in a large (if finite) number of synthetic genetic polymers. And it has opened up these novel sequence spaces for investigation. Although largely unexplored, first tentative forays have yielded ligands (aptamers) against a range of targets and several catalysts elaborated in a range of different chemistries. Finally, taking the lead from established DNA designs, simple polyhedron nanostructures have been described. We anticipate that further progress in this area will expand the range of synthetic genetic polymers that can be synthesized, replicated, and evolved providing access to a rich sequence, structure, and phenotypic space. "Synthetic genetics", that is, the exploration of these spaces, will illuminate the chemical parameter range for en- and decoding information, 3D folding, and catalysis and yield novel ligands, catalysts, and nanostructures and devices for applications in biotechnology and medicine.},
	number = {4},
	journal = {Accounts of Chemical Research},
	author = {Houlihan, Gillian and Arangundy-Franklin, Sebastian and Holliger, Philipp},
	year = {2017},
	pmid = {28383245},
	note = {ISBN: 1520-4898 (Electronic) 0001-4842 (Linking)},
	pages = {1079--1087},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\Z8HFGJQ2\\Hoolihan et al 2017.pdf:application/pdf},
}

@inproceedings{Mansuripur2004,
	address = {Monterey, California},
	title = {Macro-molecular data storage with petabyte/cm(3) density, highly parallel read/write operations, and genuine {3D} storage capability},
	isbn = {0277-786X},
	doi = {Doi 10.1117/12.562434},
	abstract = {Digital information can be encoded in the building-block sequence of macromolecules, such as RNA and single-stranded DNA. Methods of "writing" and "reading" macromolecular strands are currently available, but they are slow and expensive. In an ideal molecular data storage system, routine operations such as write, read, erase, store, and transfer must be done reliably and at high speed within an integrated chip. As a first step toward demonstrating the feasibility of this concept, we report preliminary results of DNA readout experiments conducted in miniaturized chambers that are scalable to even smaller dimensions. We show that translocation of a single-stranded DNA molecule (consisting of 50 adenosine bases followed by 100 cytosine bases) through an ion-channel yields a characteristic signal that is attributable to the 2-segment structure of the molecule. We also examine the dependence of the rate and speed of molecular translocation on the adjustable parameters of the experiment.},
	booktitle = {The {International} {Society} of {Optics} and {Photonics} - {Optical} {Data} {Storage} {Topical} {Meeting}},
	author = {Mansuripur, M and Khulbe, P},
	year = {2004},
	note = {ISSN: 0277786X},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\TI4FBB87\\Mansuripur 2004.pdf:application/pdf},
}

@article{Lapique2018,
	title = {Genetic programs can be compressed and autonomously decompressed in live cells},
	volume = {13},
	issn = {17483395},
	url = {http://dx.doi.org/10.1038/s41565-017-0004-z},
	doi = {10.1038/s41565-017-0004-z},
	abstract = {14–21 . Recent research has shown that digital information storage in DNA, imple-mented using deep sequencing and conventional software, can approach the maximum Shannon information capacity 22 of two bits per nucleotide 23 . In nature, DNA is used to store genetic programs, but the information content of the encod-ing rarely approaches this maximum 24 . We hypothesize that the biological function of a genetic program can be preserved while reducing the length of its DNA encoding and increasing the information content per nucleotide. Here we support this hypothesis by describing an experimental procedure for com-pressing a genetic program and its subsequent autonomous decompression and execution in human cells. As a test-bed we choose an RNAi cell classifier circuit 25 that comprises redun-dant DNA sequences and is therefore amenable for compres-sion, as are many other complex gene circuits 15,18,26–28 . In one example, we implement a compressed encoding of a ten-gene four-input AND gate circuit using only four genetic constructs. The compression principles applied to gene circuits can enable fitting complex genetic programs into DNA delivery vehicles with limited cargo capacity, and storing compressed and bio-logically inert programs in vivo for on-demand activation. The RNAi classifier 25},
	number = {4},
	journal = {Nature Nanotechnology},
	author = {Lapique, Nicolas and Benenson, Yaakov},
	year = {2018},
	pmid = {26928661},
	note = {arXiv: 15334406
Publisher: Springer US
ISBN: 0000000000000},
	pages = {309--315},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ZMAMHS6V\\Lapique2018.pdf:application/pdf},
}

@article{Kosuri2014,
	title = {Large-scale de novo {DNA} synthesis: {Technologies} and applications},
	volume = {11},
	issn = {15487105},
	doi = {10.1038/nmeth.2918},
	abstract = {For over 60 years, the synthetic production of new DNA sequences has helped researchers understand and engineer biology. Here we summarize methods and caveats for the de novo synthesis of DNA, with particular emphasis on recent technologies that allow for large-scale and low-cost production. In addition, we discuss emerging applications enabled by large-scale de novo DNA constructs, as well as the challenges and opportunities that lie ahead.},
	number = {5},
	journal = {Nature Methods},
	author = {Kosuri, Sriram and Church, George M.},
	year = {2014},
	pmid = {24781323},
	note = {ISBN: doi:10.1038/nmeth.2918},
	pages = {499--507},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\R56NHVWQ\\Kosuri and Church 2014.pdf:application/pdf},
}

@article{Kashiwamura2005,
	title = {Potential for enlarging {DNA} memory: {The} validity of experimental operations of scaled-up nested primer molecular memory},
	volume = {80},
	issn = {03032647},
	doi = {10.1016/j.biosystems.2004.10.007},
	abstract = {DNA is an attractive memory unit because of its immense information density. Here, we describe a memory model made of DNA, called Nested Primer Molecular Memory (NPMM). NPMM consists of many DNA strands, and each DNA strand consists of two areas: a data area and a data address area. When the address of target data is specified, only the target data can be extracted from NPMM. In this paper, we evaluate the validity of the basic operations of NPMM and then discuss the feasibility of scaled-up NPMM through some laboratory experiments. In the latter, we deal with scaled-up NPMM simulated by the Concentration Scaling method. © 2004 Elsevier Ireland Ltd. All rights reserved.},
	number = {1},
	journal = {BioSystems},
	author = {Kashiwamura, Satoshi and Yamamoto, Masahito and Kameda, Atsushi and Shiba, Toshikazu and Ohuchi, Azuma},
	year = {2005},
	pmid = {15740839},
	note = {ISBN: 0303-2647 (Print){\textbackslash}r0303-2647 (Linking)},
	keywords = {Concentration Scaling method, DNA computing, DNA memory, High accuracy, Nested PCR, NPMM},
	pages = {99--112},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\VY9AE9EI\\Kami et al 2005.pdf:application/pdf},
}

@article{Goldman2013,
	title = {Towards practical, high-capacity, low-maintenance information storage in synthesized {DNA}},
	volume = {494},
	issn = {00280836},
	url = {http://dx.doi.org/10.1038/nature11875},
	doi = {10.1038/nature11875},
	abstract = {Digital production, transmission and storage have revolutionized how we access and use information but have also made archiving an increasingly complex task that requires active, continuing maintenance of digital media. This challenge has focused some interest on DNA as an attractive target for information storage because of its capacity for high-density information encoding, longevity under easily achieved conditions and proven track record as an information bearer. Previous DNA-based information storage approaches have encoded only trivial amounts of information or were not amenable to scaling-up, and used no robust error-correction and lacked examination of their cost-efficiency for large-scale information archival. Here we describe a scalable method that can reliably store more information than has been handled before. We encoded computer files totalling 739 kilobytes of hard-disk storage and with an estimated Shannon information of 5.2 × 10(6) bits into a DNA code, synthesized this DNA, sequenced it and reconstructed the original files with 100\% accuracy. Theoretical analysis indicates that our DNA-based storage scheme could be scaled far beyond current global information volumes and offers a realistic technology for large-scale, long-term and infrequently accessed digital archiving. In fact, current trends in technological advances are reducing DNA synthesis costs at a pace that should make our scheme cost-effective for sub-50-year archiving within a decade.},
	number = {7435},
	journal = {Nature},
	author = {Goldman, Nick and Bertone, Paul and Chen, Siyuan and Dessimoz, Christophe and Leproust, Emily M. and Sipos, Botond and Birney, Ewan},
	year = {2013},
	pmid = {23354052},
	note = {arXiv: NIHMS150003
Publisher: Nature Publishing Group
ISBN: 0028-0836},
	pages = {77--80},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\XC249PU9\\Goldman et al 2013.pdf:application/pdf},
}

@article{Gibson2010,
	title = {Creation of a bacterial cell controlled by a chemically synthesized genome},
	volume = {329},
	issn = {00368075},
	doi = {10.1126/science.1190719},
	abstract = {We report the design, synthesis, and assembly of the 1.08–mega–base pair Mycoplasma mycoides JCVI-syn1.0 genome starting from digitized genome sequence information and its transplantation into a M. capricolum recipient cell to create new M. mycoides cells that are controlled only by the synthetic chromosome. The only DNA in the cells is the designed synthetic DNA sequence, including " watermark " sequences and other designed gene deletions and polymorphisms, and mutations acquired during the building process. The new cells have expected phenotypic properties and are capable of continuous self-replication. I n 1977, Sanger and colleagues determined the complete genetic sequence of phage ϕX174 (1), the first DNA genome to be completely sequenced. Eighteen years later, in 1995, our team was able to read the first complete genetic sequence of a self-replicating bacterium, Haemophilus influenzae (2). Reading the genetic sequence of a wide range of species has increased exponentially from these early studies. The ability to rapidly digitize genomic information has increased by more than eight orders of mag-nitude over the past 25 years (3). Efforts to un-derstand all this new genomic information have spawned numerous new computational and experimental paradigms, yet our genomic knowl-edge remains very limited. No single cellular system has all of its genes understood in terms of their biological roles. Even in simple bacterial cells, do the chromosomes contain the entire ge-netic repertoire? If so, can a complete genetic sys-tem be reproduced by chemical synthesis starting with only the digitized DNA sequence contained in a computer? Our interest in synthesis of large DNA mol-ecules and chromosomes grew out of our efforts over the past 15 years to build a minimal cell that contains only essential genes. This work was inaugurated in 1995 when we sequenced the genome of Mycoplasma genitalium, a bacterium with the smallest complement of genes of any known organism capable of independent growth in the laboratory. More than 100 of the 485 protein-coding genes of M. genitalium are dispensable when disrupted one at a time (4–6). We developed a strategy for assembling viral-sized pieces to produce large DNA molecules that enabled us to assemble a synthetic M. genitalium genome in four stages from chemically synthe-sized DNA cassettes averaging about 6 kb in size. This was accomplished through a combination of in vitro enzymatic methods and in vivo recombi-nation in Saccharomyces cerevisiae. The whole synthetic genome [582,970 base pairs (bp)] was stably grown as a yeast centromeric plasmid (YCp) (7). Several hurdles were overcome in transplanting and expressing a chemically synthesized chromo-some in a recipient cell. We needed to improve methods for extracting intact chromosomes from yeast. We also needed to learn how to transplant these genomes into a recipient bacterial cell to establish a cell controlled only by a synthetic ge-nome. Because M. genitalium has an extremely slow growth rate, we turned to two faster-growing mycoplasma species, M. mycoides subspecies capri (GM12) as donor, and M. capricolum sub-species capricolum (CK) as recipient. To establish conditions and procedures for transplanting the synthetic genome out of yeast, we developed methods for cloning entire bacterial chromosomes as centromeric plasmids in yeast, including a native M. mycoides genome (8, 9). However, initial attempts to extract the M. mycoides genome from yeast and transplant it into M. capricolum failed. We discovered that the donor and recipient mycoplasmas share a com-mon restriction system. The donor genome was methylated in the native M. mycoides cells and was therefore protected against restriction during the transplantation from a native donor cell (10). However, the bacterial genomes grown in yeast are unmethylated and so are not protected from the single restriction system of the recipient cell. We overcame this restriction barrier by methylat-ing the donor DNA with purified methylases or},
	number = {5987},
	journal = {Science},
	author = {Gibson, Daniel G. and Glass, John I. and Lartigue, Carole and Noskov, Vladimir N. and Chuang, Ray Yuan and Algire, Mikkel A. and Benders, Gwynedd A. and Montague, Michael G. and Ma, Li and Moodie, Monzia M. and Merryman, Chuck and Vashee, Sanjay and Krishnakumar, Radha and Assad-Garcia, Nacyra and Andrews-Pfannkoch, Cynthia and Denisova, Evgeniya A. and Young, Lei and Qi, Zhi Ng and Segall-Shapiro, Thomas H. and Calvey, Christopher H. and Parmar, Prashanth P. and Hutchison, Clyde A. and Smith, Hamilton O. and Venter, J. Craig},
	year = {2010},
	pmid = {20488990},
	note = {arXiv: z0024
ISBN: 1095-9203 (Electronic){\textbackslash}n0036-8075 (Linking)},
	pages = {52--56},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\QPX54KD3\\Gibson et al 2010.pdf:application/pdf},
}

@article{Erlich2017,
	title = {Supplementary {Materials} for {DNA} {Fountain} enables a robust and efficient storage architecture},
	volume = {954},
	doi = {10.1126/science.aaj2038},
	number = {March},
	author = {Erlich, Yaniv and Zielinski, Dina},
	year = {2017},
	pages = {950--954},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\6QM6WR5W\\Erlich&Zielinski17.pdf:application/pdf},
}

@article{Mearian,
	title = {{DNA} {Could} {Be} {Used} for {Data} {Storage}},
	journal = {Computerworld},
	author = {Mearian, Lucus},
	pages = {4},
	file = {DNA_storage 2013.PDF:C\:\\Users\\jstacey\\Zotero\\storage\\JKB8NUG9\\DNA_storage 2013.PDF:application/pdf},
}

@article{Hesketh2018,
	title = {Improving communication for interdisciplinary teams working on storage of digital information in {DNA}},
	volume = {7},
	issn = {2046-1402},
	url = {https://f1000research.com/articles/7-39/v1},
	doi = {10.12688/f1000research.13482.1},
	abstract = {Close collaboration between specialists from diverse backgrounds and working in different scientific domains is an effective strategy to overcome challenges in areas that interface between biology, chemistry, physics and engineering. Communication in such collaborations can itself be challenging.  Even when projects are successfully concluded, resulting publications - necessarily multi-authored - have the potential to be disjointed. Few, both in the field and outside, may be able to fully understand the work as a whole. This needs to be addressed to facilitate efficient working, peer review, accessibility and impact to larger audiences. We are an interdisciplinary team working in a nascent scientific area, the repurposing of DNA as a storage medium for digital information. In this note, we highlight some of the difficulties that arise from such collaborations and outline our efforts to improve communication through a glossary and a controlled vocabulary and accessibility via short plain-language summaries. We hope to stimulate early discussion within this emerging field of how our community might improve the description and presentation of our work to facilitate clear communication within and between research groups and increase accessibility to those not familiar with our respective fields - be it molecular biology, computer science, information theory or others that might become relevant in future. To enable an open and inclusive discussion we have created a glossary and controlled vocabulary as a cloud-based shared document and we invite other scientists to critique our suggestions and contribute their own ideas.},
	number = {May},
	journal = {F1000Research},
	author = {Hesketh, Emily E. and Sayir, Jossy and Goldman, Nick},
	year = {2018},
	pmid = {29707196},
	keywords = {communication, controlled vocabulary, digital information storage in, dna, dna-storage, glossary, interdisciplinary collaboration, short plain-language summaries, synthetic biology},
	pages = {39},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\5C8ZN3QI\\Hethketh et al 2018.pdf:application/pdf},
}

@article{Hameed2011,
	title = {{DNA} {Computation} {Based} {Approach} for {Enhanced} {Computing} {Power}},
	volume = {1},
	number = {1},
	journal = {International Journal of Emerging Science},
	author = {Hameed, Kashif},
	year = {2011},
	file = {Hameed 2011.PDF:C\:\\Users\\jstacey\\Zotero\\storage\\Q7YFQ6WI\\Hameed 2011.PDF:application/pdf},
}

@article{Grass2015,
	title = {Robust chemical preservation of digital information on {DNA} in silica with error-correcting codes},
	volume = {54},
	issn = {15213773},
	doi = {10.1002/anie.201411378},
	abstract = {Information, such as text printed on paper or images projected onto microfilm, can survive for over 500 years. However, the storage of digital information for time frames exceeding 50 years is challenging. Here we show that digital information can be stored on DNA and recovered without errors for considerably longer time frames. To allow for the perfect recovery of the information, we encapsulate the DNA in an inorganic matrix, and employ error-correcting codes to correct storage-related errors. Specifically, we translated 83 kB of information to 4991 DNA segments, each 158 nucleotides long, which were encapsulated in silica. Accelerated aging experiments were performed to measure DNA decay kinetics, which show that data can be archived on DNA for millennia under a wide range of conditions. The original information could be recovered error free, even after treating the DNA in silica at 70 °C for one week. This is thermally equivalent to storing information on DNA in central Europe for 2000 years.},
	number = {8},
	journal = {Angewandte Chemie - International Edition},
	author = {Grass, Robert N. and Heckel, Reinhard and Puddu, Michela and Paunescu, Daniela and Stark, Wendelin J.},
	year = {2015},
	pmid = {25650567},
	note = {ISBN: 2012290248},
	keywords = {DNA, Fossils, Information storage, Long-term memory, Sol-gel processes},
	pages = {2552--2555},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\IMS3SHTV\\Grass_et_al-2015-Angewandte_Chemie_International_Edition.pdf:application/pdf},
}

@article{Benenson2003,
	title = {{DNA} molecule provides a computing machine with both data and fuel},
	volume = {100},
	issn = {0027-8424},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0535624100},
	doi = {10.1073/pnas.0535624100},
	abstract = {The unique properties of DNA make it a fundamental building block in the fields of supramolecular chemistry, nanotechnology, nano-circuits, molecular switches, molecular devices, and molecular computing. In our recently introduced autonomous molecular automaton, DNA molecules serve as input, output, and software, and the hardware consists of DNA restriction and ligation enzymes using ATP as fuel. In addition to information, DNA stores energy, available on hybridization of complementary strands or hydrolysis of its phosphodiester backbone. Here we show that a single DNA molecule can provide both the input data and all of the necessary fuel for a molecular automaton. Each computational step of the automaton consists of a reversible software molecule input molecule hybridization followed by an irreversible software-directed cleavage of the input molecule, which drives the computation forward by increasing entropy and releasing heat. The cleavage uses a hitherto unknown capability of the restriction enzyme FokI, which serves as the hardware, to operate on a noncovalent software input hybrid. In the previous automaton, software input ligation consumed one software molecule and two ATP molecules per step. As ligation is not performed in this automaton, a fixed amount of software and hardware molecules can, in principle, process any input molecule of any length without external energy supply. Our experiments demonstrate 3 x 10(12) automata per microl performing 6.6 x 10(10) transitions per second per microl with transition fidelity of 99.9\%, dissipating about 5 x 10(-9) W microl as heat at ambient temperature.},
	number = {5},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Benenson, Y. and Adar, R. and Paz-Elizur, T. and Livneh, Z. and Shapiro, E.},
	year = {2003},
	pmid = {12601148},
	note = {ISBN: 0027-8424},
	pages = {2191--2196},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\BBICFPZL\\Benenson et al 2003.pdf:application/pdf},
}

@article{Heckel2018,
	title = {An archive written in {DNA}},
	volume = {36},
	issn = {15461696},
	url = {http://dx.doi.org/10.1038/nbt.4093},
	doi = {10.1038/nbt.4093},
	abstract = {New research sets a world record in the volume of data stored in and retrieved from DNA.},
	number = {3},
	journal = {Nature Biotechnology},
	author = {Heckel, Reinhard},
	year = {2018},
	pmid = {29509740},
	note = {Publisher: Nature Publishing Group},
	pages = {236--237},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\A288WY8H\\Heckel2018.pdf:application/pdf},
}

@article{Baum1995,
	title = {Building an {Associative} {Memory} {Vastly} {Larger} {Than} the {Brain} {Author} ( s ): {Eric} {B} . {Baum} {Published} by : {American} {Association} for the {Advancement} of {Science} {Stable} {URL} : https://www.jstor.org/stable/2886656 digitize , preserve and extend access to {Science} {B}},
	volume = {268},
	number = {5210},
	journal = {Science},
	author = {Baum, Eric B},
	year = {1995},
	pages = {583--585},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\QTYNKBC6\\Baum 2018.pdf:application/pdf},
}

@article{Bancroft2001,
	title = {Long-{Term} {Storage} of {Information} in {DNA}},
	volume = {293},
	issn = {0028-0836},
	doi = {10.1038/020493a0},
	number = {5536},
	journal = {Science},
	author = {Bancroft, Carter and Bowler, Timothy and Bloom, Brian and Clelland, Catherine Taylor},
	year = {2001},
	pmid = {8539599},
	note = {arXiv: cond-mat/0410550
ISBN: 9780470114735},
	pages = {1763--1765},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\UM2XYVUB\\Bancroft et al 2001.pdf:application/pdf},
}

@article{Boeke2016,
	title = {Genome {Project}-write: {A} {Grand} {Challenge} {Using} {Synthesis}, {Gene} {Editing} and {Other} {Technologies} to {Understand}, {Engineer} and {Test} {Living} {Systems}},
	author = {Boeke, Jef D. and Church, George M. and Hessel, Andrew and Kelley, Nancy J},
	year = {2016},
	pages = {1--22},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\UB2KRMIR\\Boeke et al 2016.pdf:application/pdf},
}

@techreport{TwistBioscience,
	title = {{DNA}-{Based} {Digital} {Storage} ({White} {Paper})},
	author = {{Twist Bioscience}},
	pages = {1--5},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\QXBZ4RZ5\\DNA_digital_data_storage_whitepaper_2017.pdf:application/pdf},
}

@article{Extance2002,
	title = {Digital {DNA}},
	author = {Extance, Andy},
	year = {2002},
	pages = {3--6},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\RIQBITJK\\Digital DNA article 2016.pdf:application/pdf},
}

@article{Church2012,
	title = {Next-{Generation} {Digital} {Information} {Storage} in {DNA}},
	volume = {337},
	doi = {10.1007/sl0869-007-9037-x},
	number = {6102},
	journal = {Science},
	author = {Church, George M. and Gao, Yuan and Kosuri, Sriram},
	year = {2012},
	note = {ISBN: 0506006905},
	pages = {1628},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\MB5KQZK4\\Church et al 2012.pdf:application/pdf},
}

@article{Bornholt2016,
	title = {A {DNA}-based archival storage system},
	volume = {50},
	issn = {0163-5980},
	doi = {10.1145/2872362.2872397},
	number = {2},
	journal = {ACM SIGOPS Operating Systems Review},
	author = {Bornholt, J},
	year = {2016},
	note = {ISBN: 9781450340915},
	keywords = {dna, archival storage, molecular computing},
	pages = {637},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\S6YVX6VA\\Bornholt et al 2016.pdf:application/pdf},
}

@article{Alaeddini2010,
	title = {Forensic implications of genetic analyses from degraded {DNA}-{A} review},
	volume = {4},
	issn = {18724973},
	url = {http://dx.doi.org/10.1016/j.fsigen.2009.09.007},
	doi = {10.1016/j.fsigen.2009.09.007},
	abstract = {Forensic DNA identification techniques are principally based on determination of the size or sequence of desired PCR products. The fragmentation of DNA templates or the structural modifications that can occur during the decomposition process can impact the outcomes of the analytical procedures. This study reviews the pathways involved in cell death and DNA decomposition and the subsequent difficulties these present in DNA analysis of degraded samples. © 2009 Elsevier Ireland Ltd.},
	number = {3},
	journal = {Forensic Science International: Genetics},
	author = {Alaeddini, Reza and Walsh, Simon J. and Abbas, Ali},
	year = {2010},
	pmid = {20215026},
	note = {Publisher: Elsevier Ireland Ltd
ISBN: 1872-4973},
	keywords = {DNA degradation, Cell death, Deoxyribonuclease, Forensics, PCR},
	pages = {148--157},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\YMWPT6UK\\Alaeddini et al  2007.pdf:application/pdf},
}

@article{Adler2011,
	title = {Survival and recovery of {DNA} from ancient teeth and bones},
	volume = {38},
	issn = {10959238},
	doi = {10.1016/j.jas.2010.11.010},
	abstract = {The recovery of genetic material from preserved hard skeletal remains is an essential part of ancient DNA, archaeological and forensic research. However, there is little understanding about the relative concentrations of DNA within different tissues, the impact of sampling methods on extracted DNA, or the role of environmentally-determined degradation rates on DNA survival in specimens. We examine these issues by characterizing the mitochondrial DNA (mtDNA) content of different hard and soft tissues in 42 ancient human and bovid specimens at a range of fragment lengths (77-235 bp) using real-time PCR. Remarkably, the standard drill speeds used to sample skeletal material (c. 1000 RPM) were found to decrease mtDNA yields up to 30 times (by 3.1 × 105mtDNA copies on average) compared to pulverization in a bone mill. This dramatic negative impact appears to relate to heat damage, and disappeared at very low drill speeds (e.g. 100 RPM). Consequently, many ancient DNA and forensic studies may have obtained false negative results, especially from important specimens which are commonly sampled with drills to minimize signs of damage. The mtDNA content of tooth cementum was found to be five times higher than the commonly used dentine (141 bp, p = 0.01), making the cementum-rich root tip the best sample for ancient human material. Lastly, mtDNA was found to display a consistent pattern of exponential fragmentation across many depositional environments, with different rates for geographic areas and tissue types, improving the ability to predict and understand DNA survival in preserved specimens. © 2010.},
	number = {5},
	journal = {Journal of Archaeological Science},
	author = {Adler, C. J. and Haak, W. and Donlon, D. and Cooper, A.},
	year = {2011},
	note = {ISBN: 0305-4403},
	keywords = {Ancient human mtDNA, Cementum, Dentine, DNA fragmentation, Drilling, Pulverizing},
	pages = {956--964},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\WHGDLQGB\\Adler et al 2010.pdf:application/pdf},
}

@article{Akin2011,
	title = {Electronic microarrays in {DNA} computing},
	volume = {11},
	issn = {15334899},
	url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-80051757205&partnerID=40&md5=12fe94548220735a574d28cb4a4f006b},
	doi = {10.1166/jnn.2011.38844717},
	abstract = {DNA Computing is a rapidly-developing interdisciplinary area which could benefit from more experimental results to solve practical problems with the current biological tools. In this study, we have integrated microelectronics and molecular biology techniques for the storage of information and basic arithmetic operations via DNA. Using 16 different complementary sequences of DNA, we stored 4 bits of information on an electronic microarray and read the data via the fluorescent signal strength coming from the microarray pads. We also showed the possibility of addition and subtraction of quantities of fluorescently tagged DNA determined via their fluorescent signal strength. We conclude that the hybrid technology we employed, based on a matured Si-CMOS platform, has the potential to strengthen the pursuit of DNA Computation as well as finding its own niche applications. Copyright © 2011 American Scientific Publishers.},
	number = {6},
	journal = {Journal of Nanoscience and Nanotechnology},
	author = {Akin, H E and Karabay, D A O and Kyle, J R and Mills Jr., A P and Ozkan, C S and Ozkan, M},
	year = {2011},
	keywords = {DNA, Computing, Fluorescence, Hybridization, Massive Parallelism, Microarray, Permeation Layer},
	pages = {4717--4723},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\VDTQQVNZ\\Atin et al 2011.pdf:application/pdf},
}

@article{Alonso2004,
	title = {Real-time {PCR} designs to estimate nuclear and mitochondrial {DNA} copy number in forensic and ancient {DNA} studies},
	volume = {139},
	issn = {03790738},
	doi = {10.1016/j.forsciint.2003.10.008},
	abstract = {We explore different designs to estimate both nuclear and mitochondrial human DNA (mtDNA) content based on the detection of the 5′ nuclease activity of the Taq DNA polymerase using fluorogenic probes and a real-time quantitative PCR detection system. Human mtDNA quantification was accomplished by monitoring the real-time progress of the PCR-amplification of two different fragment sizes (113 and 287bp) within the hypervariable region I (HV1) of the mtDNA control region, using two fluorogenic probes to specifically determine the mtDNA copy of each fragment size category. This mtDNA real-time PCR design has been used to assess the mtDNA preservation (copy number and degradation state) of DNA samples retrieved from 500 to 1500 years old human remains that showed low copy number and highly degraded mtDNA. The quantification of nuclear DNA was achieved by real-time PCR of a segment of the X-Y homologous amelogenin (AMG) gene that allowed the simultaneous estimation of a Y-specific fragment (AMGY: 112bp) and a X-specific fragment (AMGX: 106bp) making possible not only haploid or diploid DNA quantitation but also sex determination. The AMG real-time PCR design has been used to quantify a set of 57 DNA samples from 4-5 years old forensic bone remains with improved sensitivity compared with the slot-blot hybridization method. The potential utility of this technology to improve the quality of some PCR-based forensic and ancient DNA studies (microsatellite typing and mtDNA sequencing) is discussed. © 2003 Elsevier Ireland Ltd. All rights reserved.},
	number = {2-3},
	journal = {Forensic Science International},
	author = {Alonso, Antonio and Martín, Pablo and Albarrán, Cristina and García, Pilar and García, Oscar and Fernández De Simón, Lourdes and García-Hirschfeld, Julia and Sancho, Manuel and De La Rúa, Concepción and Fernández-Piqueras, Jose},
	year = {2004},
	pmid = {15040907},
	note = {ISBN: 0379-0738 (Print){\textbackslash}n0379-0738 (Linking)},
	keywords = {Amelogenin gene, Ancient DNA, Forensic genetics, Mitochondrial DNA control region, Real-time PCR},
	pages = {141--149},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\LHJPLBGR\\Alonso et al 2003.pdf:application/pdf},
}

@article{DeAlmeida2013,
	title = {Treatment of long-term stored {DNA}-{Comparison} between different methods to obtain high-quality material},
	volume = {34},
	issn = {01730835},
	doi = {10.1002/elps.201300245},
	abstract = {Long-term stored DNA can be sometimes the only source of genetic material of an organism that does not exist anymore, but a research interest still persists. However, there is a lack of information about useful methods to improve quality from such type of material. In this study, we compared four different protocols using DNA samples collected in 1998. Fresh DNA was also tested aiming to check the differences between these two material types. Sixteen samples of each DNA type treated with phenol-chloroform with PEG 5.0\%, silica-gel membrane spin column, PEG 7.5\%, and glass-fiber matrix spin column were submitted to spectrophotometer measurements, electrophoresis, PCR, and RFLP-PCR to assess the best method concerning yield, quality, and purity. Based on the results, purification with PEG 7.5\% was considered the best method to treat aged DNA samples. In addition to the efficiency, this protocol has low cost. Analyzing the data, we also conclude that long-term stored DNA may be considered a reliable and potential resource for future molecular studies.},
	number = {20-21},
	journal = {Electrophoresis},
	author = {De Almeida, Máira Pedroso and do Nascimento, Carlos Souza and Périssé, Iuri Viotti and de Souza Duarte, Marcio and Veroneze, Renata and Facioni Guimarães, Simone E.},
	year = {2013},
	pmid = {23893799},
	keywords = {DNA purification, Long-term storage, Polymerase chain reaction},
	pages = {3039--3045},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\6XQR8MND\\Almeida_et_al 2013.pdf:application/pdf},
}

@article{Ball2016,
	title = {The problems of biological information {Opinion} piece {Subject} {Areas} : {Author} for correspondence :},
	author = {Ball, Philip and Ball, Philip},
	year = {2016},
	keywords = {biophysics, chemical biology, complexity, computational biology, statistical physics},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\BYC7Y7CM\\Ball 2016.pdf:application/pdf},
}

@article{Bulik-Sullivan2015,
	title = {An atlas of genetic correlations across human diseases and traits},
	volume = {47},
	issn = {15461718},
	url = {http://dx.doi.org/10.1038/ng.3406},
	doi = {10.1038/ng.3406},
	abstract = {Identifying genetic correlations between complex traits and diseases can provide useful etiological insights and help prioritize likely causal relationships. The major challenges preventing estimation of genetic correlation from genome-wide association study (GWAS) data with current methods are the lack of availability of individual-level genotype data and widespread sample overlap among meta-analyses. We circumvent these difficulties by introducing a technique-cross-trait LD Score regression-for estimating genetic correlation that requires only GWAS summary statistics and is not biased by sample overlap. We use this method to estimate 276 genetic correlations among 24 traits. The results include genetic correlations between anorexia nervosa and schizophrenia, anorexia and obesity, and educational attainment and several diseases. These results highlight the power of genome-wide analyses, as there currently are no significantly associated SNPs for anorexia nervosa and only three for educational attainment.},
	number = {11},
	journal = {Nature Genetics},
	author = {Bulik-Sullivan, Brendan and Finucane, Hilary K. and Anttila, Verneri and Gusev, Alexander and Day, Felix R. and Loh, Po Ru and Duncan, Laramie and Perry, John R.B. and Patterson, Nick and Robinson, Elise B. and Daly, Mark J. and Price, Alkes L. and Neale, Benjamin M.},
	year = {2015},
	pmid = {26414676},
	note = {arXiv: 014498
Publisher: Nature Publishing Group
ISBN: 1061-4036},
	pages = {1236--1241},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\NAVX2IIT\\bulik-sullivan-2015-an-atlas-of-genetic-correlations.pdf:application/pdf},
}

@article{Boyle2017,
	title = {An {Expanded} {View} of {Complex} {Traits}: {From} {Polygenic} to {Omnigenic}},
	volume = {169},
	issn = {10974172},
	url = {http://dx.doi.org/10.1016/j.cell.2017.05.038},
	doi = {10.1016/j.cell.2017.05.038},
	abstract = {A central goal of genetics is to understand the links between genetic variation and disease. Intuitively, one might expect disease-causing variants to cluster into key pathways that drive disease etiology. But for complex traits, association signals tend to be spread across most of the genome—including near many genes without an obvious connection to disease. We propose that gene regulatory networks are sufficiently interconnected such that all genes expressed in disease-relevant cells are liable to affect the functions of core disease-related genes and that most heritability can be explained by effects on genes outside core pathways. We refer to this hypothesis as an “omnigenic” model.},
	number = {7},
	journal = {Cell},
	author = {Boyle, Evan A. and Li, Yang I. and Pritchard, Jonathan K.},
	year = {2017},
	pmid = {28622505},
	note = {Publisher: Elsevier
ISBN: 0092-8674, 1097-4172},
	pages = {1177--1186},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\NAUE2TMU\\boyle-2017-omnigenic.pdf:application/pdf},
}

@article{Leslie2015,
	title = {The fine-scale genetic structure of the {British} population},
	volume = {519},
	issn = {14764687},
	doi = {10.1038/nature14230},
	abstract = {Fine-scale genetic variation between human populations is interesting as a signature of historical demographic events and because of its potential for confounding disease studies. We use haplotype-based statistical methods to analyse genome-wide single nucleotide polymorphism (SNP) data from a carefully chosen geographically diverse sample of 2,039 individuals from the United Kingdom. This reveals a rich and detailed pattern of genetic differentiation with remarkable concordance between genetic clusters and geography. The regional genetic differentiation and differing patterns of shared ancestry with 6,209 individuals from across Europe carry clear signals of historical demographic events. We estimate the genetic contribution to southeastern England from Anglo-Saxon migrations to be under half, and identify the regions not carrying genetic material from these migrations. We suggest significant pre-Roman but post-Mesolithic movement into southeastern England from continental Europe, and show that in non-Saxon parts of the United Kingdom, there exist genetically differentiated subgroups rather than a general ‘Celtic’ population.},
	number = {7543},
	journal = {Nature},
	author = {Leslie, Stephen and Winney, Bruce and Hellenthal, Garrett and Davison, Dan and Boumertit, Abdelhamid and Day, Tammy and Hutnik, Katarzyna and Royrvik, Ellen C. and Cunliffe, Barry and Lawson, Daniel J. and Falush, Daniel and Freeman, Colin and Pirinen, Matti and Myers, Simon and Robinson, Mark and Donnelly, Peter and Bodmer, Walter},
	year = {2015},
	pmid = {25788095},
	note = {arXiv: NIHMS150003
ISBN: 0000017892},
	pages = {309--314},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\CP2CFICX\\leslie 2015 The fine-scale genetic structure of the British population.pdf:application/pdf},
}

@article{Gibson2012,
	title = {Rare and common variants: {Twenty} arguments},
	volume = {13},
	issn = {14710056},
	url = {http://dx.doi.org/10.1038/nrg3118},
	doi = {10.1038/nrg3118},
	abstract = {Genome-wide association studies have greatly improved our understanding of the genetic basis of disease risk. The fact that they tend not to identify more than a fraction of the specific causal loci has led to divergence of opinion over whether most of the variance is hidden as numerous rare variants of large effect or as common variants of very small effect. Here I review 20 arguments for and against each of these models of the genetic basis of complex traits and conclude that both classes of effect can be readily reconciled.},
	number = {2},
	journal = {Nature Reviews Genetics},
	author = {Gibson, Greg},
	year = {2012},
	pmid = {22251874},
	note = {arXiv: NIHMS150003
Publisher: Nature Publishing Group
ISBN: 1471-0064 (Electronic){\textbackslash}r1471-0056 (Linking)},
	pages = {135--145},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\CQXDVYCU\\gibson 2012 rare and common variants twenty arguments.pdf:application/pdf},
}

@article{Malaspinas2016,
	title = {A genomic history of {Aboriginal} {Australia}},
	volume = {538},
	issn = {14764687},
	url = {http://dx.doi.org/10.1038/nature18299},
	doi = {10.1038/nature18299},
	abstract = {The population history of Aboriginal Australians remains largely uncharacterized. Here we generate high-coverage genomes for 83 Aboriginal Australians (speakers of Pama–Nyungan languages) and 25 Papuans from the New Guinea Highlands. We find that Papuan and Aboriginal Australian ancestors diversified 25–40 thousand years ago (kya), suggesting pre-Holocene population structure in the ancient continent of Sahul (Australia, New Guinea and Tasmania). However, all of the studied Aboriginal Australians descend from a single founding population that differentiated {\textasciitilde}10–32 kya. We infer a population expansion in northeast Australia during the Holocene epoch (past 10,000 years) associated with limited gene flow from this region to the rest of Australia, consistent with the spread of the Pama–Nyungan languages. We estimate that Aboriginal Australians and Papuans diverged from Eurasians 51–72 kya, following a single out-of-Africa dispersal, and subsequently admixed with archaic populations. Finally, we report evidence of selection in Aboriginal Australians potentially associated with living in the desert.},
	number = {7624},
	journal = {Nature},
	author = {Malaspinas, Anna Sapfo and Westaway, Michael C. and Muller, Craig and Sousa, Vitor C. and Lao, Oscar and Alves, Isabel and Bergström, Anders and Athanasiadis, Georgios and Cheng, Jade Y. and Crawford, Jacob E. and Heupink, Tim H. and MacHoldt, Enrico and Peischl, Stephan and Rasmussen, Simon and Schiffels, Stephan and Subramanian, Sankar and Wright, Joanne L. and Albrechtsen, Anders and Barbieri, Chiara and Dupanloup, Isabelle and Eriksson, Anders and Margaryan, Ashot and Moltke, Ida and Pugach, Irina and Korneliussen, Thorfinn S. and Levkivskyi, Ivan P. and Moreno-Mayar, J. Víctor and Ni, Shengyu and Racimo, Fernando and Sikora, Martin and Xue, Yali and Aghakhanian, Farhang A. and Brucato, Nicolas and Brunak, Søren and Campos, Paula F. and Clark, Warren and Ellingvåg, Sturla and Fourmile, Gudjugudju and Gerbault, Pascale and Injie, Darren and Koki, George and Leavesley, Matthew and Logan, Betty and Lynch, Aubrey and Matisoo-Smith, Elizabeth A. and McAllister, Peter J. and Mentzer, Alexander J. and Metspalu, Mait and Migliano, Andrea B. and Murgha, Les and Phipps, Maude E. and Pomat, William and Reynolds, Doc and Ricaut, Francois Xavier and Siba, Peter and Thomas, Mark G. and Wales, Thomas and Wall, Colleen Ma Run and Oppenheimer, Stephen J. and Tyler-Smith, Chris and Durbin, Richard and Dortch, Joe and Manica, Andrea and Schierup, Mikkel H. and Foley, Robert A. and Lahr, Marta Mirazón and Bowern, Claire and Wall, Jeffrey D. and Mailund, Thomas and Stoneking, Mark and Nielsen, Rasmus and Sandhu, Manjinder S. and Excoffier, Laurent and Lambert, David M. and Willerslev, Eske},
	year = {2016},
	pmid = {11507039},
	note = {arXiv: NIHMS150003
Publisher: Nature Publishing Group
ISBN: 0008-5472 (Print){\textbackslash}r0008-5472 (Linking)},
	pages = {207--214},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\G5VFFPR6\\malaspinas 2016 A genomic history of Aboriginal Australia.pdf:application/pdf},
}

@article{Falush2016,
	title = {A tutorial on how (not) to over-interpret {STRUCTURE}/{ADMIXTURE} bar plots},
	issn = {2041-1723},
	doi = {10.1101/066431},
	abstract = {Genetic clustering algorithms, implemented in popular programs such as STRUCTURE and ADMIXTURE, have been used extensively in the characterisation of individuals and populations based on genetic data. A successful example is reconstruction of the genetic history of African Americans who are a product of recent admixture between highly differentiated populations. Histories can also be reconstructed using the same procedure for groups which do not have admixture in their recent history, where recent genetic drift is strong or that deviate in other ways from the underlying inference model. Unfortunately, such histories can be misleading. We have implemented an approach (available at www.paintmychromsomes.com) to assessing the goodness of fit of the model using the ancestry 'palettes' estimated by CHROMOPAINTER and apply it to both simulated and real examples. Combining these complementary analyses with additional methods that are designed to test specific hypothesis allows a richer and more robust analysis of recent demographic history based on genetic data.},
	journal = {bioRxiv},
	author = {Falush, Daniel and Dorp, Lucy van and Lawson, Daniel J.},
	year = {2016},
	note = {ISBN: 4146701805257},
	pages = {066431},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\6UJMH5I3\\falush 2016 A tutorial on how (not) to over-interpret STRUCTUREand ADMIXTURE bar plots (1).pdf:application/pdf},
}

@article{Uyeda2018,
	title = {Rethinking phylogenetic comparative methods},
	issn = {1063-5157},
	url = {https://academic.oup.com/sysbio/advance-article/doi/10.1093/sysbio/syy031/4985805},
	doi = {10.1093/sysbio/syy031},
	abstract = {As a result of the process of descent with modification, closely related species tend to be similar to one another in a myriad different ways. In statistical terms, this means that traits measured on one species will not be independent of traits measured on others. Since their introduction in the 1980s, phylogenetic comparative methods (PCMs) have been framed as a solution to this problem. In this paper, we argue that this way of thinking about PCMs is deeply misleading. Not only has this sowed widespread confusion in the literature about what PCMs are doing but has led us to develop methods that are susceptible to the very thing we sought to build defenses against {\textbackslash}---{\textbar} unreplicated evolutionary events. Through three Case Studies, we demonstrate that the susceptibility to singular events indeed a recurring problem in comparative biology that links several seemingly unrelated controversies. In each Case Study we propose a potential solution to the problem. While the details of our proposed solutions differ, they share a common theme: unifying hypothesis testing with data-driven approaches (which we term "phylogenetic natural history") to disentangle the impact of singular evolutionary events from that of the factors we are investigating. More broadly, we argue that our field has, at times, been sloppy when weighing evidence in support of causal hypotheses. We suggest that one way to refine our inferences is to re-imagine phylogenies as probabilistic graphical models; adopting this way of thinking will help clarify precisely what we are testing and what evidence supports our claims.},
	journal = {Systematic Biology},
	author = {Uyeda, Josef C and Zenil-Ferguson, Rosana and Pennell, Matthew W},
	year = {2018},
	keywords = {causality, graphical models, macroevolution, phylogenetic natural},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\U4S4DKFJ\\Uyeda_etal_2017_bioRxiv_problem_with_PCMs.pdf:application/pdf},
}

@article{Burnham2011,
	title = {{AIC} model selection and multimodel inference in behavioral ecology: {Some} background, observations, and comparisons},
	volume = {65},
	issn = {03405443},
	doi = {10.1007/s00265-010-1029-6},
	abstract = {We briefly outline the information-theoretic (I-T) approaches to valid inference including a review of some simple methods for making formal inference from all the hypotheses in the model set (multimodel inference). The I-T approaches can replace the usual t tests and ANOVA tables that are so inferentially limited, but still commonly used. The I-T methods are easy to compute and understand and provide formal measures of the strength of evidence for both the null and alternative hypotheses, given the data. We give an example to highlight the importance of deriving alternative hypotheses and representing these as probability models. Fifteen technical issues are addressed to clarify various points that have appeared incorrectly in the recent literature. We offer several remarks regarding the future of empirical science and data analysis under an I-T framework.},
	number = {1},
	journal = {Behavioral Ecology and Sociobiology},
	author = {Burnham, Kenneth P. and Anderson, David R. and Huyvaert, Kathryn P.},
	year = {2011},
	pmid = {3527},
	note = {ISBN: 0340-5443},
	keywords = {AIC, Evidence, Kullback-Leibler information, Model averaging, Model likelihoods, Model probabilities, Model selection, Multimodel inference},
	pages = {23--35},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\V9HIHWAP\\Burnham_Anderson_Huyvaert_2011_BehEcoSociobio_AIC_ecology_discrete.pdf:application/pdf},
}

@article{Burnham2014,
	title = {P values are only an index to evidence: 20th- vs. 21st-century statistical science.},
	volume = {95},
	issn = {0012-9658},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/24804444},
	doi = {10.1890/13-1066.1},
	abstract = {Early statistical methods focused on pre-data probability statements (i.e., data as random variables) such as P values; these are not really inferences nor are P values evidential. Statistical science clung to these principles throughout much of the 20th century as a wide variety of methods were developed for special cases. Looking back, it is clear that the underlying paradigm (i.e., testing and P values) was weak. As Kuhn (1970) suggests, new paradigms have taken the place of earlier ones: this is a goal of good science. New methods have been developed and older methods extended and these allow proper measures of strength of evidence and multimodel inference. It is time to move forward with sound theory and practice for the difficult practical problems that lie ahead. Given data the useful foundation shifts to post-data probability statements such as model probabilities (Akaike weights) or related quantities such as odds ratios and likelihood intervals. These new methods allow formal inference from multiple models in the a prior set. These quantities are properly evidential. The past century was aimed at finding the "best" model and making inferences from it. The goal in the 21st century is to base inference on all the models weighted by their model probabilities (model averaging). Estimates of precision can include model selection uncertainty leading to variances conditional on the model set. The 21st century will be about the quantification of information, proper measures of evidence, and multi-model inference. Nelder (1999:261) concludes, "The most important task before us in developing statistical science is to demolish the P-value culture, which has taken root to a frightening extent in many areas of both pure and applied science and technology".},
	number = {3},
	journal = {Ecology},
	author = {Burnham, K P and Anderson, David R.},
	year = {2014},
	pmid = {24804444},
	note = {ISBN: 0012-9658},
	keywords = {Data Interpretation, Models, Research Design, Statistical},
	pages = {627--30},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\M3XBLZGU\\Burnham_Anderson_2014_Ecology_p-values.pdf:application/pdf},
}

@article{Anderson2002,
	title = {Avoiding {Pitfalls} {When} {Using} {Information}-{Theoretic} {Methods} {Author} ( s ): {David} {R} . {Anderson} and {Kenneth} {P} . {Burnham} {Published} by : {Wiley} on behalf of the {Wildlife} {Society} {Stable} {URL} : http://www.jstor.org/stable/3803155},
	volume = {66},
	number = {3},
	journal = {Journal of Wildlife Management},
	author = {Anderson, David R. and Burnham, Kenneth P.},
	year = {2002},
	keywords = {akaike, analysis guidelines, any methodology can be, in the sci-, information, information-theoretic methods, kullback-leibler, misused, model selection uncertainty, modeling, p-values, s information criterion, support for null hypothesis, testing and associat-},
	pages = {912--918},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\3VRJCB9Q\\Anderson_Burnham_2002_JWildlifeManage_avoiding_pitfalls_AIC.pdf:application/pdf},
}

@book{anderson_model_2008,
	title = {Model {Based} {Inference} in the {Life} {Sciences}: {A} {Primer} on {Evidence}},
	isbn = {978-0-387-74073-7},
	publisher = {Springer},
	author = {Anderson, D.R.},
	year = {2008},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\SDU2LAX8\\Anderson_2008_model-based_inference_life_sciences.pdf:application/pdf},
}

@article{Rabosky2010,
	title = {Extinction rates should not be estimated from molecular phylogenies},
	volume = {64},
	issn = {00143820},
	doi = {10.1111/j.1558-5646.2009.00926.x},
	abstract = {Molecular phylogenies contain information about the tempo and mode of species diversification through time. Because extinction leaves a characteristic signature in the shape of molecular phylogenetic trees, many studies have used data from extant taxa only to infer extinction rates. This is a promising approach for the large number of taxa for which extinction rates cannot be estimated from the fossil record. Here, I explore the consequences of violating a common assumption made by studies of extinction from phylogenetic data. I show that when diversification rates vary among lineages, simple estimators based on the birth-death process are unable to recover true extinction rates. This is problematic for phylogenetic trees with complete taxon sampling as well as for the simpler case of clades with known age and species richness. Given the ubiquity of variation in diversification rates among lineages and clades, these results suggest that extinction rates should not be estimated in the absence of fossil data.},
	number = {6},
	journal = {Evolution},
	author = {Rabosky, Daniel L.},
	year = {2010},
	pmid = {20030708},
	note = {ISBN: 0014-3820},
	keywords = {Adaptive radiation, Extinction, Macroevolution, Phylogenetics},
	pages = {1816--1824},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\QB4HNLNP\\Rabosky_2010_Evo_dont_estimate_extinction.pdf:application/pdf},
}

@article{Quental2010,
	title = {Diversity dynamics: {Molecular} phylogenies need the fossil record},
	volume = {25},
	issn = {01695347},
	url = {http://dx.doi.org/10.1016/j.tree.2010.05.002},
	doi = {10.1016/j.tree.2010.05.002},
	abstract = {Over the last two decades, new tools in the analysis of molecular phylogenies have enabled study of the diversification dynamics of living clades in the absence of information about extinct lineages. However, computer simulations and the fossil record show that the inability to access extinct lineages severely limits the inferences that can be drawn from molecular phylogenies. It appears that molecular phylogenies can tell us only when there have been changes in diversification rates, but are blind to the true diversity trajectories and rates of origination and extinction that have led to the species that are alive today. We need to embrace the fossil record if we want to fully understand the diversity dynamics of the living biota. © 2010 Elsevier Ltd.},
	number = {8},
	journal = {Trends in Ecology and Evolution},
	author = {Quental, Tiago B. and Marshall, Charles R.},
	year = {2010},
	pmid = {20646780},
	note = {Publisher: Elsevier Ltd
ISBN: 0169-5347},
	pages = {435--441},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\9ZQXQJVX\\Quental_Marshall_2010_TREE_fossils_diversification.pdf:application/pdf},
}

@article{Marshall2017,
	title = {Five palaeobiological laws needed to understand the evolution of the living biota},
	volume = {1},
	issn = {2397334X},
	url = {http://dx.doi.org/10.1038/s41559-017-0165},
	doi = {10.1038/s41559-017-0165},
	abstract = {Five laws derived from fossil data describe the relationships between species extinction and longevity, species richness, origination rates, extinction rates and diversification. These laws are crucial to the study of evolution and ecology.},
	number = {6},
	journal = {Nature Ecology and Evolution},
	author = {Marshall, Charles R.},
	year = {2017},
	pmid = {28812640},
	note = {Publisher: Macmillan Publishers Limited},
	pages = {1--6},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\AK6QAXWI\\Marshall_2017_NEE_5_macroevolutionary_laws.pdf:application/pdf},
}

@article{Maddison2015,
	title = {Society of {Systematic} {Biologists} {Estimating} a {Binary} {Character} ' s {Effect} on {Speciation} and {Extinction} {Estimating} a {Binary} {Character} ' s {Effect} on {Speciation}},
	volume = {56},
	doi = {10.1080/10635150701607033},
	number = {5},
	author = {Maddison, Wayne P and Midford, Peter E and Otto, Sarah P},
	year = {2015},
	pages = {701--710},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\D4IBLU8B\\Maddison_etal_2007_SysBio_character-dependent_diversification.pdf:application/pdf},
}

@article{Matzke2015,
	title = {{FAQ} on “{The} {Evolution} of {Antievolution} {Legislation} {After} {Kitzmiller} v. {Dover}”},
	volume = {139},
	journal = {Science},
	author = {Matzke, Nicholas J.},
	year = {2015},
	pages = {131--139},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\FVIR9UNT\\Matzke_2016_Science_Evo_of_antievo_FAQ_cover_article_SuppMat.pdf:application/pdf},
}

@book{Harmon,
	title = {Phylogenetic {Comparative} {Methods}},
	url = {https://lukejharmon.github.io/pcm/},
	author = {Harmon, Luke},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\RSA2U7FL\\phylogeneticComparativeMethods.pdf:application/pdf},
}

@incollection{franklin_statistical_2001,
	address = {Washington, D.C.},
	title = {Statistical {Model} {Selection}: {An} {Alternative} to {Null} {Hypothesis} {Testing}},
	booktitle = {Modeling in natural resource management: {Development}, interpretation, and application},
	publisher = {Island Press},
	author = {Franklin, Alan B and Shenk, Tanya M and Anderson, David R and Burnham, Kenneth P},
	editor = {Shenk, Tanya M. and Franklin, Alan B.},
	year = {2001},
	pages = {75--90},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\TVTJWJY6\\Franklin_etal_Anderson_Burnham_2001_Stat_model_selection_alternative_NHST.pdf:application/pdf},
}

@article{Gavryushkina2017,
	title = {Bayesian total-evidence dating reveals the recent crown radiation of penguins},
	volume = {66},
	issn = {1076836X},
	doi = {10.1093/sysbio/syw060},
	abstract = {The total-evidence approach to divergence-time dating uses molecular and morphological data from extant and fossil species to infer phylogenetic relationships, species divergence times, and macroevolutionary parameters in a single coherent framework. Current model-based implementations of this approach lack an appropriate model for the tree describing the diversification and fossilization process and can produce estimates that lead to erroneous conclusions. We address this shortcoming by providing a total-evidence method implemented in a Bayesian framework. This approach uses a mechanistic tree prior to describe the underlying diversification process that generated the tree of extant and fossil taxa. Previous attempts to apply the total-evidence approach have used tree priors that do not account for the possibility that fossil samples may be direct ancestors of other samples. The fossilized birth-death (FBD) process explicitly models the diversification, fossilization, and sampling processes and naturally allows for sampled ancestors. This model was recently applied to estimate divergence times based on molecular data and fossil occurrence dates. We incorporate the FBD model and a model of morphological trait evolution into a Bayesian total-evidence approach to dating species phylogenies. We apply this method to extant and fossil penguins and show that the modern penguins radiated much more recently than has been previously estimated, with the basal divergence in the crown clade occurring at {\textasciitilde}12.7 Ma and most splits leading to extant species occurring in the last 2 million years. Our results demonstrate that including stem-fossil diversity can greatly improve the estimates of the divergence times of crown taxa. The method is available in BEAST2 (v. 2.4) www.beast2.org with packages SA (v. at least 1.1.4) and morph-models (v. at least 1.0.4).},
	number = {1},
	journal = {Systematic Biology},
	author = {Gavryushkina, Alexandra and Heath, Tracy A. and Ksepka, Daniel T. and Stadler, Tanja and Welch, David and Drummond, Alexei J.},
	year = {2017},
	pmid = {27558632},
	note = {arXiv: 1506.04797
ISBN: 1063-5157},
	keywords = {Phylogenetics, Birth-death process, Calibration, Divergence times, MCMC},
	pages = {57--73},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\JUMT5JPI\\Gavyrushkina_etal_2017_SysBio_fossil_penguins.pdf:application/pdf},
}

@article{Pagel1993,
	title = {Detecting correlated evolution on phylogenies: a general method for the comparative analysis of discrete characters},
	volume = {255},
	issn = {14712970},
	url = {http://rspb.royalsocietypublishing.org/},
	doi = {10.1098/rspb.1994.0006},
	abstract = {I present a new statistical method for analysing the relationship between two discrete characters that are measured across a group of hierarchically evolved species or populations. The method assesses whether a pattern of association across the group is evidence for correlated evolutionary change in the two characters. The method takes into account information on the lengths of the branches of phylogenetic trees, develops estimates of the rates of change of the discrete characters, and tests the hypothesis of correlated evolution without relying upon reconstructions of the ancestral character states. A likelihood ratio test statistic is used to discriminate between two models that are fitted to the data: one allowing only for independent evolution of the two characters, the other allowing for correlated evolution. Tests of specific directional hypotheses can also be made. The method is illustrated with an application to the Hominoidea.},
	number = {1342},
	journal = {Proceedings of the Royal Society: Biological Sciences},
	author = {Pagel, Mark},
	year = {1993},
	pmid = {1695},
	note = {ISBN: 09628452},
	keywords = {comparative method, phylogeny},
	pages = {37--45},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\XL4CT525\\Pagel1994.pdf:application/pdf},
}

@article{Beaulieu2015,
	title = {Extinction can be estimated from moderately sized molecular phylogenies},
	volume = {69},
	issn = {15585646},
	doi = {10.1111/evo.12614},
	abstract = {Hundreds of studies have been dedicated to estimating speciation and extinction from phylogenies of extant species. While it has long been known that estimates of extinction rates using trees of extant organisms are often uncertain, an influential paper by Rabosky (2010) suggested that when birth rates vary continuously across the tree estimates of the extinction fraction (i.e., extinction rate/speciation rate) will appear strongly bimodal, with a peak suggesting no extinction and a peak implying speciation and extinction rates are approaching equality. On the basis of these results, and the realistic nature of this form of rate variation, it is now generally assumed by many practitioners that extinction cannot be understood from molecular phylogenies alone. Here we reevaluated and extended the analyses of Rabosky (2010) and come to the opposite conclusion – namely, that it is possible to estimate extinction from molecular phylogenies, even with model violations due to heritable variation in diversification rate. Note that while it may be tempting to interpret our study as advocating the application of simple birth-death models, our goal here is to show how a particular model violation does not necessitate the abandonment of an entire field: use prudent caution, but do not abandon all hope. Key},
	number = {4},
	journal = {Evolution},
	author = {Beaulieu, Jeremy M. and O'Meara, Brian C.},
	year = {2015},
	pmid = {25639334},
	note = {ISBN: 0014-3820},
	keywords = {Extinction, Birth-death, Diversification, Molecular phylogenies, Speciation},
	pages = {1036--1043},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\LTQ53PF5\\OMeara_Beaulieu_2015_Evo_brief_extinction_can_be_estimated.pdf:application/pdf},
}

@article{Nee2006,
	title = {Birth-{Death} {Models} in {Macroevolution}},
	volume = {37},
	issn = {1543-592X},
	url = {http://www.annualreviews.org/doi/10.1146/annurev.ecolsys.37.091305.110035},
	doi = {10.1146/annurev.ecolsys.37.091305.110035},
	abstract = {Birth-death models, and their subsets—the pure birth and pure death models—have a long history of use for informing thinking about macroevolutionary patterns. Here we illustrate with examples the wide range of questions they have been used to address, including estimating and comparing rates of diversification of clades, investigating the “shapes” of clades, and some rather surprising uses such as estimating speciation rates from data that are not resolved below the level of the genus. The raw data for inference can be the fossil record or the molecular phylogeny of a clade, and we explore the similarites and differences in the behavior of the birth-death models when applied to these different forms of data.},
	number = {1},
	journal = {Annual Review of Ecology, Evolution, and Systematics},
	author = {Nee, Sean},
	year = {2006},
	note = {ISBN: 1543-592X},
	keywords = {fossil record, paleobiology, phylogenetics},
	pages = {1--17},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\9FGXBTZY\\Nee_2008_AnnRev_birth-death_models_macroevolution.pdf:application/pdf},
}

@article{Akaike1974,
	title = {A new look at the statistical model identification},
	volume = {AC-19},
	number = {6},
	journal = {IEEE Transactions on Automatic Control},
	author = {Akaike, Hirotuhu},
	year = {1974},
	note = {ISBN: 9781139095112},
	pages = {716--723},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\6RZQ4463\\09.0_pp_220_239_References.pdf:application/pdf},
}

@article{Matzke2016,
	title = {Inferring node dates from tip dates in fossil {Canidae}: {The} importance of tree priors},
	volume = {12},
	issn = {1744957X},
	doi = {10.1098/rsbl.2016.0328},
	abstract = {NJM, 0000-0002-8698-7656 Tip-dating methods are becoming popular alternatives to traditional node cali-bration approaches for building time-scaled phylogenetic trees, but questions remain about their application to empirical datasets. We compared the per-formance of the most popular methods against a dated tree of fossil Canidae derived from previously published monographs. Using a canid morphology dataset, we performed tip-dating using BEAST v. 2.1.3 and MRBAYES v. 3.2.5. We find that for key nodes (Canis, approx. 3.2 Ma, Caninae approx. 11.7 Ma) a non-mechanistic model using a uniform tree prior produces estimates that are unrealistically old (27.5, 38.9 Ma). Mechanistic models (incorporating line-age birth, death and sampling rates) estimate ages that are closely in line with prior research. We provide a discussion of these two families of models (mechanistic versus non-mechanistic) and their applicability to fossil datasets.},
	number = {8},
	journal = {Biology Letters},
	author = {Matzke, Nicholas J. and Wright, April},
	year = {2016},
	pmid = {27512133},
	keywords = {BEASTMASTER, Canidae, MRBAYES, Tip-dating, Total evidence dating, Uniform tree prior},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\7ZI9AAML\\Matzke_Wright_2016_BioLett_Canidae_tip-dating_published.pdf:application/pdf},
}

@incollection{Drummond2014k,
	title = {14.1 {Basic} patterns},
	isbn = {978-1-139-09511-2},
	booktitle = {Bayesian {Evolutionary} {Analysis} with {BEAST}},
	author = {Drummond, Alexei J. and Rambaut, Andrew},
	year = {2014},
	pages = {195--206},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\JDAXVEGQ\\08.3_pp_195_206_Coding_and_design_patterns.pdf:application/pdf},
}

@article{Joshi2014,
	title = {Putting {It} {All} {Together}},
	volume = {134},
	issn = {0032-1052},
	url = {http://content.wkhealth.com/linkback/openurl?sid=WKPTLP:landingpage&an=00006534-201410002-00015},
	doi = {10.1097/PRS.0000000000000677},
	journal = {Plastic and Reconstructive Surgery},
	author = {Joshi, Girish P.},
	year = {2014},
	pmid = {15141531},
	note = {ISBN: 0000000000000},
	pages = {94S--100S},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\STXPQSDM\\08.4_pp_207_219_Putting_it_all_together.pdf:application/pdf},
}

@article{Freyman2017,
	title = {Stochastic character mapping of state-dependent diversification reveals the tempo of evolutionary decline in self-compatible lineages},
	url = {https://www.biorxiv.org/content/early/2018/06/22/210484%0Ahttps://www.biorxiv.org/content/early/2017/10/28/210484},
	doi = {10.1101/210484},
	abstract = {A major goal of evolutionary biology is to identify key evolutionary transitions that correspond with shifts in speciation and extinction rates. Stochastic character mapping has become the primary method used to infer the timing, nature, and number of character state transitions along the branches of a phylogeny. The method is widely employed for standard substitution models of character evolution. However, current approaches cannot be used for models that specifically test the association of character state transitions with shifts in diversification rates such as state-dependent speciation and extinction (SSE) models. Here we introduce a new stochastic character mapping algorithm that overcomes these limitations, and apply it to study mating system evolution over a densely sampled fossil-calibrated phylogeny of the plant family Onagraceae. Utilizing a hidden state SSE model we tested the association of the loss of self-incompatibility with shifts in diversification rates. Confirming long standing theory, we found that self-compatible lineages have higher extinction rates and lower net diversification rates compared to self-incompatible lineages. Further, our mapped character histories show that the loss of self-incompatibility is followed by a short-term spike in speciation rates, which declines after a time lag of several million years resulting in negative net diversification. Lineages that have long been self-compatible such as Fuchsia and Clarkia are in a previously unrecognized and ongoing evolutionary decline. Our results demonstrate that stochastic character mapping of SSE models is a powerful tool for examining the timing and nature of both character state transitions and shifts in diversification rates over the phylogeny.},
	number = {2002},
	journal = {bioRxiv},
	author = {Freyman, William and Hoehna, Sebastian},
	year = {2017},
	pages = {210484},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\JHYVPCGP\\Freyman_Hoehna_2018_bioRxiv_self-compatible.pdf:application/pdf},
}

@incollection{Drummond2014d,
	title = {{BEAST} {XML}},
	isbn = {978-1-139-09511-2},
	booktitle = {Bayesian {Evolutionary} {Analysis} with {BEAST}},
	author = {Drummond, Alexei J. and Rambaut, A.},
	year = {2014},
	pages = {184--194},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\VDWFBQ4Q\\08.2_pp_184_194_BEAST_XML.pdf:application/pdf},
}

@article{Felsenstein1985,
	title = {Phylogenies and the {Comparative} {Method}},
	volume = {125},
	url = {http://www.jstor.org/stable/2461605},
	number = {1},
	journal = {The American Naturalist},
	author = {Felsenstein, Joseph},
	year = {1985},
	pages = {1--15},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\VU7V5KUD\\Felsenstein_1985_AmNat_phylo_comparative_method.pdf:application/pdf},
}

@incollection{drummond_index_2018,
	title = {Index of {Authors}},
	volume = {100},
	isbn = {978-1-139-09511-2},
	booktitle = {International {Journal} of {Radiation} {Oncology}*{Biology}*{Physics}},
	author = {Drummond, Alexei J. and Bouckaert, Remco R.},
	year = {2018},
	doi = {https://doi.org/10.1017/CBO9781139095112},
	note = {Issue: 5},
	pages = {1406--1407},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\LRW57Z8S\\10.0_pp_240_243_Index_of_authors.pdf:application/pdf},
}

@incollection{Drummond2014a,
	title = {Index of subjects},
	isbn = {978-0-511-48727-9},
	booktitle = {Bayesian {Evolutionary} {Analysis} with {BEAST}},
	author = {Drummond, Alexei J. and Rambaut, A.},
	year = {2014},
	doi = {10.1099/00221287-16-33-1-51},
	note = {ISSN: 1350-0872},
	pages = {300--304},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\4JRDSJNV\\11.0_pp_244_249_Index_of_subjects.pdf:application/pdf},
}

@incollection{Drummond2014g,
	title = {Estimating species trees from multilocus data},
	isbn = {978-1-139-09511-2},
	booktitle = {Bayesian {Evolutionary} {Analysis} with {BEAST}},
	author = {Drummond, Alexei J. and Rambaut, A.},
	year = {2014},
	doi = {10.1017/CBO9781139095112.009},
	pages = {116--126},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\HQJRGDNU\\07.3_pp_116_126_Estimating_species_trees_from_multilocus_data.pdf:application/pdf},
}

@article{Yan2012,
	title = {Advanced {Analysis}},
	journal = {Time},
	author = {Yan, Min},
	year = {2012},
	note = {ISBN: 1424404495},
	pages = {6--11},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\NHNVSLVR\\07.4_pp_127_138_Advanced_analysis.pdf:application/pdf},
}

@article{Drummond2001,
	title = {Bayesian evolutionary analysis by sampling trees {THEORY}},
	number = {May 2017},
	author = {Drummond, Alexei J. and Rambaut, Andrew},
	year = {2001},
	note = {ISBN: 9780511819049},
	pages = {564--591},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\CNZWPLR6\\07.1_pp_79_96_Bayesian_evolutionary_analysis_by_sampling_trees.pdf:application/pdf},
}

@article{Williams2008a,
	title = {Part {III}},
	author = {Williams, Phil},
	year = {2008},
	note = {ISBN: 9781139095112},
	pages = {9--10},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\VRPQCMZ8\\08.0_pp_167_168_Programming.pdf:application/pdf},
}

@incollection{Drummond2014,
	title = {Exploring phylogenetic tree space},
	isbn = {978-1-139-09511-2},
	booktitle = {Bayesian {Evolutionary} {Analysis} with {BEAST}},
	author = {Drummond, Alexei J. and Rambaut, A.},
	year = {2014},
	doi = {10.1017/CBO9781139095112.012},
	pages = {154--166},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\467UUDLW\\07.6_pp_154_166_Exploring_phylogenetic_tree_space.pdf:application/pdf},
}

@article{The2003,
	title = {Setting {Up} and {Running} a {Simulation}},
	author = {The, T},
	year = {2003},
	note = {ISBN: 9781139095112},
	pages = {2003--2003},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\32YFTJ3W\\07.2_pp_97_115_Setting_up_and_running_a_phylogenetic_analysis.pdf:application/pdf},
}

@incollection{Drummond2014f,
	title = {Posterior analysis and post-processing},
	isbn = {978-1-139-09511-2},
	booktitle = {Bayesian {Evolutionary} {Analysis} with {BEAST}},
	author = {Drummond, Alexei J. and Rambaut, A.},
	year = {2014},
	doi = {10.1017/CBO9781139095112.011},
	pages = {139--153},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\NAAMM7DM\\07.5_pp_139_153_Posterior_analysis_and_post-processing.pdf:application/pdf},
}

@incollection{Drummond2014h,
	title = {Getting started with {BEAST} 2},
	isbn = {978-1-139-09511-2},
	booktitle = {Bayesian {Evolutionary} {Analysis} with {BEAST}},
	author = {Drummond, Alexei J. and Rambaut, A.},
	year = {2014},
	doi = {10.1017/CBO9781139095112.013},
	pages = {169--183},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\Z6SLT9WF\\08.1_pp_169_183_Getting_started_with_BEAST_2.pdf:application/pdf},
}

@incollection{Drummond2014b,
	title = {Evolutionary trees},
	isbn = {978-1-139-09511-2},
	url = {http://ebooks.cambridge.org/ref/id/CBO9781139095112A016},
	booktitle = {Bayesian {Evolutionary} {Analysis} with {BEAST}},
	author = {Drummond, Alexei J. and Rambaut, A.},
	year = {2014},
	doi = {10.1017/CBO9781139095112.003},
	pages = {21--43},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\DF3YJ4I7\\06.2_pp_21_43_Evolutionary_trees.pdf:application/pdf},
}

@incollection{Drummond2014m,
	title = {Introduction},
	isbn = {978-1-139-09511-2},
	booktitle = {Bayesian {Evolutionary} {Analysis} with {BEAST}},
	author = {Drummond, Alexei J. and Rambaut, Andrew and Suchard, Marc},
	year = {2014},
	pages = {3--20},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\BFGQ9YBS\\06.1_pp_3_20_Introduction.pdf:application/pdf},
}

@incollection{Centre2017,
	title = {Part {I} - {Theory}},
	isbn = {978-1-139-09511-2},
	booktitle = {Bayesian {Evolutionary} {Analysis} with {BEAST}},
	author = {Drummond, A and Bouckaert, Remco R.},
	year = {2015},
	pages = {9--10},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ZQSZNYXH\\06.0_pp_1_2_Theory.pdf:application/pdf},
}

@incollection{Drummond2014e,
	title = {Summary of most significant capabilities of {BEAST} 2},
	isbn = {978-1-139-09511-2},
	booktitle = {Bayesian {Evolutionary} {Analysis} with {BEAST}},
	author = {Drummond, Alexei J. and Rambaut, A.},
	year = {2014},
	pages = {128--129},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\EYAUGPXP\\05.0_pp_xi_xii_Summary_of_most_significant_capabilities_of_BEAST_2.pdf:application/pdf},
}

@article{Publications2018,
	title = {Part {Ii}},
	author = {Publications, Sage and Central, Proquest Ebook},
	year = {2018},
	note = {ISBN: 9781139095112},
	pages = {9--10},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\MC7W3XKW\\07.0_pp_77_78_Practice.pdf:application/pdf},
}

@incollection{Drummond2014i,
	title = {Structured trees and phylogeography},
	isbn = {978-1-139-09511-2},
	booktitle = {Bayesian {Evolutionary} {Analysis} with {BEAST}},
	author = {Drummond, Alexei J. and Rambaut, A.},
	year = {2014},
	doi = {10.1017/CBO9781139095112.006},
	pages = {68--76},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ZWMTIM4X\\06.5_pp_68_76_Structured_trees_and_phylogeography.pdf:application/pdf},
}

@article{Rogers2014,
	title = {The {Molecular} {Clock}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780080959757012067},
	doi = {10.1016/B978-0-08-095975-7.01206-7},
	journal = {Treatise on Geochemistry},
	author = {Rogers, A.R.},
	year = {2014},
	note = {ISBN: 9780080959757},
	pages = {55--61},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\Q7ZJXZNP\\06.4_pp_58_67_The_molecular_clock.pdf:application/pdf},
}

@incollection{Drummond2014c,
	title = {Chapter 3 {Substitutions} and site models},
	isbn = {978-1-139-09511-2},
	booktitle = {Bayesian {Evolutionary} {Analysis} with {BEAST}},
	author = {Drummond, Alexei J. and Rambaut, A.},
	year = {2014},
	note = {Issue: 3},
	pages = {2008},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\FX9QHSAV\\06.3_pp_44_57_Substitution_and_site_models.pdf:application/pdf},
}

@article{Das2015,
	title = {A brief note on estimates of binomial coefficients},
	author = {Das, Shagnik},
	year = {2015},
	pages = {7},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\SZ4QNQGC\\m-api-32a9c96c-1838-ac3f-2900-987044bab8e1.pdf:application/pdf},
}

@misc{Drummond2017,
	title = {Overview of lectures today},
	author = {Drummond, Alexei J.},
	year = {2017},
	note = {Pages: 1-23
Publication Title: BIOINF 702 Lecture},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\INPQVL9Z\\7-8 Phylogenetics-Lecture3.pdf:application/pdf},
}

@article{Grassly1997,
	title = {Phylogenetic {Trees}},
	volume = {13},
	number = {5},
	author = {Grassly, Nicholas C and Adachi, Jun and Rambaut, Andrew},
	year = {1997},
	pages = {559--560},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\7MN55QBL\\4 treeslides.pdf:application/pdf},
}

@incollection{Drummond2014l,
	title = {Acknowledgements},
	isbn = {978-1-139-09511-2},
	booktitle = {Bayesian {Evolutionary} {Analysis} with {BEAST}},
	author = {Drummond, Alexei J. and Rambaut, Andrew},
	year = {2014},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\N3A4KIC2\\04.0_pp_x_x_Acknowledgements.pdf:application/pdf},
}

@incollection{Drummond2014j,
	title = {Contents},
	isbn = {0-521-80183-4},
	booktitle = {Bayesian {Evolutionary} {Analysis} with {BEAST}},
	author = {Drummond, Alexei J. and Rambaut, Andrew},
	year = {2014},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\V7HNSVQ2\\02.0_pp_v_viii_Contents.pdf:application/pdf},
}

@article{Drummond2018,
	title = {Developing {Darwin} ’ s computer {Modeling} evolution from the fossil record to infectious disease},
	number = {March},
	author = {Drummond, Alexei J.},
	year = {2018},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\WJERWF4Q\\5-6 BayesianPhylogenetics.pdf:application/pdf},
}

@article{Drummond2013,
	title = {Bayesian evolutionary analysis with {BEAST} 2.0},
	url = {papers3://publication/uuid/3823A21D-9EE4-446A-BF3C-436D972825DB},
	number = {Mcmc},
	author = {Drummond, Alexei J. and Bouckaert, Remco R.},
	year = {2013},
	note = {ISBN: 9781107019652},
	keywords = {Bayesian statistics, Tutorials},
	pages = {1--242},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\59HEL8RW\\01.0_pp_i_iv_Frontmatter.pdf:application/pdf},
}

@incollection{drummond_preface_2015,
	title = {Preface},
	isbn = {978-1-139-09511-2},
	booktitle = {Bayesian {Evolutionary} {Analysis} with {BEAST}},
	author = {Drummond, Alexei J. and Bouckaert, Remco R.},
	editor = {Drummond, Alexei J. and Bouckaert, Remco R.},
	year = {2015},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\DSY4HUHR\\03.0_pp_ix_ix_Preface.pdf:application/pdf},
}

@article{Waldorp2013,
	title = {Assignment 4},
	author = {Waldorp, Lourens},
	year = {2013},
	pages = {1--4},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\MVQADLPV\\BioInf702 Assignment 4 jbuc045.pdf:application/pdf},
}

@misc{Ogawa,
	title = {Hidden {Markov} {Models}},
	author = {Ogawa, Tetsuji},
	note = {Pages: 1-18},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\HKMI4CX3\\m-api-7fd3f91d-e3f2-203b-2220-60be4c8ac81d.pdf:application/pdf},
}

@article{Drummond2018a,
	title = {{BIOINF} 702 {Assignment} 2},
	number = {April},
	author = {Drummond, Alexei J.},
	year = {2018},
	pages = {15--17},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\KNSL67ZM\\A2-beastLabYFV.pdf:application/pdf},
}

@article{Stirling2018,
	title = {Bioinf 702, 2018},
	number = {March},
	author = {Stirling, Use},
	year = {2018},
	pages = {2--3},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\P4I87YRT\\assignment1.pdf:application/pdf},
}

@article{Ze,
	title = {Multiple {Sequence} {Alignment} ({MSA})},
	author = {Ze, Kxkyize Q K Y},
	note = {ISBN: 4444444444444},
	pages = {4--5},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\LNL3AP2K\\2 multiple-alignment.pdf:application/pdf},
}

@article{Heled2010,
	title = {Bayesian {Inference} of {Species} {Trees} from {Multilocus} {Data}},
	volume = {27},
	issn = {07374038},
	doi = {10.1093/molbev/msp274},
	abstract = {Until recently, it has been common practice for a phylogenetic analysis to use a single gene sequence from a single individual organism as a proxy for an entire species. With technological advances, it is now becoming more common to collect data sets containing multiple gene loci and multiple individuals per species. These data sets often reveal the need to directly model intraspecies polymorphism and incomplete lineage sorting in phylogenetic estimation procedures. For a single species, coalescent theory is widely used in contemporary population genetics to model intraspecific gene trees. Here, we present a Bayesian Markov chain Monte Carlo method for the multispecies coalescent. Our method coestimates multiple gene trees embedded in a shared species tree along with the effective population size of both extant and ancestral species. The inference is made possible by multilocus data from multiple individuals per species. Using a multiindividual data set and a series of simulations of rapid species radiations, we demonstrate the efficacy of our new method. These simulations give some insight into the behavior of the method as a function of sampled individuals, sampled loci, and sequence length. Finally, we compare our new method to both an existing method (BEST 2.2) with similar goals and the supermatrix (concatenation) method. We demonstrate that both BEST and our method have much better estimation accuracy for species tree topology than concatenation, and our method outperforms BEST in divergence time and population size estimation.},
	number = {3},
	journal = {Molecular Biology and Evolution},
	author = {Heled, Joseph and Drummond, Alexei J.},
	year = {2010},
	pmid = {19906793},
	note = {ISBN: 0737-4038},
	keywords = {Bayesian inference, Censored coalescent, Gene trees, Molecular systematics, Multispecies coalescent, Species trees},
	pages = {570--580},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\IZ2EQ9LM\\msp274.pdf:application/pdf},
}

@article{Drummond2005,
	title = {Bayesian coalescent inference of past population dynamics from molecular sequences},
	volume = {22},
	issn = {07374038},
	doi = {10.1093/molbev/msi103},
	abstract = {We introduce the Bayesian skyline plot, a new method for estimating past population dynamics through time from a sample of molecular sequences without dependence on a prespecified parametric model of demographic history. We describe a Markov chain Monte Carlo sampling procedure that efficiently samples a variant of the generalized skyline plot, given sequence data, and combines these plots to generate a posterior distribution of effective population size through time. We apply the Bayesian skyline plot to simulated data sets and show that it correctly reconstructs demographic history under canonical scenarios. Finally, we compare the Bayesian skyline plot model to previous coalescent approaches by analyzing two real data sets (hepatitis C virus in Egypt and mitochondrial DNA of Beringian bison) that have been previously investigated using alternative coalescent methods. In the bison analysis, we detect a severe but previously unrecognized bottleneck, estimated to have occurred 10,000 radiocarbon years ago, which coincides with both the earliest undisputed record of large numbers of humans in Alaska and the megafaunal extinctions in North America at the beginning of the Holocene.},
	number = {5},
	journal = {Molecular Biology and Evolution},
	author = {Drummond, Alexei J. and Rambaut, A. and Shapiro, B. and Pybus, O. G.},
	year = {2005},
	pmid = {15703244},
	note = {ISBN: 0737-4038},
	keywords = {Coalescent inference, Demographic model selection, Holocene, Markov chain Monte Carlo, Megafaunal extinctions, Skyline plot},
	pages = {1185--1192},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\KT5ULI5F\\msi103.pdf:application/pdf},
}

@article{Vaughan2014,
	title = {Efficient {Bayesian} inference under the structured coalescent},
	volume = {30},
	issn = {14602059},
	doi = {10.1093/bioinformatics/btu201},
	abstract = {MOTIVATION: Population structure significantly affects evolutionary dynamics. Such structure may be due to spatial segregation, but may also reflect any other gene-flow-limiting aspect of a model. In combination with the structured coalescent, this fact can be used to inform phylogenetic tree reconstruction, as well as to infer parameters such as migration rates and subpopulation sizes from annotated sequence data. However, conducting Bayesian inference under the structured coalescent is impeded by the difficulty of constructing Markov Chain Monte Carlo (MCMC) sampling algorithms (samplers) capable of efficiently exploring the state space.{\textbackslash}n{\textbackslash}nRESULTS: In this article, we present a new MCMC sampler capable of sampling from posterior distributions over structured trees: timed phylogenetic trees in which lineages are associated with the distinct subpopulation in which they lie. The sampler includes a set of MCMC proposal functions that offer significant mixing improvements over a previously published method. Furthermore, its implementation as a BEAST 2 package ensures maximum flexibility with respect to model and prior specification. We demonstrate the usefulness of this new sampler by using it to infer migration rates and effective population sizes of H3N2 influenza between New Zealand, New York and Hong Kong from publicly available hemagglutinin (HA) gene sequences under the structured coalescent.{\textbackslash}n{\textbackslash}nAVAILABILITY AND IMPLEMENTATION: The sampler has been implemented as a publicly available BEAST 2 package that is distributed under version 3 of the GNU General Public License at http://compevol.github.io/MultiTypeTree.},
	number = {16},
	journal = {Bioinformatics},
	author = {Vaughan, Timothy G. and Kühnert, Denise and Popinga, Alex and Welch, David and Drummond, Alexei J.},
	year = {2014},
	pmid = {24753484},
	note = {ISBN: 1367-4811 (Electronic){\textbackslash}r1367-4803 (Linking)},
	pages = {2272--2279},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ELGTDP5E\\btu201.pdf:application/pdf},
}

@article{Bouckaert2017,
	title = {{bModelTest}: {Bayesian} phylogenetic site model averaging and model comparison},
	volume = {17},
	issn = {14712148},
	doi = {10.1186/s12862-017-0890-6},
	abstract = {Reconstructing phylogenies through Bayesian methods has many benefits, which include providing a mathematically sound framework, providing realistic estimates of uncertainty and being able to incorporate different sources of information based on formal principles. Bayesian phylogenetic analyses are popular for interpreting nucleotide sequence data, however for such studies one needs to specify a site model and associated substitution model. Often, the parameters of the site model is of no interest and an ad-hoc or additional likelihood based analysis is used to select a single site model.},
	number = {1},
	journal = {BMC Evolutionary Biology},
	author = {Bouckaert, Remco R. and Drummond, Alexei J.},
	year = {2017},
	pmid = {28166715},
	note = {Publisher: BMC Evolutionary Biology
ISBN: 1471-2148},
	keywords = {Model averaging, Model selection, Model comparison, ModelTest, Phylogenetic model averaging, Phylogenetic model comparison, Site model, Statistical phylogenetics, Substitution model},
	pages = {1--11},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ZXKCBZZZ\\Bouckaert and Drummond.pdf:application/pdf},
}

@article{Slatkin1989a,
	title = {A cladistic measure of gene flow inferred from the phylogenies of alleles},
	volume = {123},
	issn = {00166731},
	doi = {D - NLM: PMC1203833 EDAT- 1989/11/01 MHDA- 1989/11/01 00:01 CRDT- 1989/11/01 00:00 PST - ppublish},
	abstract = {A method for estimating the average level of gene flow among populations is introduced. The method provides an estimate of Nm, where N is the size of each local population in an island model and m is the migration rate. This method depends on knowing the phylogeny of the nonrecombining segments of DNA that are sampled. Given the phylogeny, the geographic location from which each sample is drawn is treated as multistate character with one state for each geographic location. A parsimony criterion applied to the evolution of this character on the phylogeny provides the minimum number of migration events consistent with the phylogeny. Extensive simulations show that the distribution of this minimum number is a simple function of Nm. Assuming the phylogeny is accurately estimated, this method provides an estimate of Nm that is as nearly as accurate as estimates obtained using FST and other statistics when Nm is moderate. Two examples of the use of this method with mitochondrial DNA data are presented.},
	number = {3},
	journal = {Genetics},
	author = {Slatkin, M. and Maddison, W. P.},
	year = {1989},
	pmid = {2599370},
	note = {ISBN: 0016-6731 (Print){\textbackslash}r0016-6731 (Linking)},
	pages = {603--613},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\TSGG8FKJ\\Slatkin&Maddison1989.pdf:application/pdf},
}

@article{Pearse2004,
	title = {Beyond {F} {ST} : {Analysis} of population genetic data for conservation},
	volume = {5},
	issn = {1566-0621},
	doi = {10.1007/s10592-003-1863-4},
	abstract = {Both the ability to generate DNA data and the variety of analytical methods for conservation genetics are expanding at an ever-increasing pace. Analytical approaches are now possible that were unthinkable even five years ago due to limitations in computational power or the availability of DNA data, and this has vastly expanded the accuracy and types of information that may be gained from population genetic data. Here we provide a guide to recently developed methods for population genetic analysis, including identi- fication of population structure, quantification of gene flow, and inference of demographic history. We cover both allele-frequency and sequence-based approaches, with a special focus on methods relevant to conservation genetic applications. Although classical population genetic approaches such as FST (and its derivatives) have carried the field thus far, newer, more powerful, methods can infer much more from the data, rely on fewer assumptions, and are appropriate for conservation genetic management when precise estimates are needed. Background},
	journal = {Conservation Genetics},
	author = {Pearse, Devon E and Crandall, Keith a},
	year = {2004},
	pmid = {141},
	note = {ISBN: 1566-0621},
	pages = {585--602},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\WPNPTI4K\\Pearse&Crandall2004.pdf:application/pdf},
}

@article{Lyrholm1999,
	title = {Sex − biased dispersal in sperm whales : contrasting mitochondrial and nuclear genetic structure of global populations {Sex}-biased dispersal in sperm whales : contrasting mitochondrial and nuclear genetic structure of global populations},
	doi = {10.1098/rspb.1999.0644},
	number = {October 1998},
	author = {Lyrholm, Thomas and Leimar, Olof and Johanneson, Bo and Gyllensten, Ulf and Lyrholm, Thomas and Leimar, Olof and Johanneson, Bo and Gyllensten, Ulf},
	year = {1999},
	keywords = {1991, 1996 a, associating, best 1979, containing aproxi-, dispersal, gyllensten 1998, in apparently matrilineally related, lyrholm, microsatellites, mtdna, population structure, richard et al, social bonds, social organization, sperm whale, units, whitehead et al},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\TQD7YF7W\\Lyrholmetal1999.pdf:application/pdf},
}

@misc{Laverya,
	title = {Shane’s {Simple} {Guide} to {F}-statistics},
	abstract = {Instruction of Calculating Fst},
	author = {Lavery, Shane},
	keywords = {doc -, g, stats2},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\NSQYXUGB\\Lavery - Pop. g. stats2.pdf:application/pdf},
}

@article{Slatkin1989,
	title = {Detecting small amounts of gene flow from phylogenies of alleles},
	volume = {121},
	issn = {00166731},
	abstract = {The method of coalescents is used to find the probability that none of the ancestors of alleles sampled from a population are immigrants. If that is the case for samples from two or more populations, then there would be concordance between the phylogenies of those alleles and the geographic locations from which they are drawn. This type of concordance has been found in several studies of mitochondrial DNA from natural populations. It is shown that if the number of sequences sampled from each population is reasonably large (10 or more), then this type of concordance suggests that the average number of individuals migrating between populations is likely to be relatively small (Nm less than 1) but the possibility of occasional migrants cannot be excluded. The method is applied to the data of E. Bermingham and J. C. Avise on mtDNA from the bowfin, Amia calva.},
	number = {3},
	journal = {Genetics},
	author = {Slatkin, M.},
	year = {1989},
	pmid = {2714639},
	note = {ISBN: 0016-6731 (Print){\textbackslash}r0016-6731 (Linking)},
	pages = {609--612},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\LW5LZ4HH\\Slatkin 1988.pdf:application/pdf},
}

@article{Platt2018,
	title = {American {Association} for the {Advancement} of {Science}},
	volume = {326},
	number = {5952},
	journal = {Science},
	author = {Platt, John R},
	year = {2018},
	pages = {535--538},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\68C8QEE7\\O'Brian et al 1985.pdf:application/pdf},
}

@article{Jamieson2006,
	title = {Inbreeding and endangered species management: is {New} {Zealand} out of step with the rest of the world?},
	volume = {20},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1523-1739.2005.00282.x/full},
	doi = {10.1111/j.1523-1739.2006.00282.x},
	number = {1},
	journal = {Conservation Biology},
	author = {Jamieson, Ig and Wallis, Gp and Briskie, Jv},
	year = {2006},
	keywords = {black robin, genetic purging, kakapo, takahe},
	pages = {38--47},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\PR4JKKI9\\JAMIESON_et_al-2006-Conservation_Biology.pdf:application/pdf},
}

@article{Bromham2003,
	title = {The modern molecular clock},
	volume = {4},
	issn = {14710056},
	doi = {10.1038/nrg1020},
	abstract = {The discovery of the molecular clock--a relatively constant rate of molecular evolution--provided an insight into the mechanisms of molecular evolution, and created one of the most useful new tools in biology. The unexpected constancy of rate was explained by assuming that most changes to genes are effectively neutral. Theory predicts several sources of variation in the rate of molecular evolution. However, even an approximate clock allows time estimates of events in evolutionary history, which provides a method for testing a wide range of biological hypotheses ranging from the origins of the animal kingdom to the emergence of new viral epidemics.},
	number = {3},
	journal = {Nature Reviews Genetics},
	author = {Bromham, Lindell and Penny, David},
	year = {2003},
	pmid = {12610526},
	note = {ISBN: 1471-0056},
	pages = {216--224},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\KH499AX5\\The_modern_molecular_clock.pdf:application/pdf},
}

@article{Hey2003,
	title = {The study of structured populations - {New} hope for a difficult and divided science},
	volume = {4},
	issn = {14710056},
	doi = {10.1038/nrg1112},
	abstract = {Natural populations, including those of humans, have complex geographies and histories. Studying how they evolve is difficult, but it is possible with population-based DNA sequence data. However, the study of structured populations is divided by two distinct schools of thought and analysis. The phylogeographic approach is fundamentally graphical and begins with a gene-tree estimate. By contrast, the more traditional approach of using summary statistics is fundamentally mathematical. Both approaches have limitations, but there is promise in newer probabilistic methods that offer the flexibility and data exploitation of the phylogeographic approach in an explicitly model-based mathematical framework.},
	number = {7},
	journal = {Nature Reviews Genetics},
	author = {Hey, Jody and Machado, Carlos A.},
	year = {2003},
	pmid = {12838345},
	note = {ISBN: 1471-0056 (Print){\textbackslash}n1471-0056 (Linking)},
	pages = {535--543},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\TYHK7LG7\\Hey et al 2003.pdf:application/pdf},
}

@article{Baker1993,
	title = {Abundant {Mitochondrial} {DNA} {Variation} and {World}-{Wide} {Population} {Structure} in {Humpback} {Whales} {Palumbi} {Proceedings} of the {National} {Academy} of {Sciences} of the {United} {States} of {America} , {Vol} . 90 , {No} .},
	author = {Baker, C S and Perry, A and Bannister, J L and Weinrich, M T and Abernethy, R B and Calambokidis, J and Lien, J and Lambertsen, R H and Ramirez, J Urban and Vasquez, O and Clapham, P J and Alling, A and Brien, S J O},
	year = {1993},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\2YKPZQVP\\Baker et al1993 .pdf:application/pdf},
}

@article{Tallmon2004,
	title = {The alluring simplicity and complex reality of genetic rescue},
	volume = {19},
	issn = {01695347},
	doi = {10.1016/j.tree.2004.07.003},
	abstract = {A series of important new theoretical, experimental and observational studies demonstrate that just a few immigrants can have positive immediate impacts on the evolutionary trajectory of local populations. In many cases, a low level of immigration into small populations has produced fitness benefits that are greater than those predicted by theoretical models, resulting in what has been termed 'genetic rescue'. However, the opposite result (reduced fitness) can also be associated with immigration of genetically divergent individuals. Central to our understanding of genetic rescue are complex interactions among fundamental concepts in evolutionary and population biology, including both genetic and non-genetic (environmental, behavioral and demographic) factors. Developing testable models to predict when genetic rescue is likely to occur is a daunting challenge that will require carefully controlled, multi-generation experiments as well as creative use of information from natural 'experiments'.},
	number = {9},
	journal = {Trends in Ecology and Evolution},
	author = {Tallmon, David A. and Luikart, Gordon and Waples, Robin S.},
	year = {2004},
	pmid = {16701312},
	note = {ISBN: 0169-5347 (Print){\textbackslash}n0169-5347 (Linking)},
	pages = {489--496},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\UZ27RBSG\\Tallmon et al 2004.pdf:application/pdf},
}

@incollection{Avise,
	title = {Chapter 4 {Molecular} {Clock}},
	booktitle = {Philosophies and {Methods} of {Molecular} {Data} {Analysis}},
	author = {Avise, J. C.},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\D8BLM933\\Avise04_Ch4.Molecular Clock.pdf:application/pdf},
}

@article{Osborne2016,
	title = {Genetic evidence of a population bottleneck and inbreeding in the endangered {New} {Zealand} sea lion {Phocarctos} hookeri},
	volume = {107},
	issn = {14657333},
	doi = {10.1093/jhered/esw015},
	abstract = {The New Zealand sea lion (NZSL) is of high conservation concern due to its limited distribution and its declining population size. Historically, it occupied most of coastal New Zealand, but is now restricted to a few coastal sites in southern mainland New Zealand and the sub-Antarctic Islands. NZSLs have experienced a recent reduction in population size due to sealing in the 1900s, which is expected to have resulted in increased inbreeding and a loss of genetic variation, potentially reducing the evolutionary capacity of the species and negatively impacting on its long-term prospects for survival. We used 17 microsatellite loci, previously shown to have cross-species applications in pinnipeds, to determine locus- and population-specific statistics for 1205 NZSLs from 7 consecutive breeding seasons. We show that the NZSL population has a moderate level of genetic diversity in comparison to other pinnipeds. We provide genetic evidence for a population reduction, likely caused by historical sealing, and a measure of allele sharing/parental relatedness (internal relatedness) that is suggestive of increased inbreeding in pups that died during recent epizootic episodes. We hypothesize that population bottlenecks and nonrandom mating have impacted on the population genetic architecture of NZSLs, affecting its population recovery.},
	number = {5},
	journal = {Journal of Heredity},
	author = {Osborne, Amy J. and Negro, Sandra S. and Chilvers, B. Louise and Robertson, Bruce C. and Kennedy, Martin A. and Gemmell, Neil J.},
	year = {2016},
	pmid = {26995741},
	note = {ISBN: 3037244941},
	keywords = {Bottleneck, Diversity, Microsatellites, Structure},
	pages = {392--402},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\EI3RBRQP\\Osbourne et al 2016.pdf:application/pdf},
}

@incollection{Avise2004,
	title = {Chapter 2 {THe} {History} of {Interest} in {Genetic} {Variation}},
	booktitle = {Molecular {Markers}, {Natural} {History} and {Evolution}.},
	publisher = {Sinauer},
	author = {Avise, J. C.},
	year = {2004},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ILC89UMG\\Avise04_Ch2.NeutralEvolution.pdf:application/pdf},
}

@incollection{rodrigo_coalescent:_2009,
	title = {The coalescent: population genetic inference using genealogies},
	booktitle = {The {Phylogenetic} {Handbook}: {A} {Practical} {Approach} to {Phylogenetic} {Analysis} and {Hypothesis} {Testing}},
	publisher = {Cambridge University Press},
	author = {Rodrigo, Allen},
	editor = {Lemey, Phillipe and Salemi, Marco and Vandamme, Anne-Mieke},
	year = {2009},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\DIZAP2FE\\Rodrigo 2009 _ The Phylogenetic Handbook.pdf:application/pdf},
}

@article{Kimura1991,
	title = {The neutral theory of molecular evolution: a review of recent evidence.},
	volume = {66},
	issn = {0021-504X},
	doi = {10.1266/jjg.66.367},
	abstract = {In sharp contrast to the Darwinian theory of evolution by natural selection, the neutral theory claims that the overwhelming majority of evolutionary changes at the molecular level are caused by random fixation (due to random sampling drift in finite populations) of selectively neutral (i.e., selectively equivalent) mutants under continued inputs of mutations. The theory also asserts that most of the genetic variability within species at the molecular level (such as protein and DNA polymorphism) are selectively neutral or very nearly neutral and that they are maintained in the species by the balance between mutational input and random extinction. The neutral theory is based on simple assumptions, enabling us to develop mathematical theories based on population genetics to treat molecular evolution and variation in quantitative terms. The theory can be tested against actual observations. Neo-Darwinians continue to criticize the neutral theory, but evidence for it has accumulated over the last two decades. The recent outpouring of DNA sequence data has greatly strengthened the theory. In this paper, I review some recent observations that strongly support the neutral theory. They include such topics as pseudoglobin genes of the mouse, alpha A-crystallin genes of the blind mole rat, genes of influenza A virus and nuclear vs. mitochondrial genes of fruit flies. I also discuss such topics as the evolution of deviant coding systems in Mycoplasma, the origin of life and the unified understanding of molecular and phenotypic evolution. I conclude that since the origin of life on Earth, neutral evolutionary changes have predominated over Darwinian evolutionary changes, at least in number.},
	number = {4},
	journal = {Japanese Journal of Genetics},
	author = {Kimura, M},
	year = {1991},
	pmid = {1954033},
	note = {ISBN: 0021-504X (Print){\textbackslash}r0021-504X (Linking)},
	pages = {367--386},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\DAJH342E\\Kimura 1991.pdf:application/pdf},
}

@article{Kimura1968,
	title = {Evolutionary rate at the molecular level.},
	volume = {217},
	issn = {00280836},
	doi = {10.1038/217624a0},
	abstract = {Calculating the rate of evolution in terms of nucleotide substitutions seems to give a value so high that many of the mutations involved must be neutral ones.},
	number = {5129},
	journal = {Nature},
	author = {Kimura, M},
	year = {1968},
	pmid = {5637732},
	note = {arXiv: NIHMS150003
ISBN: 0028-0836 (Print){\textbackslash}n0028-0836 (Linking)},
	pages = {624--626},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\RH6Y2W5X\\Kimura 1968.pdf:application/pdf},
}

@article{Olsen2018,
	title = {American {Association} for the {Advancement} of {Science}},
	volume = {312},
	number = {5772},
	author = {Olsen, Björn and Munster, Vincent J and Wallensten, Anders and Waldenström, Jonas and Osterhaus, Albert D M E and Fouchier, Ron A M},
	year = {2018},
	pages = {384--388},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\DTMJ9W32\\Bazin et al 2006.pdf:application/pdf},
}

@article{Patterson2011,
	title = {A multilocus phylogeny of the {Sulidae} ({Aves}: {Pelecaniformes})},
	volume = {58},
	issn = {10557903},
	url = {http://dx.doi.org/10.1016/j.ympev.2010.11.021},
	doi = {10.1016/j.ympev.2010.11.021},
	abstract = {Gene trees will often differ from the true species history, the species tree, as a result of processes such as incomplete lineage sorting. New methods such as Bayesian Estimation of the Species Tree (BEST) use the multispecies coalescent to model lineage sorting, and directly infer the species tree from multilocus DNA sequence data. The Sulidae (Aves: Pelecaniformes) is a family of ten booby and gannet species with a global distribution. We sequenced five nuclear intron loci and one mitochondrial locus to estimate a species tree for the Sulidae using both BEST and by concatenating nuclear loci. We also used fossil calibrated strict and relaxed molecular clocks in BEAST to estimate divergence times for major nodes in the sulid phylogeny. Individual gene trees showed little phylogenetic conflict but varied in resolution. With the exception of the mitochondrial gene tree, no gene tree was completely resolved. On the other hand, both the BEST and concatenated species trees were highly resolved, strongly supported, and topologically consistent with each other. The three sulid genera (Morus, Sula, Papasula) were monophyletic and the relationships within genera were mostly consistent with both a previously estimated mtDNA gene tree and the mtDNA gene tree estimated here. However, our species trees conflicted with the mtDNA gene trees in the relationships among the three genera. Most notably, we find that the endemic and endangered Abbott's booby (Papasula abbotti) is likely basal to all other members of the Sulidae and diverged from them approximately 22. million years ago. © 2010 Elsevier Inc.},
	number = {2},
	journal = {Molecular Phylogenetics and Evolution},
	author = {Patterson, S. A. and Morris-Pocock, J. A. and Friesen, V. L.},
	year = {2011},
	pmid = {21144905},
	note = {Publisher: Elsevier Inc.
ISBN: 1055-7903},
	keywords = {Booby, Gannet, Gene tree, Intron, Species tree, Sulidae},
	pages = {181--191},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\E8SABE4U\\Patterson2011.MolPhylEvol.pdf:application/pdf},
}

@article{Fujita2012,
	title = {Coalescent-based species delimitation in an integrative taxonomy},
	volume = {27},
	issn = {01695347},
	doi = {10.1016/j.tree.2012.04.012},
	abstract = {The statistical rigor of species delimitation has increased dramatically over the past decade. Coalescent theory provides powerful models for population genetic inference, and is now increasingly important in phylogenetics and speciation research. By applying probabilistic models, coalescent-based species delimitation provides clear and objective testing of alternative hypotheses of evolutionary independence. As acquisition of multilocus data becomes increasingly automated, coalescent-based species delimitation will improve the discovery, resolution, consistency, and stability of the taxonomy of species. Along with other tools and data types, coalescent-based species delimitation will play an important role in an integrative taxonomy that emphasizes the identification of species limits and the processes that have promoted lineage diversification. © 2012 Elsevier Ltd.},
	number = {9},
	journal = {Trends in Ecology and Evolution},
	author = {Fujita, Matthew K. and Leaché, Adam D. and Burbrink, Frank T. and McGuire, Jimmy A. and Moritz, Craig},
	year = {2012},
	pmid = {22633974},
	note = {ISBN: 0169-5347},
	pages = {480--488},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\NTJ3KYQY\\fujita2012.Cell.pdf:application/pdf},
}

@article{Edwards2009a,
	title = {Is a new and general theory of molecular systematics emerging?},
	volume = {63},
	issn = {00143820},
	doi = {10.1111/j.1558-5646.2008.00549.x},
	abstract = {The advent and maturation of algorithms for estimating species trees—phylogenetic trees that allow gene tree heterogeneity and whose tips represent lineages, populations and species, as opposed to genes—represent an exciting confluence of phylogenetics, phylogeography, and population genetics, and ushers in a new generation of concepts and challenges for the molecular systematist. In this essay I argue that to better deal with the large multilocus datasets brought on by phylogenomics, and to better align the fields of phylogeography and phylogenetics, we should embrace the primacy of species trees, not only as a new and useful practical tool for systematics, but also as a long-standing conceptual goal of systematics that, largely due to the lack of appropriate computational tools, has been eclipsed in the past few decades. I suggest that phylogenies as gene trees are a “local optimum” for systematics, and review recent advances that will bring us to the broader optimum inherent in species trees. In addition to adopting new methods of phylogenetic analysis (and ideally reserving the term “phylogeny” for species trees rather than gene trees), the new paradigm suggests shifts in a number of practices, such as sampling data to maximize not only the number of accumulated sites but also the number of independently segregating genes; routinely using coalescent or other models in computer simulations to allow gene tree heterogeneity; and understanding better the role of concatenation in influencing topologies and confidence in phylogenies. By building on the foundation laid by concepts of gene trees and coalescent theory, and by taking cues from recent trends in multilocus phylogeography, molecular systematics stands to be enriched. Many of the challenges and lessons learned for estimating gene trees will carry over to the challenge of estimating species trees, although adopting the species tree paradigm will clarify many issues (such as the nature of polytomies and the star tree paradox), raise conceptually new challenges, or provide new answers to old questions. [ABSTRACT FROM AUTHOR] Copyright of Evolution is the property of Blackwell Publishing Limited and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
	number = {1},
	journal = {Evolution},
	author = {Edwards, Scott V.},
	year = {2009},
	pmid = {19146594},
	note = {ISBN: 00143820},
	keywords = {Macroevolution, Fossil, Genome, Neanderthal, Phylogeography, Polytomy},
	pages = {1--19},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\RMM67ZYH\\Edwards2008.Evol.pdf:application/pdf},
}

@article{Ayala1986,
	title = {On the virtues and pitfalls of the molecular evolutionary clock},
	volume = {77},
	issn = {00221503},
	doi = {10.1093/oxfordjournals.jhered.a110227},
	abstract = {"Informational" macromolecules--i.e., proteins and nucleic acids--have in their sequences a register of evolutionary history. Zuckerkandl and Pauling suggested in 1965 that these molecules might provide a "molecular clock" of evolution. The molecular clock would time evolutionary events and make it possible to reconstruct phylogenetic history--the branching relationships among lineages leading to modern species. Kimura's neutrality theory postulates that rates of molecular evolution are stochastically constant and, hence, that there is a molecular clock. A variety of tests have shown that molecular evolution does not behave like a stochastic clock. The variance in evolutionary rates is much too large and thus inconsistent with the neutrality theory. This, however, does not invalidate the clock, but rather leaves it without a theoretical foundation to anticipate its properties. Sequence comparisons show that molecular evolution is sufficiently regular to serve in many situations as a clock, but uncertainty concerning the properties of the clock (for example, about the circumstances that may yield large oscillations in substitution rates from time to time or from lineage to lineage) demands that it be used with caution. Few DNA or protein sequences are known from organisms that range from closely related, e.g., different mammals, to very remote, e.g., mammals and fungi. One example is cytochrome c, which has an acceptable clockwise behavior over the whole span, in spite of some irregularities. Another example is the copper-zinc superoxide dismutase (SOD), which behaves like a very erratic clock. The SOD average rate of amino acid substitution per 100 residues per 100 million years (MY) is 5.5 when fungi and animals are compared, 9.1 when comparisons are made between insects and mammals, and 27.8 when mammals are compared with each other. The question is which mode is more common over broad evolutionary spans: the regularity of cytochrome c or the capriciousness of SOD? Additional data sets will be required in order to obtain the answer and to develop expectations about the accuracy of the clock in particular instances. Until such data exist, conclusions solely based on the molecular clock are potentially fraught with error.},
	number = {4},
	journal = {Journal of Heredity},
	author = {Ayala, Francisco J.},
	year = {1986},
	pmid = {3020121},
	note = {ISBN: 0022-1503},
	pages = {226--235},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\YICEPHN7\\Ayala 1985.pdf:application/pdf},
}

@article{Pybus2006,
	title = {Model selection and the molecular clock},
	volume = {4},
	issn = {15457885},
	doi = {10.1371/journal.pbio.0040151},
	abstract = {A brief overview of the methods used to determine phylogenetic distances sets the stage for understanding new research published in PLoS Biology.},
	number = {5},
	journal = {PLoS Biology},
	author = {Pybus, Oliver G.},
	year = {2006},
	pmid = {16683863},
	note = {ISBN: 1545-7885 (Electronic)},
	pages = {686--688},
	file = {Pybus 2006.PDF:C\:\\Users\\jstacey\\Zotero\\storage\\NGY6CQV7\\Pybus 2006.PDF:application/pdf},
}

@article{Cohen2004,
	title = {Bioinformatics---an introduction for computer scientists},
	volume = {36},
	issn = {03600300},
	url = {http://portal.acm.org/citation.cfm?doid=1031120.1031122},
	doi = {10.1145/1031120.1031122},
	abstract = {The article aims to introduce computer scientists to the new field of bioinformatics. This area has arisen from the needs of biologists to utilize and help interpret the vast amounts of data that are constantly being gathered in genomic research--and its more recent counterparts, proteomics and functional genomics. The ultimate goal of bioinformatics is to develop in silico models that will complement in vitro and in vivo biological experiments. The article provides a bird's eye view of the basic concepts in molecular cell biology, outlines the nature of the existing data, and describes the kind of computer algorithms and techniques that are necessary to understand cell behavior. The underlying motivation for many of the bioinformatics approaches is the evolution of organisms and the complexity of working with incomplete and noisy data. The topics covered include: descriptions of the current software especially developed for biologists, computer and mathematical cell models, and areas of computer science that play an important role in bioinformatics.},
	number = {2},
	journal = {ACM Computing Surveys},
	author = {Cohen, Jacques},
	year = {2004},
	note = {ISBN: 0360-0300},
	pages = {122--158},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\97HAHJD3\\p122-cohen.pdf:application/pdf},
}

@article{Edwards2009,
	title = {Natural selection and phylogenetic analysis},
	volume = {106},
	issn = {0027-8424},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0904103106},
	doi = {10.1073/pnas.0904103106},
	number = {22},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Edwards, S. V.},
	year = {2009},
	pmid = {19470454},
	note = {ISBN: 0027-8424{\textbackslash}r1091-6490},
	pages = {8799--8800},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\36S66QPM\\Edwards2009.PNAS.pdf:application/pdf},
}

@article{Novichkov2004,
	title = {Genome-wide molecular clock and horizontal gene ransfer in bacterial evolution},
	volume = {186},
	issn = {0021-9193},
	doi = {10.1128/JB.186.19.6575},
	number = {19},
	journal = {Journal of Bacteriology},
	author = {Novichkov, Pavel S and Omelchenko, Marina V and Gelfand, Mikhail S and Mironov, Andrei a and Wolf, Yuri I and Koonin, Eugene V},
	year = {2004},
	pmid = {15375139},
	note = {ISBN: 0021-9193},
	pages = {6575--6585},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\MM62LG4M\\Novichov et al 2004.pdf:application/pdf},
}

@article{Shao2018,
	title = {Creating a functional single-chromosome yeast},
	volume = {560},
	issn = {14764687},
	url = {http://dx.doi.org/10.1038/s41586-018-0382-x},
	doi = {10.1038/s41586-018-0382-x},
	abstract = {Eukaryotic genomes are generally organized in multiple chromosomes. Here we have created a functional single-chromosome yeast from a Saccharomyces cerevisiae haploid cell containing sixteen linear chromosomes, by successive end-to-end chromosome fusions and centromere deletions. The fusion of sixteen native linear chromosomes into a single chromosome results in marked changes to the global three-dimensional structure of the chromosome due to the loss of all centromere-associated inter-chromosomal interactions, most telomere-associated inter-chromosomal interactions and 67.4\% of intra-chromosomal interactions. However, the single-chromosome and wild-type yeast cells have nearly identical transcriptome and similar phenome profiles. The giant single chromosome can support cell life, although this strain shows reduced growth across environments, competitiveness, gamete production and viability. This synthetic biology study demonstrates an approach to exploration of eukaryote evolution with respect to chromosome structure and function.},
	number = {7718},
	journal = {Nature},
	author = {Shao, Yangyang and Lu, Ning and Wu, Zhenfang and Cai, Chen and Wang, Shanshan and Zhang, Ling Li and Zhou, Fan and Xiao, Shijun and Liu, Lin and Zeng, Xiaofei and Zheng, Huajun and Yang, Chen and Zhao, Zhihu and Zhao, Guoping and Zhou, Jin Qiu and Xue, Xiaoli and Qin, Zhongjun},
	year = {2018},
	pmid = {30069045},
	note = {Publisher: Springer US},
	pages = {331--335},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\J7JQKP98\\Shao-etal18.pdf:application/pdf},
}

@article{Palumbi1989,
	title = {Rates of molecular evolution and the fraction of nucleotide positions free to vary},
	volume = {29},
	issn = {00222844},
	doi = {10.1007/BF02100116},
	abstract = {Selective constraints on DNA sequence change were incorporated into a model of DNA divergence by restricting substitutions to a subset of nucleotide positions. A simple model showed that both mutation rate and the fraction of nucleotide positions free to vary are strong determinants of DNA divergence over time. When divergence between two species approaches the fraction of positions free to vary, standard methods that correct for multiple mutations yield severe underestimates of the number of substitutions per site. A modified method appropriate for use with DNA sequence, restriction site, or thermal renaturation data is derived taking this fraction into account. The model also showed that the ratio of divergence in two gene classes (e.g., nuclear and mitochondrial) may vary widely over time even if the ratio of mutation rates remains constant. DNA sequence divergence data are used increasingly to detect differences in rates of molecular evolution. Often, variation in divergence rate is assumed to represent variation in mutation rate. The present model suggests that differing divergence rates among comparisons (either among gene classes or taxa) should be interpreted cautiously. Differences in the fraction of nucleotide positions free to vary can serve as an important alternative hypothesis to explain differences in DNA divergence rates},
	number = {2},
	journal = {Journal of Molecular Evolution},
	author = {Palumbi, Stephen R.},
	year = {1989},
	pmid = {2509718},
	keywords = {Constraints, Divergence, Evolution, Evolutionary distance, Mitochondrial DNA, Molecular evolution, Mutation, Neutral space, Two-parameter model, Variable positions},
	pages = {180--187},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\IG5U5ADV\\Palumbi1989.pdf:application/pdf},
}

@article{Hu2013,
	title = {The genetic equidistance result: {Misreading} by the molecular clock and neutral theory and reinterpretation nearly half of a century later},
	volume = {56},
	issn = {16747305},
	doi = {10.1007/s11427-013-4452-x},
	abstract = {In 1963, Margoliash discovered the unexpected genetic equidistance result after comparing cytochrome c sequences from different species. This finding, together with the hemoglobin analyses of Zuckerkandl and Pauling in 1962, directly inspired the ad hoc molecular clock hypothesis. Unfortunately, however, many biologists have since mistakenly viewed the molecular clock as a genuine reality, which in turn inspired Kimura, King, and Jukes to propose the neutral theory of molecular evolution. Many years of studies have found numerous contradictions to the theory, and few today believe in a universal constant clock. What is being neglected, however, is that the failure of the molecular clock hypothesis has left the original equidistance result an unsolved mystery. In recent years, we fortuitously rediscovered the equidistance result, which remains unknown to nearly all researchers. Incorporating the proven virtues of existing evolutionary theories and introducing the novel concept of maximum genetic diversity, we proposed a more complete hypothesis of evolutionary genetics and reinterpreted the equidistance result and other major evolutionary phenomena. The hypothesis may rewrite molecular phylogeny and population genetics and solve major biomedical problems that challenge the existing framework of evolutionary biology.},
	number = {3},
	journal = {Science China Life Sciences},
	author = {Hu, TaoBo B. and Long, MengPing P. and Yuan, DeJian J. and Zhu, ZhuBing B. and Huang, YiMin M. and Huang, Shi},
	year = {2013},
	pmid = {23526392},
	note = {ISBN: 1142701344},
	keywords = {macroevolution, evolution, genetic equidistance, maximum genetic diversity hypothesis, microevolution, molecular clock, neutral theory, overlap feature},
	pages = {254--261},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\DI6AC2IX\\Hu2013_Article_TheGeneticEquidistanceResultMi.pdf:application/pdf},
}

@article{Zuckerkandl1965,
	title = {Evolutionary divergence and convergence in proteins},
	issn = {02955075},
	doi = {10.1209/epl/i1998-00224-x},
	journal = {Evolving Genes and Proteins, Academic Press, New York},
	author = {Zuckerkandl, Emile and Pauling, Linus},
	year = {1965},
	pmid = {6453},
	note = {ISBN: 0295-5075},
	pages = {97--166},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ZBJQBYI6\\Zuckerkandl and Pauling 1965.pdf:application/pdf},
}

@article{Reanney1986,
	title = {genome design},
	number = {February},
	author = {Reanney, Darryl C.},
	year = {1986},
	pages = {41--46},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\3RB7GVN3\\Reanney86.pdf:application/pdf},
}

@article{Ho2014,
	title = {The changing face of the molecular evolutionary clock},
	volume = {29},
	issn = {01695347},
	url = {http://dx.doi.org/10.1016/j.tree.2014.07.004},
	doi = {10.1016/j.tree.2014.07.004},
	abstract = {The molecular clock has played an important role in biological research, both as a description of the evolutionary process and as a tool for inferring evolutionary timescales. Genomic data have provided valuable insights into the molecular clock, allowing the patterns and causes of evolutionary rate variation to be characterized in increasing detail. I explain how genome sequences offer exciting opportunities for estimating the timescale of the Tree of Life. I describe the different approaches that have been used to deal with the computational and statistical challenges encountered in molecular clock analyses of genomic data. Finally, I offer a perspective on the future of molecular clocks, highlighting some of the key limitations and the most promising research directions. © 2014 Elsevier Ltd.},
	number = {9},
	journal = {Trends in Ecology and Evolution},
	author = {Ho, Simon Y.W.},
	year = {2014},
	pmid = {25086668},
	note = {Publisher: Elsevier Ltd
ISBN: 0169-5347},
	keywords = {Genomic data, Molecular clock, Pacemaker models, Phylogenetic analysis, Rate heterogeneity},
	pages = {496--503},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\D4CDTBK8\\Ho 2014.pdf:application/pdf},
}

@article{Gillooly2005,
	title = {The rate of {DNA} evolution: {Effects} of body size and temperature on the molecular clock},
	volume = {102},
	issn = {0027-8424},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0407735101},
	doi = {10.1073/pnas.0407735101},
	abstract = {Observations that rates of molecular evolution vary widely within and among lineages have cast doubts on the existence of a single "molecular clock." Differences in the timing of evolutionary events estimated from genetic and fossil evidence have raised further questions about the accuracy of molecular clocks. Here, we present a model of nucleotide substitution that combines theory on metabolic rate with the now-classic neutral theory of molecular evolution. The model quantitatively predicts rate heterogeneity and may reconcile differences in molecular- and fossil-estimated dates of evolutionary events. Model predictions are supported by extensive data from mitochondrial and nuclear genomes. By accounting for the effects of body size and temperature on metabolic rate, this model explains heterogeneity in rates of nucleotide substitution in different genes, taxa, and thermal environments. This model also suggests that there is indeed a single molecular clock, as originally proposed by Zuckerkandl and Pauling [Zuckerkandl, E. \& Pauling, L. (11965) in Evolving Genes and Proteins, eds. Bryson, V. \& Vogel, H. J. (Academic, New York), pp. 97-166], but that it "ticks" at a constant substitution rate per unit of mass-specific metabolic energy rather than per unit of time. This model therefore links energy flux and genetic change. More generally, the model suggests that body size and temperature combine to control the overall rate of evolution through their effects on metabolism.},
	number = {1},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Gillooly, J. F. and Allen, A. P. and West, G. B. and Brown, J. H.},
	year = {2005},
	pmid = {15618408},
	note = {ISBN: 0027-8424},
	pages = {140--145},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\WDYTD4HR\\Gillooly et al 2005.pdf:application/pdf},
}

@article{Powell2004,
	title = {The {Scientific} {Method} of {Inquiry}},
	author = {Powell, Donald R.},
	year = {2004},
	pages = {18--21},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\8XGGWGEV\\Firestein2016_Ch8.pdf:application/pdf},
}

@article{Morgan1998,
	title = {Emile {Zuckerkandl}, {Linus} {Pauling}, and the {Molecular} {Evolutionary} {Clock}, 1959-1965},
	volume = {31},
	issn = {00225010},
	doi = {10.1023/A:1004394418084},
	abstract = {N/A},
	number = {2},
	journal = {Journal of the History of Biology},
	author = {Morgan, Gregory J.},
	year = {1998},
	pmid = {11620303},
	note = {arXiv: astro-ph/0005074v1
ISBN: 0022-5010},
	pages = {155--178},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\FLFBR2XN\\Morgan1998_Article_EmileZuckerkandlLinusPaulingAn.pdf:application/pdf},
}

@article{Luo2018,
	title = {Karyotype engineering by chromosome fusion leads to reproductive isolation in yeast},
	volume = {560},
	issn = {14764687},
	url = {http://dx.doi.org/10.1038/s41586-018-0374-x},
	doi = {10.1038/s41586-018-0374-x},
	abstract = {Extant species have wildly different numbers of chromosomes, even among taxa with relatively similar genome sizes (for example, insects)1,2. This is likely to reflect accidents of genome history, such as telomere–telomere fusions and genome duplication events3–5. Humans have 23 pairs of chromosomes, whereas other apes have 24. One human chromosome is a fusion product of the ancestral state6. This raises the question: how well can species tolerate a change in chromosome numbers without substantial changes to genome content? Many tools are used in chromosome engineering in Saccharomyces cerevisiae7–10, but CRISPR–Cas9-mediated genome editing facilitates the most aggressive engineering strategies. Here we successfully fused yeast chromosomes using CRISPR–Cas9, generating a near-isogenic series of strains with progressively fewer chromosomes ranging from sixteen to two. A strain carrying only two chromosomes of about six megabases each exhibited modest transcriptomic changes and grew without major defects. When we crossed a sixteen-chromosome strain with strains with fewer chromosomes, we noted two trends. As the number of chromosomes dropped below sixteen, spore viability decreased markedly, reaching less than 10\% for twelve chromosomes. As the number of chromosomes decreased further, yeast sporulation was arrested: a cross between a sixteen-chromosome strain and an eight-chromosome strain showed greatly reduced full tetrad formation and less than 1\% sporulation, from which no viable spores could be recovered. However, homotypic crosses between pairs of strains with eight, four or two chromosomes produced excellent sporulation and spore viability. These results indicate that eight chromosome–chromosome fusion events suffice to isolate strains reproductively. Overall, budding yeast tolerates a reduction in chromosome number unexpectedly well, providing a striking example of the robustness of genomes to change.},
	number = {7718},
	journal = {Nature},
	author = {Luo, Jingchuan and Sun, Xiaoji and Cormack, Brendan P. and Boeke, Jef D.},
	year = {2018},
	pmid = {30069047},
	note = {Publisher: Springer US},
	pages = {392--396},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ZGIB4ICH\\Luo-etal18.pdf:application/pdf},
}

@article{Lee2016,
	title = {Molecular clocks},
	volume = {26},
	issn = {09609822},
	url = {http://dx.doi.org/10.1016/j.cub.2016.03.071},
	doi = {10.1016/j.cub.2016.03.071},
	abstract = {In the 1960s, several groups of scientists, including Emile Zuckerkandl and Linus Pauling, had noted that proteins experience amino acid replacements at a surprisingly consistent rate across very different species. This presumed single, uniform rate of genetic evolution was subsequently described using the term 'molecular clock'. Biologists quickly realised that such a universal pacemaker could be used as a yardstick for measuring the timescale of evolutionary divergences: estimating the rate of amino acid exchanges per unit of time and applying it to protein differences across a range of organisms would allow deduction of the divergence times of their respective lineages (Figure 1).},
	number = {10},
	journal = {Current Biology},
	author = {Lee, Michael S.Y. and Ho, Simon Y.W.},
	year = {2016},
	pmid = {27218841},
	note = {Publisher: Elsevier
ISBN: 9780470015902},
	pages = {R399--R402},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\GZHVEGYB\\Lee and Ho 2016 Primer.pdf:application/pdf},
}

@incollection{Witten2011cc,
	title = {Index},
	isbn = {0-08-089036-9},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000249},
	abstract = {Index 0 − 1 loss function, 160 0.632 bootstrap, 155 1R (1-rule), 86–90 discretization, 315 example use, 87t learning procedure, 89–90 missing values and numeric data, 87–89 overfitting for, 88 pseudocode, 86f 11-point average recall, 175 A accuracy, of association rules, 72, 73, 116 minimum, 72, 119, 122–123 accuracy, of classification rules, 110, 205 activation function, 241–242 acuity parameter, 281 AD trees. See all-dimensions trees AdaBoost, 358–361 AdaBoostM1 algorithm, 358–359, 475t, 476–477 Add filter, 433t–435t, 436 AddClassification filter, 444t, 445 AddCluster filter, 433t–435t, 436–437 AddExpression filter, 433t–435t, 437 AddID filter, 433t–435t, 436 additive logistic regression, 364–365 additive regression, 362–365 AdditiveRegression algorithm, 475t, 476 AddNoise filter, 433t–435t, 441, 568 AddValues filter, 433t–435t, 438 ADTree algorithm, 446t–450t, 457 adversarial data mining, 393–395 agglomerative clustering, 273, 275–276 Akaike Information Criterion (AIC), 267, 456 algorithms. See specific Weka algorithms all-dimensions (AD) trees, 270–271 generation, 271–272 illustrated examples, 271f alternating decision trees, 366–367 example, 367, 367f prediction nodes, 366–367 splitter nodes, 366–367 Analyze panel, 505–509, 512–515 ancestor-of relation, 46 AND, 233 anomalies, detecting, 334–335 antecedent, of rule, 67, 69 AODE. See averaged one-dependence estimator AODE algorithm, 446t–450t, 451 AODEsr algorithm, 446t–450t, 451 applications, 375–399 automation, 28 challenge of, 375 data stream learning, 380–383 diagnosis, 25–26 fielded, 21–28 incorporating domain knowledge, 384–386 massive datasets, 378–380 message classifier, 531–538 text mining, 386–389 Apriori algorithm, 216 Apriori rule learner, 485–486, 486t default options, 582 output for association rules, 430f parameters, 584 area under the curve (AUC), 177, 580 area under the precision-recall curve (AUPRC), 177 ARFF files, 52–56 attribute specifications in, 54 attribute types in, 54 converting files to, 417–419 defined, 52–56 illustrated, 53f in Weka, 407 arithmetic underflow, 266–267 assignment of key phrases, 387–388 association learning, 40 association rule mining, 582–584 association rules, 11, 72–73. See also rules accuracy (confidence), 72–73, 116 characteristics, 72 computation requirement, 123–124 converting item sets to, 119 coverage (support), 72, 116 double-consequent, 123 examples, 11 finding, 116 finding large item sets, 219–222 frequent-pattern tree, 216–219 mining, 116–124 predicting multiple consequences, 72 relationships between, 73},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Witten, Ian H. and Frank, Eibe and {Hall} and {Mark A.}},
	year = {2011},
	pmid = {11221713},
	doi = {10.1016/B978-0-12-374856-0.00024-9},
	note = {ISSN: 14337851},
	pages = {607--629},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\XL9RVSW4\\Index_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Witten2011q,
	title = {Front {Matter}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000183},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
	year = {2011},
	doi = {10.1016/B978-0-12-374856-0.00018-3},
	note = {Issue: Third Edition},
	pages = {i--iii},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\7HVRYV8J\\Front-Matter_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@article{Liti2016,
	title = {Chromosomes get together},
	doi = {10.1038/nrclinonc.2017.11},
	journal = {Nature},
	author = {Liti, Gianni},
	year = {2016},
	pages = {8--9},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\GANH2NGR\\Liti18.pdf:application/pdf},
}

@incollection{Witten2011bb,
	title = {Copyright},
	isbn = {978-0-12-374856-0},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000250},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Witten, Ian H.},
	year = {2011},
	doi = {10.1016/B978-0-12-374856-0.00025-0},
	note = {ISSN: 14337851},
	pages = {iv},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\EP8M2V7R\\Copyright_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@article{Kaufmann2011,
	title = {Abe, {N}., {Zadrozny}, {B}., \& {Langford}, {J}. (2006). {Outlier} detection by active learning. {In}},
	issn = {14337851},
	doi = {10.1016/B978-0-12-374856-0.00023-7},
	number = {2006},
	author = {Kaufmann, Morgan},
	year = {2011},
	pmid = {11221713},
	note = {ISBN: 0080890369},
	pages = {767--772},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\VH8BFZ2Y\\References_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Witten2011z,
	title = {List of {Tables}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000201},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {{Witten Ian H.}},
	editor = {{Witten Ian H.} and Frank, Eibe},
	year = {2011},
	doi = {10.1016/B978-0-12-374856-0.00020-1},
	pages = {xix--xx},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\B2KIDW4C\\List-of-Tables_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Witten2011d,
	title = {Moving on},
	isbn = {978-0-12-374856-0},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000092},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
	year = {2011},
	pmid = {11221713},
	doi = {10.1016/B978-0-12-374856-0.00009-2},
	note = {ISSN: 14337851},
	pages = {375--399},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\9AT8N7TR\\Chapter-9-Moving-on-Applications-and-Beyond_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Witten2011l,
	title = {Algorithms},
	isbn = {0-08-089036-9},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000043},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
	year = {2011},
	pmid = {11221713},
	doi = {10.1016/B978-0-12-374856-0.00004-3},
	note = {ISSN: 14337851},
	pages = {85--145},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\PH3ME9XH\\Chapter-4-Algorithms-The-Basic-Methods_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Witten2011aa,
	title = {Preface},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Witten, Ian H.},
	year = {2011},
	doi = {10.1016/B978-0-12-374856-0.00021-3},
	pages = {xxi--xxvii},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\TM7FBH4U\\Preface_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Larose2011,
	title = {List of {Figures}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000195},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Larose, Daniel T. and Larose, Chantal D.},
	year = {2011},
	doi = {10.1016/B978-0-12-374856-0.00019-5},
	pages = {xv--xviii},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\M72PSKRE\\List-of-Figures_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Witten2011m,
	title = {Output},
	isbn = {978-0-12-374856-0},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000031},
	abstract = {Most of the techniques in this book produce easily comprehensible descriptions of the structural patterns in the data. Before looking at how these techniques work, we have to see how structural patterns can be expressed. There are many different ways for representing the patterns that can be discovered by machine learning, and each one dictates the kind of technique that can be used to infer that output structure from data. Once you understand how the output is represented, you have come a long way toward understanding how it can be generated. We saw many examples of data mining in Chapter 1. In these cases the output took the form of decision trees and classification rules, which are basic knowledge representation styles that many machine learning methods use. Knowledge is really too imposing a word for a decision tree or a collection of rules, and by using it we don't mean to imply that these structures vie with the real kind of knowledge that we carry in our heads—it's just that we need some word to refer to the structures that learning methods produce. There are more complex varieties of rules that allow exceptions to be specified, and ones that can express relations among the values of the attributes of different instances. Some problems have a numeric class, and—as mentioned in Chapter 1—the classic way of dealing with these is to use linear models. Linear models can also be adapted to deal with binary classification. More-over, special forms of trees can be developed for numeric prediction. Instance-based representations focus on the instances themselves rather than rules that govern their attribute values. Finally, some learning schemes generate clusters of instances. These different knowledge representation methods parallel the different kinds of learning problems introduced in Chapter 2. 3.1 TABLES The simplest, most rudimentary way of representing the output from machine learn-ing is to make it just the same as the input—a table. For example, Table 1.2 is a decision table for the weather data: You just look up the appropriate conditions to decide whether or not to play. Exactly the same process can be used for numeric prediction too—in this case, the structure is sometimes referred to as a regression table. Less trivially, creating a decision or regression table might involve selecting},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
	year = {2011},
	doi = {10.1016/B978-0-12-374856-0.00003-1},
	note = {ISSN: 14337851},
	pages = {61--83},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\WCDQ9QYV\\Chapter-3-Output-Knowledge-Representation_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Witten2011c,
	title = {Input},
	isbn = {0-08-089036-9},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B978012374856000002X},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
	year = {2011},
	pmid = {11221713},
	doi = {10.1016/B978-0-12-374856-0.00002-X},
	note = {ISSN: 14337851},
	pages = {39--60},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\L5ILNZ4X\\Chapter-2-Input-Concepts-Instances-and-Attributes_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Witten2011,
	title = {Tutorial {Exercises} for the {Weka} {Explorer}},
	isbn = {978-0-12-374856-0},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000171},
	abstract = {FIGURE 17.1   The data viewer.      FIGURE 17.2   Output after building and testing the classifier: (a) screenshot and (b) decision tree.      FIGURE 17.3   The decision tree that has been built.      Table 17.1   Accuracy Obtained Using  IBk , for ...},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
	year = {2011},
	pmid = {11221713},
	doi = {10.1016/B978-0-12-374856-0.00017-1},
	note = {ISSN: 14337851},
	pages = {559--585},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\8HLBE79X\\Chapter-17-Tutorial-Exercises-for-the-Weka-Explorer_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Witten2011n,
	title = {Ensemble {Learning}},
	isbn = {978-0-12-374856-0},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000080},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
	year = {2011},
	doi = {10.1016/B978-0-12-374856-0.00008-0},
	pages = {351--373},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\PLKUH7VC\\Chapter-8-Ensemble-Learning_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Witten2011f,
	title = {Data {Transformations}},
	isbn = {978-0-12-374856-0},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000079},
	abstract = {Analysis of Ecological Communities offers a rationale and guidance for selecting appropriate, effective, analytical methods in community ecology. The book is suitable as a textbook and reference book on methods for multivariate analysis of ecological communities and their environments. The book covers distance measures, data transformation, outlier analysis, coordination, cluster analysis, PCA RA, CA, DCA, NMS, NMS, CCA, Bray-Curtis, MRPP, Mantel test, discriminant analysis, twinspan, classification and regression trees, structural equation modeling, and more. It also includes brief treatments of community sampling and diversity measures. The 304 page book is richly illustrated. It provides many examples from the literature and demonstrations of basic principles with simulated and real data sets.},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
	year = {2011},
	pmid = {8215535},
	doi = {10.1016/B978-0-12-374856-0.00007-9},
	note = {arXiv: 1011.1669v3
ISSN: 14337851},
	pages = {305--349},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\EH5JZ2GE\\Chapter-7-Data-Transformations_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Witten2011g,
	title = {Credibility},
	isbn = {978-0-12-374856-0},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000055},
	abstract = {Evaluation is the key to making real progress in data mining. There are lots of ways of inferring structure from data: We have encountered many already and will see further refinements, and new methods, in Chapter 6. However, in order to determine which ones to use on a particular problem we need systematic ways to evaluate how different methods work and to compare one with another. But evaluation is not as simple as it might appear at first sight. What's the problem? We have the training set; surely we can just look at how well different methods do on that. Well, no: As we will see very shortly, performance on the training set is definitely not a good indicator of performance on an indepen-dent test set. We need ways of predicting performance bounds in practice, based on experiments with whatever data can be obtained. When a vast supply of data is available, this is no problem: Just make a model based on a large training set, and try it out on another large test set. But although data mining sometimes involves " big data " —particularly in marketing, sales, and customer support applications—it is often the case that data, quality data, is scarce. The oil slicks mentioned in Chapter 1 (page 23) had to be detected and marked manually—a skilled and labor-intensive process—before being used as training data. Even in the personal loan application data (page 22), there turned out to be only 1000 training examples of the appropriate type. The electricity supply data (page 24) went back 15 years, 5000 days—but only 15 Christmas days and Thanks-givings, and just four February 29s and presidential elections. The electromechanical diagnosis application (page 25) was able to capitalize on 20 years of recorded experience, but this yielded only 300 usable examples of faults. The marketing and sales applications (page 26) certainly involve big data, but many others do not: Training data frequently relies on specialist human expertise—and that is always in short supply. The question of predicting performance based on limited data is an interesting, and still controversial, one. We will encounter many different techniques, of which one—repeated cross-validation—is probably the method of choice in most practical limited-data situations. Comparing the performance of different machine learning schemes on a given problem is another matter that is not as easy as it sounds: To be sure that apparent differences are not caused by chance effects, statistical tests are needed. 148 CHAPTER Credibility: Evaluating What's Been Learned So far we have tacitly assumed that what is being predicted is the ability to clas-sify test instances accurately; however, some situations involve predicting class probabilities rather than the classes themselves, and others involve predicting numeric rather than nominal values. Different methods are needed in each case. Then we look at the question of cost. In most practical data mining situations, the cost of a misclassification error depends on the type of error it is—whether, for example, a positive example was erroneously classified as negative or vice versa. When doing data mining, and evaluating its performance, it is often essential to take these costs into account. Fortunately, there are simple techniques to make most learning schemes cost sensitive without grappling with the algorithm's internals. Finally, the whole notion of evaluation has fascinating philosophical connections. For 2000 years, philosophers have debated the question of how to evaluate scientific theories, and the issues are brought into sharp focus by data mining because what is extracted is essentially a " theory " of the data.},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
	year = {2011},
	pmid = {11221713},
	doi = {10.1016/B978-0-12-374856-0.00005-5},
	note = {ISSN: 14337851},
	pages = {147--187},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\YWZI4UFN\\Chapter-5-Credibility-Evaluating-What-s-Been-Learned_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Witten2011b,
	title = {The {Knowledge} {Flow} {Interface}},
	isbn = {978-0-12-374856-0},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000122},
	abstract = {With the Knowledge Flow interface, users select Weka components from a tool bar, place them on a layout canvas, and connect them into a directed graph that processes and analyzes data. It provides an alternative to the Explorer for those who like think-ing in terms of how data flows through the system. It also allows the design and execution of configurations for streamed data processing, which the Explorer cannot do. You invoke the Knowledge Flow interface by selecting KnowledgeFlow from the choices in the right panel shown in Figure 11.3(a). 12.1 GETTING STARTED Here is a step-by-step example that loads an ARFF file and performs a cross-validation using J4.8. We describe how to build up the final configuration shown in Figure 12.1. First, create a source of data by clicking on the DataSources tab (left-most entry in the bar at the top) and selecting ARFFLoader from the toolbar. The mouse cursor changes to crosshairs to signal that you should next place the compo-nent. Do this by clicking anywhere on the canvas, whereupon a copy of the ARFF loader icon appears there. To connect it to an ARFF file, right-click it to bring up the pop-up menu shown in Figure 12.2(a). Click Configure to get the file browser in Figure 12.2(b), from which you select the desired ARFF file (alternatively, double-clicking on a component's icon is a short-cut for selecting Configure from the pop-up menu). Now we specify which attribute is the class using a ClassAssigner object. This is on the Evaluation panel, so click the Evaluation tab, select the ClassAssigner, and place it on the canvas. To connect the data source to the class assigner, right-click the data source icon and select dataSet from the menu, as shown in Figure 12.2(a). A rubber-band line appears. Move the mouse over the class assigner com-ponent and left-click. A red line labeled dataSet appears, joining the two compo-nents. Having connected the class assigner, choose the class by right-clicking it, selecting Configure, and entering the location of the class attribute. We will perform cross-validation on the J48 classifier. In the data flow model, we first connect the CrossValidationFoldMaker to create the folds on which the classifier will run, and then pass its output to an object representing J48.},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
	year = {2011},
	pmid = {11221713},
	doi = {10.1016/B978-0-12-374856-0.00012-2},
	note = {ISSN: 14337851},
	pages = {495--503},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\Y2HS4QM6\\Chapter-12-The-Knowledge-Flow-Interface_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Witten2011i,
	title = {Implementations},
	isbn = {0-08-089036-9},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000067},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
	year = {2011},
	pmid = {11221713},
	doi = {10.1016/B978-0-12-374856-0.00006-7},
	note = {ISSN: 14337851},
	pages = {191--304},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\JJ9TIHVN\\Chapter-6-Implementations-Real-Machine-Learning-Schemes_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Witten2011k,
	title = {The {Explorer}},
	volume = {2},
	isbn = {978-0-12-374856-0},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000110},
	abstract = {Weka's main graphical user interface, the Explorer, gives access to all its facilities using menu selection and form filling. It is illustrated in Figure 11.1. There are six different panels, selected by the tabs at the top, corresponding to the various data mining tasks that Weka supports. 11.1 GETTING STARTED Suppose you have some data and you want to build a decision tree from it. First, you need to prepare the data, then fire up the Explorer and load it in. Next, you select a decision tree construction method, build a tree, and interpret the output. It's easy to do it again with a different tree construction algorithm or a different evalu-ation method. In the Explorer you can flip back and forth between the results you have obtained, evaluate the models that have been built on different datasets, and visualize graphically both the models and the datasets themselves, including any classification errors the models make. Preparing the Data The data is often presented in a spreadsheet or database. However, Weka's native data storage method is the ARFF format (see Section 2.4, page 52). You can easily convert from a spreadsheet to ARFF. The bulk of an ARFF file consists of a list of the instances, and the attribute values for each instance are separated by commas (see Figure 2.2). Most spreadsheet and database programs allow you to export data into a file in comma-separated value (CSV) format as a list of records with commas between items. Having done this, you need only load the file into a text editor or word processor; add the dataset's name using the @relation tag, the attribute infor-mation using @attribute, and an @data line; then save the file as raw text. For example, Figure 11.2 shows an Excel spreadsheet containing the weather data from Section 1.2 (page 9), the data in CSV form loaded into Microsoft Word, and the result of converting it manually into an ARFF file. However, you don't actually have to go through these steps to create the ARFF file yourself because the Explorer can read CSV spreadsheet files directly, as described later.},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
	year = {2011},
	pmid = {11221713},
	doi = {10.1016/B978-0-12-374856-0.00011-0},
	note = {Issue: page 9
ISSN: 14337851},
	pages = {407--494},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\WIEPNYXE\\Chapter-11-The-Explorer_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Witten2011p,
	title = {Introduction to {Weka}},
	isbn = {978-0-12-374856-0},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000109},
	abstract = {Technology now allows us to capture and store vast quantities of data. Finding patterns, trends, and anomalies in these datasets, and summarizing them with simple quantitative models, is one of the grand challenges of the infor- mation age—turning data into information and turning information into knowledge. There has been stunning progress in data mining and machine learning. The synthesis of statistics, machine learning, information theory, and computing has created a solid science, with a firm mathematical base, and with very powerful tools.Witten and Frank present much of this progress in this book and in the companion implementation of the key algorithms. As such, this is a milestone in the synthesis of data mining, data analysis, information theory, and machine learning. If you have not been following this field for the last decade, this is a great way to catch up on this exciting progress. If you have, then Witten and Frank’s presentation and the companion open-source workbench, called Weka, will be a useful addition to your toolkit. They present the basic theory of automatically extracting models from data, and then validating those models. The book does an excellent job of explaining the various models (decision trees, association rules, linear models, clustering, Bayes nets, neural nets) and how to apply them in practice.With this basis, they then walk through the steps and pitfalls of various approaches. They describe how to safely scrub datasets, how to build models, and how to evaluate a model’s predictive quality.Most of the book is tutorial, but Part II broadly describes how commercial systems work and gives a tour of the publicly available data mining workbench that the authors provide through a website. This Weka workbench has a graphical user interface that leads you through data mining tasks and has excellent data visualization tools that help understand the models. It is a great companion to the text and a useful and popular tool in its own right. This book presents this new discipline in a very accessible form: as a text both to train the next generation of practitioners and researchers and to inform lifelong learners like myself. Witten and Frank have a passion for simple and elegant solutions. They approach each topic with this mindset, grounding all concepts in concrete examples, and urging the reader to consider the simple techniques first, and then progress to the more sophisticated ones if the simple ones prove inadequate. If you are interested in databases, and have not been following the machine learning field, this book is a great way to catch up on this exciting progress. If you have data that you want to analyze and understand, this book and the asso- ciated Weka toolkit are an excellent way to start.},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
	year = {2011},
	pmid = {11221713},
	doi = {10.1016/B978-0-12-374856-0.00010-9},
	note = {ISSN: 14337851},
	pages = {403--406},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\HYD7N4DS\\Chapter-10-Introduction-to-Weka_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Witten2011h,
	title = {What's {It} {All} {About}?},
	isbn = {0-08-089036-9},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000018},
	abstract = {"Data Mining: Practical Machine Learning Tools and Techniques" offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. This highly anticipated third edition of the most acclaimed work on data mining and machine learning will teach you everything you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining. Thorough updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including new material on Data Transformations, Ensemble Learning, Massive Data Sets, Multi-instance Learning, plus a new version of the popular Weka machine learning software developed by the authors. Witten, Frank, and Hall include both tried-and-true techniques of today as well as methods at the leading edge of contemporary research. Provides a thorough grounding in machine learning concepts as well as practical advice on applying the tools and techniques to your data mining projects Offers concrete tips and techniques for performance improvement that work by transforming the input or output in machine learning methods Includes downloadable Weka software toolkit, a collection of machine learning algorithms for data mining tasks-in an updated, interactive interface. Algorithms in toolkit cover: data pre-processing, classification, regression, clustering, association rules, visualization},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
	year = {2011},
	pmid = {11221713},
	doi = {10.1016/B978-0-12-374856-0.00001-8},
	note = {arXiv: 1011.1669v3
ISSN: 14337851},
	pages = {3--38},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\2WPZNN4F\\Chapter-1-What-s-It-All-About-_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Witten2011r,
	title = {Writing {New} {Learning} {Schemes}},
	isbn = {978-0-12-374856-0},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B978012374856000016X},
	abstract = {Suppose you need to implement a special-purpose learning algorithm that is not included in Weka. Or suppose you are engaged in machine learning research and want to investigate a new learning scheme. Or suppose you just want to learn more about the inner workings of an induction algorithm by actually programming it yourself. This section uses a simple example to show how to make full use of Weka's class hierarchy when writing classifiers. Weka includes the elementary learning schemes listed in Table 16.1, mainly for educational purposes. None take any scheme-specific command-line options. They are all useful for understanding the inner workings of a classifier. As an example, we describe the weka.classifiers.trees.Id3 scheme, which implements the ID3 deci-sion tree learner from Section 4.3 (page 99). Other schemes, such as clustering algorithms and association rule learners, are organized in a similar manner. 16.1 AN EXAMPLE CLASSIFIER Figure 16.1 gives the source code of weka.classifiers.trees.Id3, which extends the Classifier class, as you can see from what is shown in the eight-page figure that follows the next page. Every classifier in Weka does so, whether it predicts a nominal class or a numeric one. It also implements two interfaces, Technical­ InformationHandler and Sourcable, which, respectively, allow the implementing class to provide bibliographical references for display in Weka's graphical user interface and a source code representation of its learned model. The first method in weka.classifiers.trees.Id3 is globalInfo(): We mention it here before moving on to the more interesting parts. It simply returns a string that is displayed in Weka's graphical user interface when this scheme is selected. Part of the string includes information generated by the second method, getTech­ nicalInformation(), which formats a bibliographic reference for the ID3 algorithm. The third method, getCapabilities(), returns information on the data characteristics that Id3 can handle, namely nominal attributes and a nominal class—and the fact that it can deal with missing class values and data that contains no instances (although the latter does not produce a useful model!). Capabilities are described in Section 16.2.},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
	year = {2011},
	pmid = {11221713},
	doi = {10.1016/B978-0-12-374856-0.00016-X},
	note = {ISSN: 14337851},
	pages = {539--557},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\VMTKDYJU\\Chapter-16-Writing-New-Learning-Schemes_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Witten2011o,
	title = {Embedded {Machine} {Learning}},
	isbn = {978-0-12-374856-0},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000158},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
	year = {2011},
	pmid = {11221713},
	doi = {10.1016/B978-0-12-374856-0.00015-8},
	note = {ISSN: 14337851},
	pages = {531--538},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\8CTK8NVQ\\Chapter-15-Embedded-Machine-Learning_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Witten2011a,
	title = {The {Command}-{Line} {Interface}},
	isbn = {978-0-12-374856-0},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000146},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
	year = {2011},
	pmid = {11221713},
	doi = {10.1016/B978-0-12-374856-0.00014-6},
	note = {ISSN: 14337851},
	pages = {519--529},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\QT8795J5\\Chapter-14-The-Command-Line-Interface_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Witten2011e,
	title = {The {Experimenter}},
	volume = {1},
	isbn = {978-0-12-374856-0},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000134},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
	year = {2011},
	pmid = {11221713},
	doi = {10.1016/B978-0-12-374856-0.00013-4},
	note = {ISSN: 14337851},
	pages = {505--517},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\GWK9MTF5\\Chapter-13-The-Experimenter_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@article{Soleymani2017k,
	title = {Stats 707},
	author = {Soleymani, Mehdi},
	year = {2017},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\LZUDDDAK\\9 Comparing two populations - Examples.pdf:application/pdf},
}

@book{Kuhn2013,
	title = {Applied {Predictive} {Modeling} [{Hardcover}]},
	isbn = {1-4614-6848-5},
	url = {http://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485/ref=pd_bxgy_b_img_z},
	abstract = {Applied Predictive Modeling covers the overall predictive modeling process, beginning with the crucial steps of data preprocessing, data splitting and foundations of model tuning.  The text then provides intuitive explanations of numerous common and modern regression and classification techniques, always with an emphasis on illustrating and solving real data problems.  Addressing practical concerns extends beyond model fitting to topics such as handling class imbalance, selecting predictors, and pinpointing causes of poor model performance―all of which are problems that occur frequently in practice. 
  
The text illustrates all parts of the modeling process through many hands-on, real-life examples.  And every chapter contains extensive R code for each step of the process.  The data sets and corresponding code are available in the book's companion AppliedPredictiveModeling R package, which is freely available on the CRAN archive. 
  
This multi-purpose text can be used as an introduction to predictive models and the overall modeling process, a practitioner's reference handbook, or as a text for advanced undergraduate or graduate level predictive modeling courses.  To that end, each chapter contains problem sets to help solidify the covered concepts and uses data available in the book's R package. 
  
Readers and students interested in implementing the methods should have some basic knowledge of R.  And a handful of the more advanced topics require some mathematical knowledge.},
	author = {Kuhn, Max and Johnson, Kjell},
	year = {2013},
	pmid = {17629633},
	doi = {10.1007/978-1-4614-6849-3},
	note = {ISSN: 03781119},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\WBDZWMNV\\Applied Predictive Modelling.pdf:application/pdf},
}

@incollection{Cunningham2011,
	title = {Acknowledgments},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000225},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	author = {Cunningham, Jo and Humphrey, Matt and Hunt, Lyn and Mcqueen, Bob and Smith, Lloyd and Inglis, Stuart and Nevill-manning, Craig},
	year = {2011},
	doi = {10.1016/B978-0-12-374856-0.00022-5},
	pages = {xxix--xxxi},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\VT2HAX7B\\Acknowledgments_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@incollection{Witten2011j,
	title = {About the {Authors}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123748560000262},
	booktitle = {Data {Mining}: {Practical} {Machine} {Learning} {Tools} and {Techniques}},
	publisher = {Elsevier Inc.},
	author = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
	year = {2011},
	doi = {10.1016/B978-0-12-374856-0.00026-2},
	note = {Issue: 1999},
	pages = {xxxiii},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\9GS9D2UZ\\About-the-Authors_2011_Data-Mining-Practical-Machine-Learning-Tools-and-Techniques-Third-Edition-.pdf:application/pdf},
}

@article{Soleymani2017m,
	title = {Stats 707},
	author = {Soleymani, Mehdi},
	year = {2017},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\8DZFMEBU\\3 Concept of testing hypothese - Examples.pdf:application/pdf},
}

@article{Soleymani2017o,
	title = {Stats 707},
	author = {Soleymani, Mehdi},
	year = {2017},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\M6E2SP25\\2b Concept of testing hypotheses.pdf:application/pdf},
}

@article{Soleymani2017f,
	title = {Stats 707},
	author = {Soleymani, Mehdi},
	year = {2017},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\56Q8KQ6Z\\8 Comparing two populations.pdf:application/pdf},
}

@article{Soleymani2017p,
	title = {Stats 707},
	author = {Soleymani, Mehdi},
	year = {2017},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\A9J5KT87\\4 Normal distribution and statistic.pdf:application/pdf},
}

@article{Soleymani2017j,
	title = {Stats 707},
	author = {Soleymani, Mehdi},
	year = {2017},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\GRYL9YZ7\\2a Introduction to R.pdf:application/pdf},
}

@article{Soleymani2017h,
	title = {Stats 707},
	author = {Soleymani, Mehdi},
	year = {2017},
	pages = {1--47},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\5FIW3SQP\\6 Inferences about a population - part 2.pdf:application/pdf},
}

@article{Soleymani2017c,
	title = {Warm-up exercise},
	author = {Soleymani, Mehdi},
	year = {2017},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\DP3TXGGQ\\5 Inferences about a population.pdf:application/pdf},
}

@article{Soleymani2017q,
	title = {Stats 707},
	author = {Soleymani, Mehdi},
	year = {2017},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\NK7TUJWC\\7 Inferences about a population - part 3.pdf:application/pdf},
}

@article{Soleymani2017i,
	title = {Stats 707},
	author = {Soleymani, Mehdi},
	year = {2017},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\42SGNTSU\\13 Categorical data.pdf:application/pdf},
}

@article{Soleymani2017d,
	title = {Recall ( testing hypotheses )},
	author = {Soleymani, Mehdi},
	year = {2017},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\EU9PHB5J\\12 Regression Analysis - Examples.pdf:application/pdf},
}

@article{Soleymani2017,
	title = {Stats 707},
	author = {Soleymani, Mehdi},
	year = {2017},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\MYKGIBUB\\11 Regression Analysis - Model checking and multiple LR.pdf:application/pdf},
}

@article{Soleymani2017e,
	title = {Stats 707},
	author = {Soleymani, Mehdi},
	year = {2017},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\SC8E2UN7\\10 Regression Analysis - Simple linear relation.pdf:application/pdf},
}

@article{Soleymani2017a,
	title = {Stats 707},
	author = {Soleymani, Mehdi},
	year = {2017},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\QNF8SVX6\\1b Introduction to probability.pdf:application/pdf},
}

@article{Soleymani2017n,
	title = {Stats 707},
	author = {Soleymani, Mehdi},
	year = {2017},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\CUETHCQT\\1A introduction.pdf:application/pdf},
}

@article{Soleymani2017l,
	title = {Stats 707},
	author = {Soleymani, Mehdi},
	year = {2017},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\EHPR8HAJ\\15 Revision.pdf:application/pdf},
}

@article{Soleymani2017g,
	title = {Stats 707},
	author = {Soleymani, Mehdi},
	year = {2017},
	pages = {1--16},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\PYYIH9FY\\14 Categorical data - Examples.pdf:application/pdf},
}

@article{Unay2011,
	title = {Assignment 3 {Instructions}},
	volume = {25},
	doi = {10.1142/S0217979211101806},
	number = {c},
	author = {Unay, S D G and Kavanoz, H B},
	year = {2011},
	note = {ISBN: 0217979211},
	keywords = {bredig transition, molecular dynamics simulation, mopysical properties, ther-, uranium dioxide},
	pages = {3211--3223},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\9EDQ8UQ6\\Assignment 3 Instructions.pdf:application/pdf},
}

@article{Soleymani2017b,
	title = {Stats 707},
	author = {Soleymani, Mehdi},
	year = {2017},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\DKYIX73Y\\1 overview.pdf:application/pdf},
}

@incollection{Ruggiero,
	title = {Appendix {A} - {A} brief introduction to {R}},
	isbn = {978-1-139-02209-5},
	author = {Ruggiero, Kathy},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\YS48I6M8\\A brief introduction to R.pdf:application/pdf},
}

@article{Anderson2001,
	title = {Permutation tests for univariate or multivariate analysis of variance and regression},
	volume = {58},
	issn = {0706-652X},
	url = {http://www.nrcresearchpress.com/doi/abs/10.1139/f01-004},
	doi = {10.1139/f01-004},
	abstract = {The most appropriate strategy to be used to create a permutation distribution for tests of individual terms in complex experimental designs is currently unclear. There are often many possibilities, including restricted permutation or permutation of some form of residuals. This paper provides a summary of recent empirical and theoretical results concerning available methods and gives recommendations for their use in univariate and multivariate applications. The focus of the paper is on complex designs in analysis of variance and multiple regression (i.e., linear models). The assumption of exchangeability required for a permutation test is assured by random allocation of treatments to units in experimental work. For observational data, exchangeability is tantamount to the assumption of independent and identically distributed errors under a null hypothesis. For partial regression, the method of permutation of residuals under a reduced model has been shown to provide the best test. For analysis of variance, o..., La strat\{é\}gie la plus appropri\{é\}e pour g\{é\}n\{é\}rer une distribution de permutation en vue de tester les termes individuels d'un plan exp\{é\}rimental complexe n'est pas \{é\}vidente \{à\} l'heure actuelle. Il y a souvent plusieurs options, dont la permutation restreinte et la permutation d'une quelconque forme des r\{é\}siduels. On trouvera ici un r\{é\}sum\{é\} d'informations r\{é\}centes empiriques et th\{é\}oriques sur les m\{é\}thodes disponibles, ainsi que des recommandations pour leur utilisation dans des applications unidimensionnelles et multidimensionnelles. L'emphase est mise sur les plans complexes d'analyse de variance et de r\{é\}gression multiple (i.e. les mod\{è\}les lin\{é\}aires). Dans un travail exp\{é\}rimental, la supposition d'\{é\}changeabilit\{é\} requise pour un test par permutation est assur\{é\}e par l'assignation au hasard \{à\} des unit\{é\}s des divers traitements. Dans le cas d'observations, l'\{é\}changeabilit\{é\} \{é\}quivaut \{à\} supposer que les erreurs, dans une hypoth\{è\}se nulle, sont ind\{é\}pendantes et distribu\{é\}es de fa\{ç\}on identique. Pour la r\{é\}gression partielle,...},
	number = {3},
	journal = {Canadian Journal of Fisheries and Aquatic Sciences},
	author = {Anderson, Marti J},
	year = {2001},
	pmid = {5684003},
	note = {ISBN: 0706-652X},
	pages = {626--639},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ZL6TV84Y\\Permutation tests for uni- or multi-variate ANOVA and regression.pdf:application/pdf},
}

@article{Solutions2000,
	title = {Histogram of data [, 1 ]},
	volume = {5},
	number = {c},
	author = {Solutions, Partial},
	year = {2000},
	keywords = {f\_miss, togram of data},
	pages = {4000},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\JSAUN2DW\\Assignment 1 - SOLUTIONS.pdf:application/pdf},
}

@article{Ruggiero2016,
	title = {Week 5:},
	number = {April},
	author = {Ruggiero, Kathy},
	year = {2016},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\GLJ8HASC\\Seminar 5 [2018].pdf:application/pdf},
}

@article{Ruggiero2018a,
	title = {Week 6 : {Resampling} procedures},
	number = {April},
	author = {Ruggiero, Kathy},
	year = {2018},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\AQMDUT2B\\Seminar 6 [2018].pdf:application/pdf},
}

@article{Ruggiero2018b,
	title = {Week 4 : {More} on design and analysis of experiments : {Last} of the three principles of design ; {Split}-block designs},
	number = {April},
	author = {Ruggiero, Kathy},
	year = {2018},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\VR4GBR3W\\Seminar 4 [2018].pdf:application/pdf},
}

@article{Park2008,
	title = {Assignment \# 5},
	volume = {2018},
	number = {June},
	author = {Park, Jungkyu},
	year = {2008},
	pages = {2018},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\FALSCGXM\\Assignment5.pdf:application/pdf},
}

@article{Clarke1997,
	title = {Introduction to the {Design} and {Analysis} of {Experiments}},
	number = {March},
	author = {Clarke, G M and Kempson, R E},
	year = {1997},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\5ICEFH8I\\Seminar 1 [2018] - Full size.pdf:application/pdf},
}

@article{Moore2004,
	title = {Bootstrap {Methods} and {Permutation} {Tests} ({Chapter} 14)},
	issn = {0040-1706},
	url = {papers2://publication/uuid/861C080D-C170-4EBA-BCDD-07A1DEDE1AA6},
	doi = {10.1016/j.fishres.2006.11.017},
	abstract = {The continuing revolution in computing is having a dramatic influence on statistics. Exploratory analysis of data becomes easier as graphs and calcula- tions are automated. Statistical study of very large and very complex data sets becomes feasible. Another impact of fast and cheap computing is less obvious: new methods that apply previously unthinkable amounts of computation to small sets of data to produce confidence intervals and tests of significance in settings that don’t meet the conditions for safe application of the usual meth- ods of inference.},
	journal = {Introduction to the Practice of Statistics},
	author = {Moore, David S and McCabe, George P and Craig, Bruce and Hesterberg, Tim and Monaghan, Shaun and Clipson, Ashley and Epstein, Rachel},
	year = {2004},
	pmid = {21401589},
	note = {ISBN: 978-0716766544},
	pages = {1--70},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\FCPJELMA\\Bootstrap methods and permutation tests.pdf:application/pdf},
}

@article{Ruggiero2018d,
	title = {Errors in hypothesis testing ; {The} multiple testing problem ; {Factorial} experiments},
	number = {March},
	author = {Ruggiero, Kathy},
	year = {2018},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\PI2UMJHY\\Seminar 3 [2018].pdf:application/pdf},
}

@article{Ruggiero2018c,
	title = {Week 2 {More} on the analysis of {CRDs} ; {Errors} in hypothesis testing ; {The}},
	number = {March},
	author = {Ruggiero, Kathy},
	year = {2018},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\7G3CKQKN\\Seminar 2 [2018].pdf:application/pdf},
}

@article{Lennerfors2018,
	title = {Assignment 4},
	number = {May},
	author = {Lennerfors, Thomas},
	year = {2018},
	pages = {4--6},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\PR646NL4\\Assignment4.pdf:application/pdf},
}

@article{Physics2013,
	title = {Assignment \# 3},
	volume = {010},
	number = {May},
	author = {Physics, Atomic and Prof, I I and Spring, Wolfgang Ketterle},
	year = {2013},
	pages = {1--6},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\VHS4QEFC\\Assignment3.pdf:application/pdf},
}

@article{First2017,
	title = {Workshop 2 {Read} the data into {R}},
	volume = {1},
	number = {March},
	author = {First, Read M E and Zealand, New and Vigour, Plant},
	year = {2017},
	pages = {1--7},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\GLYZF83N\\Workshop 3 Exercises.pdf:application/pdf},
}

@article{Rankin2016,
	title = {Assignment 1},
	issn = {1098-6596},
	doi = {10.1017/CBO9781107415324.004},
	number = {March},
	author = {Rankin, Hallie},
	year = {2016},
	pmid = {25246403},
	note = {arXiv: 1011.1669v3
ISBN: 9202122232425},
	pages = {2016},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\JI329BDJ\\Assignment1.pdf:application/pdf},
}

@article{Ruggiero2018,
	title = {{BIOSCI} 738 - {Assignment} 1 {Model} {Answers}},
	volume = {1},
	number = {January},
	author = {Ruggiero, Kathy},
	year = {2018},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ASKUF96M\\Assignment1 - Model Answers with R code.pdf:application/pdf},
}

@article{Russell2018,
	title = {Workshop 6 {The} study},
	volume = {2018},
	number = {May 2016},
	author = {Russell, James and Ocean, Western Indian},
	year = {2018},
	pages = {65--71},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\LYJVWNDJ\\Workshop 6 Exercises.pdf:application/pdf},
}

@article{Pritchard2010,
	title = {Documentation for structure software : {Version} 2 . 3},
	volume = {6},
	issn = {00380644},
	doi = {10.1002/spe.4380060305},
	abstract = {We describe a model-based clustering method for using multilocus genotype data to infer population structure and assign individuals to populations. We assume a model in which there are K populations (where K may be unknown), each of which is characterized by a set of allele frequencies at each locus. Individuals in the sample are assigned (probabilistically) to populations, or jointly to two or more populations if their genotypes indicate that they are admixed. Our model does not assume a particular mutation process, and it can be applied to most of the commonly used genetic markers, provided that they are not closely linked. Applications of our method include demonstrating the presence of population structure, assigning individuals to populations, studying hybrid zones, and identifying migrants and admired individuals. We show that the method can produce highly accurate assignments using modest numbers of loci-e.g., seven microsatellite loci in an example using genotype data from an endangered bird species. The software used for this article is available from http://www.stats.ox.ac.uk/similar to pritch/home.html},
	number = {3},
	journal = {University of Chicago. IL},
	author = {Pritchard, Jonathan K and Wen, X and Falush, Daniel},
	year = {2010},
	pmid = {21565030},
	note = {ISBN: {\textless}null{\textgreater}},
	pages = {321--326},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\EQFMUTTG\\m-api-6abc6f89-feaf-643d-0c13-a73cdb4b91ad.pdf:application/pdf},
}

@misc{Meyer2017,
	title = {{WOMBAT} {A} program for {Mixed} {Model} {Analyses} by {Restricted} {Maximum} {Likelihood} - {User} {Notes}},
	abstract = {Meyer, K. (2006). WOMBAT– Digging deep for quantitative genetic analyses by restricted maximum likelihood. Proc. 8thWorld Congr. Genet. Appl. Livest. Prod., Communication No. 27–14. Meyer, K. (2006). WOMBAT – A program for mixed model analyses by restricted maximum likelihood. User notes. Animal Genetics and Breeding Unit, Armidale, npp. All},
	author = {Meyer, Karin},
	year = {2017},
	keywords = {Research Design, Chromosome, Covariance components, DNA content, Estimation, flow cytometry, Gene Expression Profiling, Gene Expression Profiling: methods, Genes, Genes: physiology, genome size, grasspea, karyotype, lathyrus sativus, Mixed Model, Nucleic Acids, Nucleic Acids: isolation \& purification, Quality Control, Reference Standards, REML, Reproducibility of Results, Research Design: standards, Reverse Transcriptase Polymerase Chain Reaction, Reverse Transcriptase Polymerase Chain Reaction: s, Software, Statistics as Topic, Workflow},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\E8NH2F3U\\WombatManual.pdf:application/pdf},
}

@article{June2013,
	title = {‘{Single}-step’ genetic evaluation in {WOMBAT} 1},
	author = {June, M},
	year = {2013},
	pages = {1--4},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\4YQ76AGW\\RNote_WOMBATS1Step.pdf:application/pdf},
}

@article{Model2013,
	title = {Workshop \# 2},
	number = {April},
	author = {Model, Conceptual},
	year = {2013},
	pages = {1--9},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\7YMYBM22\\Workshop2 jbuc045.pdf:application/pdf},
}

@misc{Lecturea,
	title = {Hardy-{Weinberg} {Equilibrium}},
	author = {Lecture, BIOINF 703},
	note = {Pages: 1-8},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\EU6X74AP\\9 HardyWeinberg1.pdf:application/pdf},
}

@article{August2012,
	title = {Estimating “ social ” genetic e ff ects using {WOMBAT}},
	author = {August, M},
	year = {2012},
	pages = {1--4},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\VMH2F47T\\RNote_WOMBATSocial.pdf:application/pdf},
}

@incollection{April2012,
	title = {Pooling estimates of covariance components by penalized maximum likelihood using {WOMBAT} {Background} : {The} likelihood approach},
	author = {April, M},
	year = {2012},
	pages = {1--13},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\9CRQX4IW\\RNote_WOMBATPool.pdf:application/pdf},
}

@book{M2011,
	title = {‘{Automatic}’ {GWAS} analyses using {WOMBAT}},
	isbn = {1211001011011},
	author = {M, Karrin},
	year = {2011},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\R9EPIVDF\\RNote_WOMBATSnappy.pdf:application/pdf},
}

@article{Reticulation2005,
	title = {Phylogenetic networks {II} : algorithmic aspects {The} {Minimum} {Hybridisation} problem},
	author = {Reticulation, Minimum},
	year = {2005},
	pages = {1--7},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\4SG7BMFZ\\7 Networks2.pdf:application/pdf},
}

@article{Browning2011,
	title = {Haplotype phasing: {Existing} methods and new developments},
	volume = {12},
	issn = {14710056},
	doi = {10.1038/nrg3054},
	abstract = {Determination of haplotype phase is becoming increasingly important as we enter the era of large-scale sequencing because many of its applications, such as imputing low-frequency variants and characterizing the relationship between genetic variation and disease susceptibility, are particularly relevant to sequence data. Haplotype phase can be generated through laboratory-based experimental methods, or it can be estimated using computational approaches. We assess the haplotype phasing methods that are available, focusing in particular on statistical methods, and we discuss the practical aspects of their application. We also describe recent developments that may transform this field, particularly the use of identity-by-descent for computational phasing.},
	number = {10},
	journal = {Nature Reviews Genetics},
	author = {Browning, Sharon R. and Browning, Brian L.},
	year = {2011},
	pmid = {21921926},
	note = {ISBN: 1471-0064 (Electronic){\textbackslash}r1471-0056 (Linking)},
	pages = {703--714},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\YYH89BR5\\18 lec09_haplotyping.pdf:application/pdf},
}

@article{Lecture,
	title = {1 {Mathematical}},
	volume = {1},
	number = {2},
	author = {Lecture, BIOINF 703},
	pages = {1--3},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\S6DT6BMJ\\1A Mathematical Induction.pdf:application/pdf},
}

@incollection{Kitching1998,
	title = {Consensus trees},
	booktitle = {Cladistics: the theory and practice of parsimony analysis.},
	author = {Kitching, Ian J. and Forey, Peter L. and Humphries, Christopher J. and Williams, David M.},
	year = {1998},
	pages = {139--150},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\SZTKYPMH\\4 ConsensusTrees.pdf:application/pdf},
}

@article{Price2006,
	title = {Principal components analysis corrects for stratification in genome-wide association studies},
	volume = {38},
	issn = {10614036},
	doi = {10.1038/ng1847},
	abstract = {Population stratification--allele frequency differences between cases and controls due to systematic ancestry differences-can cause spurious associations in disease studies. We describe a method that enables explicit detection and correction of population stratification on a genome-wide scale. Our method uses principal components analysis to explicitly model ancestry differences between cases and controls. The resulting correction is specific to a candidate marker's variation in frequency across ancestral populations, minimizing spurious associations while maximizing power to detect true associations. Our simple, efficient approach can easily be applied to disease studies with hundreds of thousands of markers.},
	number = {8},
	journal = {Nature Genetics},
	author = {Price, Alkes L. and Patterson, Nick J. and Plenge, Robert M. and Weinblatt, Michael E. and Shadick, Nancy A. and Reich, David},
	year = {2006},
	pmid = {16862161},
	note = {ISBN: 1061-4036 (Print){\textbackslash}r1061-4036 (Linking)},
	pages = {904--909},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\DSBSW8CH\\price 2006 Principal components analysis corrects for stratification in genome-wide association studies.pdf:application/pdf},
}

@article{Pickrell2016,
	title = {Detection and interpretation of shared genetic influences on 42 human traits},
	volume = {48},
	issn = {15461718},
	url = {http://dx.doi.org/10.1038/ng.3570},
	doi = {10.1038/ng.3570},
	abstract = {We performed a genome-wide scan for genetic variants that influence multiple human phenotypes by comparing large genome-wide association studies (GWAS) of 40 traits or diseases, including anthropometric traits (e.g. nose size and male pattern baldness), immune traits (e.g. susceptibility to childhood ear infections and Crohn's disease), metabolic phenotypes (e.g. type 2 diabetes and lipid levels), and psychiatric diseases (e.g. schizophrenia and Parkinson's disease). First, we identified 307 loci (at a false discovery rate of 10\%) that influence multiple traits (excluding “trivial” phenotype pairs like type 2 diabetes and fasting glucose). Several loci influence a large number of phenotypes; for example, variants near the blood group gene ABO influence eleven of these traits, including risk of childhood ear infections (rs635634: log-odds ratio = 0.06, P = 1.4 × 10−8) and allergies (log-odds ratio = 0.05, P = 2.5 × 10−8), among others. Similarly, a nonsynonymous variant in the zinc transporter SLC39A8 influences seven of these traits, including risk of schizophrenia (rs13107325: log-odds ratio = 0.15, P = 2 × 10−12) and Parkinson’s disease (log-odds ratio = -0.15, P = 1.6 × 10−7), among others. Second, we used these loci to identify traits that share multiple genetic causes in common. For example, genetic variants that delay age of menarche in women also, on average, delay age of voice drop in men, decrease body mass index (BMI), increase adult height, and decrease risk of male pattern baldness. Finally, we identified four pairs of traits that show evidence of a causal relationship. For example, we show evidence that increased BMI causally increases triglyceride levels, and that increased liability to hypothyroidism causally decreases adult height.},
	number = {7},
	journal = {Nature Genetics},
	author = {Pickrell, Joseph K. and Berisa, Tomaz and Liu, Jimmy Z. and Ségurel, Laure and Tung, Joyce Y. and Hinds, David A.},
	year = {2016},
	pmid = {27182965},
	note = {arXiv: 15334406
Publisher: Nature Publishing Group
ISBN: 9780128000977},
	pages = {709--717},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\G7UMBTUP\\pickerell-2016-shared-genetic-influences.pdf:application/pdf},
}

@article{Slatkin2008,
	title = {Linkage disequilibrium - {Understanding} the evolutionary past and mapping the medical future},
	volume = {9},
	issn = {14710056},
	doi = {10.1038/nrg2361},
	abstract = {Linkage disequilibrium--the nonrandom association of alleles at different loci--is a sensitive indicator of the population genetic forces that structure a genome. Because of the explosive growth of methods for assessing genetic variation at a fine scale, evolutionary biologists and human geneticists are increasingly exploiting linkage disequilibrium in order to understand past evolutionary and demographic events, to map genes that are associated with quantitative characters and inherited diseases, and to understand the joint evolution of linked sets of genes. This article introduces linkage disequilibrium, reviews the population genetic processes that affect it and describes some of its uses. At present, linkage disequilibrium is used much more extensively in the study of humans than in non-humans, but that is changing as technological advances make extensive genomic studies feasible in other species.},
	number = {6},
	journal = {Nature Reviews Genetics},
	author = {Slatkin, Montgomery},
	year = {2008},
	pmid = {18427557},
	note = {ISBN: 1471-0064 (Electronic){\textbackslash}r1471-0056 (Linking)},
	pages = {477--485},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\2SU9QJNU\\13 2008_Slatkin_LinkageDisequilibrium.pdf:application/pdf},
}

@article{Drummond,
	title = {Genome editing to the fittest / healthiest},
	author = {Drummond, Alexei J.},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\3I6T3GDE\\12 Genome editing to the fittest_healthiest.pdf:application/pdf},
}

@book{IlluminaInc.2015,
	title = {{ForenSeq}™ {DNA} {Signature} {Prep} {Reference} {Guide}},
	isbn = {1-000-00000-1},
	author = {{Illumina Inc.}},
	year = {2015},
	doi = {# TG-450-9001DOC Material # 20000923 Document # 15049528 v01},
	note = {Issue: September},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\CIVH6VYI\\forenseq-dna-signature-prep-guide-15049528-01.pdf:application/pdf},
}

@article{IlluminaInc.2016,
	title = {Nextera® {XT} {Library} {Prep} {Reference} {Guide}},
	url = {http://support.illumina.com/downloads/nextera_xt_sample_preparation_guide_15031942.html},
	doi = {FC-121-9006DOC},
	number = {January},
	journal = {Sample Preparation Guide},
	author = {{Illumina Inc.}},
	year = {2016},
	pages = {1--28},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\6NZ9ZZK3\\nextera-xt-library-prep-guide-15031942-01.pdf:application/pdf},
}

@article{Shane2016,
	title = {Neutral theory and the molecular clock},
	volume = {2016},
	number = {2004},
	author = {Shane, Coordinator and Klaere, Steffen and Lavery, Shane},
	year = {2016},
	file = {Bio733_16_Course Handbook-5.pdf:C\:\\Users\\jstacey\\Zotero\\storage\\VW2AVJ43\\Bio733_16_Course Handbook-5.pdf:application/pdf;Bio733_16_Course Handbook-6.pdf:C\:\\Users\\jstacey\\Zotero\\storage\\27UC5WLN\\Bio733_16_Course Handbook-6.pdf:application/pdf;Bio733_18_Course Handbook-1.pdf:C\:\\Users\\jstacey\\Zotero\\storage\\VL2ZQB5N\\Bio733_18_Course Handbook-1.pdf:application/pdf},
}

@article{RStudio2015,
	title = {{GGplot2} cheat sheet},
	issn = {00978493},
	doi = {10.1016/S0097-8493(02)00051-1},
	author = {{R Studio}},
	year = {2015},
	note = {ISBN: 9780470258866},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\R4SHZ5UL\\m-api-4d3959f4-ae72-09f1-69b2-90c3025bddd9.pdf:application/pdf},
}

@article{Stekhoven2012,
	title = {Missforest-{Non}-parametric missing value imputation for mixed-type data},
	volume = {28},
	issn = {13674803},
	doi = {10.1093/bioinformatics/btr597},
	abstract = {Modern data acquisition based on high-throughput technology is often facing the problem of missing data. Algorithms commonly used in the analysis of such large-scale data often depend on a complete set. Missing value imputation offers a solution to this problem. However, the majority of available imputation methods are restricted to one type of variable only: continuous or categorical. For mixed-type data the different types are usually handled separately. Therefore, these methods ignore possible relations between variable types. We propose a nonparametric method which can cope with different types of variables simultaneously. We compare several state of the art methods for the imputation of missing values. We propose and evaluate an iterative imputation method (missForest) based on a random forest. By averaging over many unpruned classification or regression trees random forest intrinsically constitutes a multiple imputation scheme. Using the built-in out-of-bag error estimates of random forest we are able to estimate the imputation error without the need of a test set. Evaluation is performed on multiple data sets coming from a diverse selection of biological fields with artificially introduced missing values ranging from 10\% to 30\%. We show that missForest can successfully handle missing values, particularly in data sets including different types of variables. In our comparative study missForest outperforms other methods of imputation especially in data settings where complex interactions and nonlinear relations are suspected. The out-of-bag imputation error estimates of missForest prove to be adequate in all settings. Additionally, missForest exhibits attractive computational efficiency and can cope with high-dimensional data.},
	number = {1},
	journal = {Bioinformatics},
	author = {Stekhoven, Daniel J. and Bühlmann, Peter},
	year = {2012},
	pmid = {22039212},
	note = {arXiv: 1105.0828
ISBN: 1367-4811 (Electronic){\textbackslash}n1367-4803 (Linking)},
	pages = {112--118},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\83EFDJS4\\Stekhoven.pdf:application/pdf},
}

@phdthesis{Albani2017,
	title = {Enhancement of {mRNA}-{Based} {Methods} for {Body} {Fluid} and {Celltype} {Identification}},
	author = {Albani, Patricia Pearl},
	year = {2017},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\TFM98MAN\\FINAL DRAFT 20170626.pdf:application/pdf},
}

@article{Stekhoven2011,
	title = {Using the {missForest} {Package}},
	author = {Stekhoven, Daniel J},
	year = {2011},
	pages = {1--11},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\QPLXAKZ8\\missForest.pdf:application/pdf},
}

@article{Lee2017d,
	title = {Lecture 9 : {Non}-linear {Classification} {Rules} {Outline} {Introduction}},
	author = {Lee, Alan},
	year = {2017},
	pages = {1--45},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\9VULJK83\\9 Non-linear classification methods.pdf:application/pdf},
}

@article{Sheet2016,
	title = {R {Markdown} : : {CHEAT} {SHEET}},
	volume = {5},
	author = {Sheet, Cheat},
	year = {2016},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\XKCQBVBE\\rmarkdown-2.0.pdf:application/pdf},
}

@article{Lee2017g,
	title = {Lecture 8 : {Linear} methods for classification {Outline} {Introduction}},
	author = {Lee, Alan},
	year = {2017},
	pages = {1--30},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\QLEH8Q2I\\8 Linear classification methods.pdf:application/pdf},
}

@article{Buuren2011,
	title = {{MICE} : {Multivariate} {Imputation} by {Chained} {Equations} in {R}},
	volume = {45},
	issn = {1548-7660},
	url = {http://www.jstatsoft.org/v45/i03/},
	doi = {10.18637/jss.v045.i03},
	abstract = {The R package mice imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice 2.9, which extends the functionality of mice 1.0 in several ways. In mice 2.9, the analysis of imputed data is made completely general, whereas the range ofmodels under which pooling works is substantially extended. mice 2.9 adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. mice 2.9 can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.},
	number = {3},
	journal = {Journal of Statistical Software},
	author = {Buuren, Stef van and Groothuis-Oudshoorn, Karin},
	year = {2011},
	pmid = {22289957},
	note = {arXiv: 1501.0228
ISBN: 9067436771},
	keywords = {chained equations, fully conditional specification, gibbs sampler, mice, multiple imputation, passive imputation, predictor selection, r},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\UZSRM2GA\\mice.pdf:application/pdf},
}

@article{Lee2017c,
	title = {Lecture 7 : {Data} {Preprocessing} and {Feature} {Engineering} {Outline} {Introduction}},
	author = {Lee, Alan},
	year = {2017},
	pages = {1--20},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ZZI7VUNR\\7 Preprocessing and feature engineering.pdf:application/pdf},
}

@article{Talia2016,
	title = {Chapter 1 - {Introduction} to {Data} {Mining}},
	url = {http://www.sciencedirect.com/science/article/pii/B9780128028810000019},
	doi = {http://dx.doi.org/10.1016/B978-0-12-802881-0.00001-9},
	abstract = {Abstract We introduce in this chapter the main concepts of data mining. This scientific field, together with Cloud computing, discussed in Chapter 2, is a basic pillar on which the contents of this book are built. Section 1.1 explores the main notions and principles of data mining introducing readers to this scientific field and giving them the needed information on sequential data mining techniques and algorithms that will be used in other sections and chapters of this book. Section 1.2 outlines the most important parallel and distributed data mining strategies and techniques.},
	number = {July},
	journal = {Data Analysis in the Cloud},
	author = {Talia, Domenico and Trunfio, Paolo and Marozzo, Fabrizio},
	year = {2016},
	note = {ISBN: 978-0-12-802881-0},
	keywords = {association rules, classification, clustering, collective data mining, data mining, distributed data mining, ensemble learning, meta-learning, parallel data mining},
	pages = {1--25},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\57T4IH34\\kdmb-1.pdf:application/pdf},
}

@article{Lee2017f,
	title = {Lecture 6 : {Boosting} and {Bagging} {Outline} {Introduction}},
	author = {Lee, Alan},
	year = {2017},
	pages = {1--26},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\TFMX8XDE\\6 Boosting and Bagging.pdf:application/pdf},
}

@article{Hofner2012,
	title = {Model-based boosting in {R}},
	issn = {0943-4062},
	doi = {10.1007/s00180-012-0382-5.The},
	abstract = {Data 19 503 observations of the following covariates: 1. damage–response variable, no= healthy, yes= damaged tree. 2. year–ordered factor with levels 1991 and 2002. 3. species– factor with four levels (beech, fir, rowan, sycamore). 4. height–one-dimensional ... {\textbackslash}n},
	journal = {Imbe.Med.Uni-Erlangen.De},
	author = {Hofner, Benjamin and Mayr, Andreas and Robinzonov, Nikolay and Schmid, Matthias},
	year = {2012},
	note = {ISBN: 0018001203825},
	pages = {1--26},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\Y7XQ2NCG\\m-api-580e688a-0d6e-f97f-4e99-b73e6d3bee41.pdf:application/pdf},
}

@article{Kuhn2008,
	title = {caret {Package}},
	volume = {28},
	issn = {15487660},
	url = {http://www.jstatsoft.org/v28/i05/paper},
	abstract = {The caret package, short for classification and regression training, contains numerous tools for developing predictive models using the rich set of models available in R. The package focuses on simplifying model training and tuning across a wide variety of modeling techniques. It also includes methods for pre-processing training data, calculating variable importance, and model visualizations. An example from computational chemistry is used to illustrate the functionality on a real data set and to benchmark the benefits of parallel processing with several types of models.},
	number = {5},
	journal = {Journal Of Statistical Software},
	author = {Kuhn, Max},
	year = {2008},
	keywords = {r, model building, networkspaces, parallel processing, tuning parameters},
	pages = {1--26},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\HWEHTC4C\\caret.pdf:application/pdf},
}

@article{Lee2017h,
	title = {Lecture 11 : {Imputation} {Outline} {Introduction} {Iris} data},
	author = {Lee, Alan},
	year = {2017},
	pages = {1--27},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\TJ3Q7M7V\\11 Imputation.pdf:application/pdf},
}

@article{Lee2017e,
	title = {Lecture 1 : {What} is {Data} {Mining} ? {Outline} {Housekeeping}},
	author = {Lee, Alan},
	year = {2017},
	pages = {1--34},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\LI59TQR4\\1 Introduction.pdf:application/pdf},
}

@article{Lee2017,
	title = {Lecture 10 : {Regularization} {Outline} {Introduction} {Lasso}},
	author = {Lee, Alan},
	year = {2017},
	pages = {1--28},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\RDWD8TC8\\10 Regularisation.pdf:application/pdf},
}

@book{Lee2017a,
	title = {Lecture 5 : {Non}-{Linear} {Prediction} {Methods} ( {Cont} ) {Outline} {Introduction}},
	isbn = {7-900875-79-4},
	author = {Lee, Alan},
	year = {2017},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\BEB8BENA\\5 Non-linear methods part 2.pdf:application/pdf},
}

@article{Lee2017b,
	title = {Lecture 4 : {Non}-{Linear} {Prediction} {Methods} {Outline} {Introduction} {Smoothing} {Models}},
	author = {Lee, Alan},
	year = {2017},
	pages = {1--30},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\8QYBZHQ3\\4 Non-linear methods.pdf:application/pdf},
}

@article{Lee2017i,
	title = {Lecture 3 : {Linear} methods for prediction ( cont ) {Outline} {Introduction} {Cross}-validation},
	author = {Lee, Alan},
	year = {2017},
	pages = {1--34},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\AAVLA9DG\\3 Linear methods part2.pdf:application/pdf},
}

@article{Lee2017j,
	title = {Lecture 2 : {Linear} methods for prediction {Outline} {Introduction} {Linear} predictors {Prediction} error},
	author = {Lee, Alan},
	year = {2017},
	pages = {1--23},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\FVW6RLGS\\2 Linear methods.pdf:application/pdf},
}

@article{DepartmentofStatisticsUOA2017,
	title = {{STATS} 784 {Statistical} {Data} {Mining} {Assignment} 4},
	number = {1},
	author = {{Department of Statistics UOA}},
	year = {2017},
	pages = {6--7},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\629VKKIC\\assign4_2017_784.pdf:application/pdf},
}

@article{Rathjen2013,
	title = {Question 1 {Question} 1 {Question} 2},
	number = {x},
	author = {Rathjen, Philipp},
	year = {2013},
	note = {ISBN: 9210125150},
	pages = {3002},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\EQY5LSZL\\JStacey 784 Assignment 2.pdf:application/pdf},
}

@article{Rizki2018,
	title = {Question 1},
	volume = {580},
	number = {March},
	author = {Rizki, Muhammad Agus},
	year = {2018},
	pages = {1--8},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\3JMR5DM6\\STATS784 J Stacey Assignment 3.pdf:application/pdf},
}

@article{Mining2015,
	title = {{STATS} 784 {SC} {Terms} {Test} {Instructions} : {Surname} : {First} name :},
	author = {Mining, Statistical Data},
	year = {2015},
	pages = {1--10},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\PJ56GQWY\\tt2015.pdf:application/pdf},
}

@article{County2017,
	title = {Department of {Statistics} {STATS} 784 : {Data} {Mining} {Consult} the references in {Lecture} 1 and identify and describe two applications of data {The} dataset for this question contains house sale prices for {King} {County} , {Washington} function train in the caret packa},
	author = {County, King and State, Washington},
	year = {2017},
	pages = {1--2},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\HD2JA6R6\\784Ass1.2017.pdf:application/pdf},
}

@book{Hastie2009,
	title = {The {Elements} of {Statistical} {Learning}},
	isbn = {978-0-387-84857-0},
	abstract = {During the past decade there has been an explosion in computation and information tech-nology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting—the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for " wide " data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2009},
	pmid = {15512507},
	doi = {10.1007/978-0-387-84858-7},
	note = {arXiv: 1010.3003
Publication Title: Elements of Statistical Learning
ISSN: 0964-1998},
	keywords = {.-{\textgreater}ZXCaszxdcfvgbhnm, .-gfvvbvfvvvbnbnnmhgbvxcc, .-mm{\textless}asaassasa{\textless}zxasaAASDFGHJCVGFDFDFGHJKJJJBNMKLÆN, .0441, ', ¨'pæh4441æh542266455577747451010774778749988, +, 0, øøASDFGHJNM, q11+p´´+pååå897eawer5689+{\textgreater}aA{\textless}},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\SJN56MZ3\\m-api-788e42ff-011b-a8b3-1187-cc834acc1e4d.pdf:application/pdf},
}

@article{Taurus2009,
	title = {Chapter 9; {Neural} {Networks}},
	volume = {7},
	doi = {10.1016/B978-0-12-373580-5.50043-0},
	author = {Taurus, Bos},
	year = {2009},
	note = {ISBN: 9781617790959},
	pages = {187--208},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ANSX62WZ\\ch9.pdf:application/pdf},
}

@incollection{Larose2014b,
	title = {Index},
	isbn = {978-0-471-68753-5},
	booktitle = {Discovering {Knowledge} in {Data}: {An} {Introduction} to {Data} {Mining}},
	author = {Larose, Daniel T. and Larose, Chantal D.},
	year = {2014},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\GBKTFB7W\\index.pdf:application/pdf},
}

@incollection{Larose,
	title = {Preface; {KNOWLEDGE} {IN} {DATA} {WILEY} {SERIES} {ON} {METHODS} {AND} {APPLICATIONS}},
	author = {Larose, Daniel T.},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\9PBA9WWF\\fmatter.pdf:application/pdf},
}

@article{Venables2010,
	title = {An {Introduction} to {R}},
	volume = {57},
	issn = {00652881},
	doi = {10.1016/B978-0-12-381308-4.00001-7},
	abstract = {This chapter provides a background to research on Northern krill biology, starting with a description of its morphology and identifying features, and the historical path to its eventual position as a single-species genus. There is a lack of any euphausiid fossil material, so phylogenetic analysis has relied on comparative morphology and ontogeny and, more recently, genetic methods. Although details differ, the consensus of these approaches is that Meganyctiphanes is most closely related to the genus Thysanoessa. The light organs (or photophores) are well developed in Northern krill and the control of luminescence in these organs is described. A consideration of the distribution of the species shows that it principally occupies shelf and slope waters of both the western and eastern coasts of the North Atlantic, with a southern limit at the boundary with sub-tropical waters (plus parts of the Mediterranean) and a northern limit at the boundary with Arctic water masses. Recent evidence of a northward expansion of these distributional limits is considered further. There have been a variety of techniques used to sample and survey Northern krill populations for a variety of purposes, which this chapter collates and assesses in terms of their effectiveness. Northern krill play an important ecological role, both as a contributor to the carbon pump through the transport of faecal material to the deeper layers, and as a key prey item for groundfish, squid, baleen whales, and seabirds. The commercial exploitation of Northern krill has been slow to emerge since its potential was considered by Mauchline [Mauchline, J (1980). The biology of mysids and euphausiids. Adv. Mar. Biol. 18, 1-681]. However, new uses for products derived from krill are currently being found, which may lead to a new wave of exploitation. © 2010 Elsevier Ltd.},
	number = {C},
	journal = {Advances in Marine Biology},
	author = {Venables, W.N. and Smith, D.M.},
	year = {2010},
	pmid = {20955887},
	note = {arXiv: 1011.1669v3
ISBN: 9780123813084},
	pages = {1--40},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\UNJEDHKB\\R-intro.pdf:application/pdf},
}

@article{DeJonge2013,
	title = {An introduction to data cleaning with {R}},
	issn = {1572-0314},
	doi = {60083 201313- X-10-13},
	abstract = {Data cleaning, or data preparation is an essential part of statistical analysis. In fact, in practice it is often more time-consuming than the statistical analysis itself. These lecture notes describe a range of techniques, implemented in the R statistical environment, that allow the reader to build data cleaning scripts for data suffering from a wide range of errors and inconsistencies, in textual format. These notes cover technical as well as subject-matter related aspects of data cleaning. Technical aspects include data reading, type conversion and string matching and manipulation. Subject-matter related aspects include topics like data checking, error localization and an introduction to imputation methods in R. References to relevant literature and R packages are provided throughout.},
	journal = {Statistics Netherlands},
	author = {de Jonge, Edwin and van der Loo, Mark},
	year = {2013},
	note = {ISBN: 1572-0314},
	keywords = {data editing, methodology, statistical software},
	pages = {53},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\94TWFIK2\\m-api-27fcc1bf-b899-5f19-6fc2-bdee634c27ad.pdf:application/pdf},
}

@book{Gareth2013,
	title = {An {Introduction} to {Statistical} {Learning}},
	isbn = {978-0-387-78188-4},
	abstract = {Review From the reviews: .,."There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006 "The reader sees not only how measure theory is used to develop probability theory, but also how probability theory is used in applications. a The discourse is delivered in a theorem proof format and thus is better suited for classroom a . The authors prose is generally well thought out a . will make an attractive choice for a two-semester course on measure and probability, or as a second course for students with a semester of measure or probability theory under their belt." (Peter C. Kiessler, Journal of the American Statistical Association, Vol. 102 (479), 2007) "The book is a well written self-contained textbook on measure and probability theory. It consists of 18 chapters. Every chapter contains many well chosen examples and ends with several problems related to the earlier developed theory (some with hints). a At the very end of the book there is an appendix collecting necessary facts from set theory, calculus and metric spaces. The authors suggest a few possibilities on how to use their book." (Kazimierz Musial, Zentralblatt MATH, Vol. 1125 (2), 2008) "The title of the book consists of the names of its two basic parts. The booka (TM)s third part is comprised of some special topics from probability theory. a The authors suggest using the book intwo-semester graduate programs in statistics or a one-semester seminar on special topics. The material of the book is standard a is clear, comprehensive and a {\textasciitilde}without being intimidatinga (TM)." (Rimas NorvaiAa, Mathematical Reviews, Issue 2007 f) Product Description This is a graduate level textbook on measure theory and probability theory. The book can be used as a text for a two semester sequence of courses in measure theory and probability theory, with an option to include supplemental material on stochastic processes and special topics. It is intended primarily for first year Ph.D. students in mathematics and statistics although mathematically advanced students from engineering and economics would also find the book useful. Prerequisites are kept to the minimal level of an understanding of basic real analysis concepts such as limits, continuity, differentiability, Riemann integration, and convergence of sequences and series. A review of this material is included in the appendix. The book starts with an informal introduction that provides some heuristics into the abstract concepts of measure and integration theory, which are then rigorously developed. The first part of the book can be used for a standard real analysis course for both mathematics and statistics Ph.D. students as it provides full coverage of topics such as the construction of Lebesgue-Stieltjes measures on real line and Euclidean spaces, the basic convergence theorems, L p spaces, signed measures, Radon-Nikodym theorem, Lebesgue's decomposition theorem and the fundamental theorem of Lebesgue integration on R, product spaces and product measures, and Fubini-Tonelli theorems. It also provides an elementary introduction to Banach and Hilbert spaces, convolutions, Fourier series and Fourier and Plancherel transforms. Thus part I would be particularly useful for students in a typical Statistics Ph.D. program if a separate course on real analysis is not a standard requirement. Part II (chapters 6-13) provides full coverage of standard graduate level probability theory. It starts with Kolmogorov's probability model and Kolmogorov's existence theorem. It then treats thoroughly the laws of large numbers including renewal theory and ergodic theorems with applications and then weak convergence of probability distributions, characteristic functions, the Levy-Cramer continuity theorem and the central limit theorem as well as stable laws. It ends with conditional expectations and conditional probability, and an introduction to the theory of discrete time martingales. Part III (chapters 14-18) provides a modest coverage of discrete time Markov chains with countable and general state spaces, MCMC, continuous time discrete space jump Markov processes, Brownian motion, mixing sequences, bootstrap methods, and branching processes. It could be used for a topics/seminar course or as an introduction to stochastic processes. From the reviews: "...There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006},
	publisher = {Springer},
	author = {Gareth, James and Daniela, Witten and Trevor, Hastie and Tibshirani, Robert},
	year = {2013},
	pmid = {10911016},
	doi = {10.1016/j.peva.2007.06.006},
	note = {arXiv: 1011.1669v3
ISSN: 01621459},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\DVM4H5DG\\Intro to Statistical Learning.pdf:application/pdf},
}

@incollection{Larose2014e,
	title = {Chapter 4 {Univariate} {Statistical} {Analysis}},
	booktitle = {Discovering {Knowledge} in {Data}: {An} {Introduction} to {Data} {Mining}},
	author = {Larose, Daniel T. and Larose, Chantal D.},
	year = {2014},
	pages = {91--108},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\8PXFPKC8\\ch4.pdf:application/pdf},
}

@article{Evaluation2014,
	title = {Chapter 14; {Model} {Evaluation} {Techniques}},
	author = {Evaluation, Model},
	year = {2014},
	pages = {277--293},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\7IZQZTRJ\\ch14.pdf:application/pdf},
}

@incollection{Larose2014d,
	title = {Chapter 8 {Decision} {Trees}},
	booktitle = {Discovering {Knowledge} in {Data}: {An} {Introduction} to {Data} {Mining}},
	author = {Larose, Daniel T. and Larose, Chantal D.},
	year = {2014},
	pages = {165--186},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\MASQDAHB\\ch8.pdf:application/pdf},
}

@incollection{Larose2014a,
	title = {Chapter 3 {Exploratory} {Data} {Analysis}},
	booktitle = {Discovering {Knowledge} in {Data}: {An} {Introduction} to {Data} {Mining}},
	author = {Larose, Daniel T. and Larose, Chantal D.},
	year = {2014},
	pages = {51--90},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\7Y2VDMLK\\ch3.pdf:application/pdf},
}

@incollection{Larose2014h,
	title = {Chapter 2 {Data} {Preprocessing}},
	isbn = {978-0-585-18187-5},
	booktitle = {Discovering {Knowledge} in {Data}: {An} {Introduction} to {Data} {Mining}},
	author = {Larose, Daniel T. and Larose, Chantal D.},
	year = {2014},
	pmid = {21728113},
	doi = {10.1111/j.1600-0404.1995.tb01704.x},
	note = {ISSN: 1422-6405},
	pages = {1--30},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\33DHPITD\\ch2.pdf:application/pdf},
}

@incollection{Larose2014f,
	title = {Chapter 6 {Preparing} to {Model} {The} {Data}},
	booktitle = {Discovering {Knowledge} in {Data}: {An} {Introduction} to {Data} {Mining}},
	author = {Larose, Daniel T. and Larose, Chantal D.},
	year = {2014},
	pages = {138--148},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\E2LMZR85\\ch6.pdf:application/pdf},
}

@article{Modeling2001,
	title = {Chapter 5; {Multivariate} {Statistics}},
	doi = {10.1227/01.NEU.0000028161.91504.4F},
	author = {Modeling, Nonlinear},
	year = {2001},
	note = {ISBN: 9789966029171},
	pages = {109--137},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\DR7RKMLF\\ch5.pdf:application/pdf},
}

@article{Task2014,
	title = {Chapter 7; k-{Nearest} {Neighbor} {Algorithm}},
	author = {Task, Classification},
	year = {2014},
	pages = {149--164},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\DSQKN7KK\\ch7.pdf:application/pdf},
}

@incollection{Larose2014g,
	title = {Chapter 1 {An} {Introduction} {To} {Data} {Mining}},
	booktitle = {Discovering {Knowledge} in {Data}: {An} {Introduction} to {Data} {Mining}},
	author = {Larose, Daniel T. and Larose, Chantal D.},
	year = {2014},
	pages = {1--15},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\TRK74T6R\\ch1.pdf:application/pdf},
}

@incollection{Dean2014u,
	title = {References},
	isbn = {978-1-118-61804-2},
	url = {http://www.amazon.com/Data-Mining-Machine-Learning-Practitioners/dp/1118618041},
	abstract = {With big data analytics comes big insights into profitabilityBig data is big business. But having the data and the computational power to process it isn't nearly enough to produce meaningful results. Big Data, Data Mining, and Machine Learning: Value Creation for Business Leaders and Practitioners is a complete resource for technology and marketing executives looking to cut through the hype and produce real results that hit the bottom line. Providing an engaging, thorough overview of the current state of big data analytics and the growing trend toward high performance computing architectures, the book is a detail-driven look into how big data analytics can be leveraged to foster positive change and drive efficiency.With continued exponential growth in data and ever more competitive markets, businesses must adapt quickly to gain every competitive advantage available. Big data analytics can serve as the linchpin for initiatives that drive business, but only if the underlying technology and analysis is fully understood and appreciated by engaged stakeholders. This book provides a view into the topic that executives, managers, and practitioners require, and includes:A complete overview of big data and its notable characteristicsDetails on high performance computing architectures for analytics, massively parallel processing (MPP), and in-memory databasesComprehensive coverage of data mining, text analytics, and machine learning algorithmsA discussion of explanatory and predictive modeling, and how they can be applied to decision-making processesBig Data, Data Mining, and Machine Learning provides technology and marketing executives with the complete resource that has been notably absent from the veritable libraries of published books on the topic. Take control of your organization's big data analytics to produce real results with a resource that is comprehensive in scope and light on hyperbole.},
	booktitle = {Big {Data}, {Data} {Mining} and {Machine} {Learning}. {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	doi = {10.1126/science.1247727},
	pages = {265},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\3BYBPKIS\\refs.pdf:application/pdf},
}

@article{Of2014,
	title = {Chapter 13; {Imputation} of {Missing} {Data}},
	author = {Of, Imputation and Data, Missing},
	year = {2014},
	pages = {266--276},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\IVMD6KN2\\ch13.pdf:application/pdf},
}

@incollection{Larose2014c,
	title = {Appendex {Data} {Summarization} {And} {Visualization}},
	booktitle = {Discovering {Knowledge} in {Data}: {An} {Introduction} to {Data} {Mining}},
	author = {Larose, Daniel T. and Larose, Chantal D.},
	year = {2014},
	pages = {294--307},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\LDMAXXA8\\app1.pdf:application/pdf},
}

@article{Networks2014,
	title = {Chapter 11; {Kohonen} networks},
	author = {Networks, Kohonen},
	year = {2014},
	pages = {228--246},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\WJE3SCG5\\ch11.pdf:application/pdf},
}

@incollection{Dean2014n,
	title = {Part 3: {Success} {Stories} of {Putting} it all {Together}},
	url = {http://doi.wiley.com/10.1002/9781118691786.part3},
	booktitle = {Big {Data}, {Data} {Mining} and {Machine} {Learning}. {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	doi = {10.1002/9781118691786.part3},
	pages = {193--195},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\TFYQVJU3\\part3.pdf:application/pdf},
}

@incollection{Larose2014,
	title = {Chapter 10 {Hierarchial} {And} k-{Means} {Clustering}},
	booktitle = {Discovering {Knowledge} in {Data}: {An} {Introduction} to {Data} {Mining}},
	author = {Larose, Daniel T. and Larose, Chantal D.},
	year = {2014},
	pages = {209--227},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\2FQE7925\\ch10.pdf:application/pdf},
}

@incollection{Dean2014m,
	title = {Chapter 9 {Recommendation} {Systems}},
	isbn = {1-107-01535-9 978-1-107-01535-7},
	booktitle = {Big {Data}, {Data} {Mining}, and {Machine} {Learning}: {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\5CC34GTJ\\ch9.pdf:application/pdf},
}

@article{Statsoft2011,
	title = {Chapter 12; {Association} {Rules}},
	url = {http://www.statsoft.com/textbook/association-rules/},
	author = {{Statsoft}},
	year = {2011},
	pages = {247--265},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ADSYEPCA\\ch12.pdf:application/pdf},
}

@incollection{Dean2014p,
	title = {Chapter 8 {Time} series data mining},
	abstract = {In almost every scientific field, measurements are performed over time. These observations lead to a collection of organized data called time series. The purpose of time series data mining is to try to extract all meaningful knowledge from the shape of data. Even if humans have a natural capacity to perform these tasks, it remains a complex problem for computers. In this paper we intend to provide a survey of the techniques applied for time series data mining. The first part is devoted to an overview of the tasks that have captured most of the interest of researchers. Considering that in most cases, time series task relies on the same components for implementation, we divide the literature depending on these common aspects, namely representation techniques, distance measures and indexing methods. The study of the relevant literature has been categorized for each individual aspects. Four types of robustness could then be formalized and any kind of distance could then be classified. Finally, the study submit various research trends and avenues that can be explored in the near future. We hope that this paper can provide a broad and deep understanding of the time series data mining research field.},
	booktitle = {Big {Data}, {Data} {Mining}, and {Machine} {Learning}: {Value} {Creation} for {Business} {Leaders} and {Practitioners}.},
	author = {Dean, Jared},
	year = {2014},
	doi = {10.1145/0000000.0000000},
	note = {ISSN: 23344547},
	keywords = {data mining, data indexing, Distance measures, query by content, sequence matching, similarity measures, stream analysis, temporal analysis, Time Series},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\78W8ZI5G\\ch8.pdf:application/pdf},
}

@incollection{Dean2014l,
	title = {7: {Incremental} {Response} {Modeling}},
	url = {http://doi.wiley.com/10.1002/9781118691786.ch7},
	booktitle = {Big {Data}, {Data} {Mining} and {Machine} {Learning}. {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	doi = {10.1002/9781118691786.ch7},
	pages = {141--148},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\3HT7BCTD\\ch7.pdf:application/pdf},
}

@incollection{Dean2014c,
	title = {Ch6 {Segmentation}},
	booktitle = {Big {Data}, {Data} {Mining} and {Machine} {Learning}. {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	pages = {127--139},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\I5HPQSXW\\ch6.pdf:application/pdf},
}

@incollection{Dean2014j,
	title = {Part 2: {Turning} {Data} {Into} {Intelligence}},
	url = {http://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=28017316&site=ehost-live},
	abstract = {The article offers information on data integration, modeling, and analytics as the areas of focus and investment of several Property and Casualty (P\&C) carriers. It is noted that business intelligence platforms, which perform in-depth analysis over a wide range of data sources, have improved continuously over the years. Excellent outputs were seen by carriers after converting data into real insight for product development teams. Moreover, the author also mentioned that the demand to transfer wide and complex data stores that can be accessed faster and flexible will be an exciting challenge to the Property and Casualty insurance's technology staffs.},
	booktitle = {Big {Data}, {Data} {Mining} and {Machine} {Learning}. {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	note = {ISSN: 10540733},
	keywords = {BUSINESS development, BUSINESS enterprises, BUSINESS intelligence, DATA analysis, FUNCTIONAL integration, INFORMATION retrieval, INSURANCE, MANAGEMENT, TECHNOLOGY},
	pages = {53--54},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\7HQF7YAX\\part2.pdf:application/pdf},
}

@article{Wickens1987,
	title = {part1 {The} computing environment},
	volume = {45},
	issn = {00104655},
	doi = {10.1016/0010-4655(87)90135-4},
	number = {1-3},
	journal = {Computer Physics Communications},
	author = {Wickens, Fred},
	year = {1987},
	pages = {1--8},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\FW6HFCZS\\part1.pdf:application/pdf},
}

@incollection{Dean2014k,
	title = {Index},
	isbn = {978-85-7811-079-6},
	abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-α-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 Å for the interface backbone atoms) increased from 21\% with default Glide SP settings to 58\% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63\% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40\% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
	booktitle = {Big {Data}, {Data} {Mining} and {Machine} {Learning}. {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	pmid = {25246403},
	doi = {10.1017/CBO9781107415324.004},
	note = {arXiv: 1011.1669v3
ISSN: 16130073},
	keywords = {Mobile, Named entity disambiguation, Natural language processing, News, Recommender system},
	pages = {ev},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\YN82BW8M\\index.pdf:application/pdf},
}

@incollection{Dean2014e,
	title = {Front {Matter}},
	booktitle = {Big {Data}, {Data} {Mining} and {Machine} {Learning}. {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	pages = {i--xix},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\HCKK3ACS\\fmatter.pdf:application/pdf},
}

@incollection{Dean2014a,
	title = {17: {Looking} to the future},
	isbn = {978-1-315-81871-9},
	booktitle = {Big {Data}, {Data} {Mining} and {Machine} {Learning}. {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	doi = {10.4324/9781315818719},
	pages = {233--241},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\LU3U5RJN\\ch17.pdf:application/pdf},
}

@incollection{Dean2014t,
	title = {16: {Case} {Study} of a {High}‐ {Tech} {Product} {Manufacturer}},
	booktitle = {Big {Data}, {Data} {Mining} and {Machine} {Learning}. {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	pages = {229--232},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\PVMNNC6B\\ch16.pdf:application/pdf},
}

@incollection{Dean2014q,
	title = {14: {Case} {Study} of {Online} {Brand} {Management}},
	booktitle = {Big {Data}, {Data} {Mining} and {Machine} {Learning}. {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	pages = {221--224},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\YRXJEUK9\\ch14.pdf:application/pdf},
}

@incollection{Dean2014h,
	title = {Common {Predictive} {Modeling} {Techniques}},
	isbn = {978-1-118-61804-2},
	booktitle = {Big {Data}, {Data} {Mining} and {Machine} {Learning}. {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	pages = {71--126},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\9PLRZXD9\\ch5.pdf:application/pdf},
}

@incollection{Dean2014g,
	title = {ch3 {Analytical} {Tools}},
	booktitle = {Big {Data}, {Data} {Mining} and {Machine} {Learning}. {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	pages = {43--52},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ZZCYHH4X\\ch3.pdf:application/pdf},
}

@incollection{Dean2014v,
	title = {15: {Case} {Study} of {Mobile} {Application} {Recommendations}},
	booktitle = {Big {Data}, {Data} {Mining} and {Machine} {Learning}. {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	pages = {225--228},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\T8DMF3SX\\ch15.pdf:application/pdf},
}

@incollection{Dean2014,
	title = {2: {Distributed} {System}},
	booktitle = {Big {Data}, {Data} {Mining} and {Machine} {Learning}. {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	note = {Issue: 1},
	pages = {35--41},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\YRU4L7XU\\ch2.pdf:application/pdf},
}

@incollection{Dean2014f,
	title = {ch4 {Predictive} {Modeling}},
	booktitle = {Big {Data}, {Data} {Mining} and {Machine} {Learning}. {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	pages = {233--239},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\FU2ZVS7B\\ch4.pdf:application/pdf},
}

@incollection{Dean2014w,
	title = {Introduction},
	booktitle = {Big {Data}, {Data} {Mining} and {Machine} {Learning}. {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	doi = {10.1002/9781118691786},
	pages = {1--21},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\F92365AP\\ch0.pdf:application/pdf},
}

@incollection{Dean2014o,
	title = {Appendix 1: {Nike}+ fuelband script to retrieve information},
	booktitle = {Big {Data}, {Data} {Mining} and {Machine} {Learning}. {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	pages = {245--246},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\6NJVIZ5G\\app1.pdf:application/pdf},
}

@incollection{Dean2014s,
	title = {12: {Case} {Study} of a {Major} {Health} {Care} {Provider}},
	booktitle = {Big {Data}, {Data} {Mining} and {Machine} {Learning}. {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	pages = {205--214},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\X8ZUALS3\\ch12.pdf:application/pdf},
}

@incollection{Dean2014d,
	title = {Ch 11 {Case} {Study} of a {Large} {U}.{S}. ‐ {Based} {Financial} {Services} {Company}},
	booktitle = {Big {Data}, {Data} {Mining} and {Machine} {Learning}. {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	pages = {197--204},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\4L2UH6KC\\ch11.pdf:application/pdf},
}

@incollection{Dean2014r,
	title = {About the {Author}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780124046481000181},
	booktitle = {Big {Data}, {Data} {Mining}, and {Machine} {Learning}: {Value} {Creation} for {Business} {Leaders} and {Practitioners}.},
	author = {Dean, Jared},
	year = {2014},
	doi = {10.1016/B978-0-12-404648-1.00018-1},
	pages = {xii},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\HWP6VZS8\\about.pdf:application/pdf},
}

@article{Eddy2004,
	title = {How do {RNA} folding algorithms work?},
	volume = {22},
	issn = {10870156},
	doi = {10.1038/nbt1104-1457},
	abstract = {Programs such as MFOLD and ViennaRNA are widely used to predict RNA secondary structures. How do these algorithms work? Why can't they predict RNA pseudoknots? How accurate are they, and will they get better?},
	number = {11},
	journal = {Nature Biotechnology},
	author = {Eddy, Sean R.},
	year = {2004},
	pmid = {15529172},
	note = {ISBN: 1087-0156},
	pages = {1457--1458},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\45KW4B9H\\Eddy04-RNA.pdf:application/pdf},
}

@incollection{Dean2014i,
	title = {13: {Case} {Study} of a {Technology} {Manufacturer}},
	booktitle = {Big {Data}, {Data} {Mining} and {Machine} {Learning}. {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	pages = {215--219},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\SETZVTG3\\ch13.pdf:application/pdf},
}

@incollection{Dean2014b,
	title = {ch10 {Text} {Analytics}},
	booktitle = {Big {Data}, {Data} {Mining} and {Machine} {Learning}. {Value} {Creation} for {Business} {Leaders} and {Practitioners}},
	author = {Dean, Jared},
	year = {2014},
	pages = {175--191},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ICEA6E5A\\ch10.pdf:application/pdf},
}

@article{Eddy2004b,
	title = {What is dynamic programming?},
	volume = {22},
	issn = {1087-0156},
	url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:What+is+dynamic+programming?#0},
	doi = {10.1038/nbt0704-909},
	abstract = {Don't expect much enlightenment from the etymology of the term 'dynamic programming,' though. Dynamic programming was formalized in the early 1950s by mathematician Richard Bellman, who was working at RAND Corporation on},
	number = {7},
	journal = {Nature biotechnology},
	author = {Eddy, Sean R.},
	year = {2004},
	pmid = {15229554},
	note = {ISBN: 1087-0156},
	pages = {909--910},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\EPDWE9SZ\\Eddy04-DP.pdf:application/pdf},
}

@article{Eddy2004c,
	title = {Where did the {BLOSUM62} alignment score matrix come from?},
	volume = {22},
	issn = {10870156},
	doi = {10.1038/nbt0804-1035},
	abstract = {Many sequence alignment programs use the BLOSUM62 score matrix to score pairs of aligned residues. Where did BLOSUM62 come from?},
	number = {8},
	journal = {Nature Biotechnology},
	author = {Eddy, Sean R.},
	year = {2004},
	pmid = {15286655},
	note = {ISBN: 1087-0156 (Print){\textbackslash}r1087-0156 (Linking)},
	pages = {1035--1036},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ZPQVH2MG\\Eddy04-blosum.pdf:application/pdf},
}

@article{Drummond2006,
	title = {Relaxed phylogenetics and dating with confidence},
	volume = {4},
	issn = {15457885},
	doi = {10.1371/journal.pbio.0040088},
	abstract = {In phylogenetics, the unrooted model of phylogeny and the strict molecular clock model are two extremes of a continuum. Despite their dominance in phylogenetic inference, it is evident that both are biologically unrealistic and that the real evolutionary process lies between these two extremes. Fortunately, intermediate models employing relaxed molecular clocks have been described. These models open the gate to a new field of "relaxed phylogenetics." Here we introduce a new approach to performing relaxed phylogenetic analysis. We describe how it can be used to estimate phylogenies and divergence times in the face of uncertainty in evolutionary rates and calibration times. Our approach also provides a means for measuring the clocklikeness of datasets and comparing this measure between different genes and phylogenies. We find no significant rate autocorrelation among branches in three large datasets, suggesting that autocorrelated models are not necessarily suitable for these data. In addition, we place these datasets on the continuum of clocklikeness between a strict molecular clock and the alternative unrooted extreme. Finally, we present analyses of 102 bacterial, 106 yeast, 61 plant, 99 metazoan, and 500 primate alignments. From these we conclude that our method is phylogenetically more accurate and precise than the traditional unrooted model while adding the ability to infer a timescale to evolution.},
	number = {5},
	journal = {PLoS Biology},
	author = {Drummond, Alexei J. and Ho, Simon Y.W. and Phillips, Matthew J. and Rambaut, Andrew},
	year = {2006},
	pmid = {16683862},
	note = {ISBN: 1544-9173},
	pages = {699--710},
	file = {Drummond et al 2006.PDF:C\:\\Users\\jstacey\\Zotero\\storage\\65GIVFFJ\\Drummond et al 2006.PDF:application/pdf},
}

@article{Easteal1992,
	title = {A {Mammalian} {Molecular} {Clock}?},
	volume = {14},
	number = {6},
	journal = {BioEssays},
	author = {Easteal, Simon},
	year = {1992},
	pages = {415--419},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\XEWTHZ66\\Easteal-1992-BioEssays.pdf:application/pdf},
}

@article{Welch2018a,
	title = {Lecture {Notes} 703 2018 {Graphs} , networks and their algorithms in bioinformatics and systems biology},
	author = {Welch, David},
	year = {2018},
	pages = {1--15},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\D87UHDFL\\BIOINF 703 L03 lecture notes.pdf:application/pdf},
}

@article{Lavery,
	title = {Population genetic analyses {Evolutionary} fate ( and rate ) of mutations},
	author = {Lavery, Shane},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\E3HB2AQG\\733 lab popg analyses-2.pdf:application/pdf},
}

@article{Welch,
	title = {L01 : {Introduction} to graphs , networks},
	author = {Welch, David},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\UDA48B9J\\BIOINF 703 L01 Intro to graphs.pdf:application/pdf},
}

@article{DHaeseleer2006,
	title = {What are {DNA} sequence motifs?},
	volume = {24},
	issn = {1087-0156},
	url = {http://dx.doi.org/10.1038/nbt0406-423},
	doi = {10.1038/nbt0406-423},
	abstract = {Sequence motifs are becoming increasingly important in the analysis of gene regulation. How do we define sequence motifs, and why should we use sequence logos instead of consensus sequences to represent them? Do they have any relation with binding affinity? How do we search for new instances of a motif in this sea of DNA?},
	number = {4},
	journal = {Nature Biotechnology},
	author = {D'Haeseleer, Patrik},
	year = {2006},
	pmid = {16601727},
	note = {ISBN: 1087-0156 (Print){\textbackslash}r1087-0156 (Linking)},
	pages = {423--425},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ZBFFM9EG\\Dhaeseleer06-motifs.pdf:application/pdf},
}

@article{Properties2007,
	title = {When is one graph different from another ?},
	volume = {119},
	author = {Properties, Network Structural},
	year = {2007},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\J4NSN65J\\BIOINF 703 L02 Degree Distributions.pdf:application/pdf},
}

@article{Brent2007,
	title = {How does eukaryotic gene prediction work?},
	volume = {25},
	issn = {10870156},
	doi = {10.1038/nbt0807-883},
	abstract = {Computational prediction of gene structure is crucial for interpreting genomic sequences. But how do the algorithms involved work and how accurate are they?},
	number = {8},
	journal = {Nature Biotechnology},
	author = {Brent, Michael R.},
	year = {2007},
	pmid = {17687368},
	note = {ISBN: 1087-0156 (Print)},
	pages = {883--885},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ST9VKPVP\\Brent07.pdf:application/pdf},
}

@article{Dhaeseleer2006,
	title = {How does {DNA} sequence motif discovery work?},
	volume = {24},
	issn = {10870156},
	doi = {10.1038/nbt0806-959},
	abstract = {How can we computationally extract an unknown motif from a set of target sequences? What are the principles behind the major motif discovery algorithms? Which of these should we use, and how do we know we've found a 'real' motif? Extracting regulatory motifs1 from DNA sequences seems to be all the rage these days. Take your favorite cluster of coexpressed genes, and with some luck you might hope to find a short pattern of nucleotides upstream of the transcription start sites of these genes, indicating a common transcription factor binding site responsible for their coordinate regulation.},
	number = {8},
	journal = {Nature Biotechnology},
	author = {D'haeseleer, Patrik},
	year = {2006},
	pmid = {16900144},
	note = {ISBN: 1087-0156 (Print){\textbackslash}r1087-0156 (Linking)},
	pages = {959--961},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\CR6LPXJK\\Dhaeseleer06-motifdiscovery.pdf:application/pdf},
}

@article{Compeau2011,
	title = {How to apply de {Bruijn} graphs to genome assembly},
	volume = {29},
	issn = {10870156},
	url = {http://dx.doi.org/10.1038/nbt.2023},
	doi = {10.1038/nbt.2023},
	abstract = {A mathematical concept known as a de Bruijn graph turns the formidable challenge of assembling a contiguous genome from billions of short sequencing reads into a tractable computational problem.},
	number = {11},
	journal = {Nature Biotechnology},
	author = {Compeau, Phillip E.C. and Pevzner, Pavel A. and Tesler, Glenn},
	year = {2011},
	pmid = {22068540},
	note = {arXiv: doi:10.1038/nbt.2023
Publisher: Nature Publishing Group
ISBN: 0000110010111},
	pages = {987--991},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\IUF2YDSL\\Compeau-etal2011.pdf:application/pdf},
}

@article{Park2003,
	title = {Free energy calculation from steered molecular dynamics simulations using {Jarzynski}’s equality},
	volume = {119},
	issn = {00219606},
	url = {http://scitation.aip.org/content/aip/journal/jcp/119/6/10.1063/1.1590311},
	doi = {10.1063/1.1590311},
	abstract = {Jarzynski’s equality is applied to free energy calculations from steered molecular dynamics simulations of biomolecules. The helix-coil transition of deca-alanine in vacuum is used as an example. With about ten trajectories sampled, the second order cumulant expansion, among the various averaging schemes examined, yields the most accurate estimates. We compare umbrella sampling and the present method, and find that their efficiencies are comparable.},
	number = {6},
	journal = {The Journal of Chemical Physics},
	author = {Park, Sanghyun and Khalili-Araghi, Fatemeh and Tajkhorshid, Emad and Schulten, Klaus},
	year = {2003},
	pmid = {184350300066},
	note = {ISBN: 00219606},
	pages = {3559},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\38H5JBQC\\paper.pdf:application/pdf},
}

@article{Motion,
	title = {Slide 1 : {Chromatophore}},
	author = {Motion, Thermal},
	pages = {1--5},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\WWEEIWXV\\LH2-module-instructions.pdf:application/pdf},
}

@misc{VMD,
	title = {Slide 1 : {Brownian} {Motion} {Slides} 2- ­ ‐ 7 : {Hydrophobic} {Effect}},
	author = {{VMD}},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\CDCAMQ8S\\membrane-module-instructions.pdf:application/pdf},
}

@article{Modelling,
	title = {246.201 bioinf703},
	author = {Modelling, Biochemical},
	pages = {1--12},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\RGDSEHYP\\BIOINF703_VMDintro.pdf:application/pdf},
}

@article{Golding1983,
	title = {Estimates of {DNA} and protein sequence divergence: an examination of some assumptions.,},
	volume = {1},
	url = {https://doi.org/10.1093/oxfordjournals.molbev.a040303},
	number = {1},
	journal = {Molecular Biology and Evolution},
	author = {Golding, G B},
	year = {1983},
	pages = {Pages 125--142},
}

@incollection{Zuckerkandl1963,
	title = {Molecular {Disease}, {Evolution}, and {Genic} {Heterogeneity}},
	booktitle = {Molecular {Disease} {Evolution}, {And} {The} {Gene}},
	author = {Zuckerkandl, Emile and Pauling, Linus},
	year = {1963},
	pages = {p189 --},
	file = {ZuckerkandlPauling Molecular Disease 1963.docx:C\:\\Users\\jstacey\\Zotero\\storage\\ECQ78D57\\ZuckerkandlPauling Molecular Disease 1963.docx:application/vnd.openxmlformats-officedocument.wordprocessingml.document},
}

@article{Goodman1974,
	title = {The phylogeny of human globin genes investigated by the maximum parsimony method},
	volume = {3},
	number = {1},
	journal = {Journal of Molecular Evolution},
	author = {Goodman, Morris and Moore, G. William and Barnabas, John and Matsuda, Genji},
	year = {1974},
	pages = {1--48},
}

@book{Kimura1983,
	address = {Cambridge},
	title = {The {Neutral} {Theory} of {Molecular} {Evolution}.},
	publisher = {Cambridge Univ. Press},
	author = {Kimura, M},
	year = {1983},
}

@article{Li1987,
	title = {An {Evaluation} of the {Molecular} {Clock} {Hypothesis} {Using} {Mammalian} {DNA} {Sequences}.},
	volume = {25},
	url = {https://doi.org/10.1007/BF02603118},
	number = {330},
	journal = {Journal of Molecular Evolution},
	author = {Li, WH. and Tanimura, M. and Sharp, P.M.},
	year = {1987},
}

@article{Gillespie1979,
	title = {Are evolutionary rates really variable?},
	volume = {13},
	number = {1},
	journal = {Journal of Molecular Evolution},
	author = {Gillespie, John H. and Langley, Charles H.},
	year = {1979},
	pages = {27--34},
}

@article{Hasegawa1985,
	title = {Dating of the human-ape splitting by a molecular clock of mitochondrial {DNA}},
	volume = {22},
	issn = {1432-1432},
	url = {https://doi.org/10.1007/BF02101694},
	doi = {10.1007/BF02101694},
	abstract = {A new statistical method for estimating divergence dates of species from DNA sequence data by a molecular clock approach is developed. This method takes into account effectively the information contained in a set of DNA sequence data. The molecular clock of mitochondrial DNA (mtDNA) was calibrated by setting the date of divergence between primates and ungulates at the Cretaceous-Tertiary boundary (65 million years ago), when the extinction of dinosaurs occurred. A generalized leastsquares method was applied in fitting a model to mtDNA sequence data, and the clock gave dates of 92.3±11.7, 13.3±1.5, 10.9±1.2, 3.7±0.6, and 2.7±0.6 million years ago (where the second of each pair of numbers is the standard deviation) for the separation of mouse, gibbon, orangutan, gorilla, and chimpanzee, respectively, from the line leading to humans. Although there is some uncertainty in the clock, this dating may pose a problem for the widely believed hypothesis that the bipedal creatureAustralopithecus afarensis, which lived some 3.7 million years ago at Laetoli in Tanzania and at Hadar in Ethiopia, was ancestral to man and evolved after the human-ape splitting. Another likelier possibility is that mtDNA was transferred through hybridization between a proto-human and a protochimpanzee after the former had developed bipedalism.},
	number = {2},
	journal = {Journal of Molecular Evolution},
	author = {Hasegawa, Masami and Kishino, Hirohisa and Yano, Taka-aki},
	year = {1985},
	pages = {160--174},
}

@article{jukes1969evolution,
	title = {Evolution of protein molecules},
	volume = {3},
	number = {21},
	journal = {Mammalian protein metabolism},
	author = {Jukes, Thomas H and Cantor, Charles R},
	year = {1969},
	note = {Publisher: New York},
	pages = {132},
}

@book{mayr1982growth,
	title = {The {Growth} of {Biological} {Thought}: {Diversity}, {Evolution}, and {Inheritance}},
	isbn = {978-0-674-36446-2},
	url = {https://books.google.co.nz/books?id=pHThtE2R0UQC},
	publisher = {Belknap Press of Harvard University Press},
	author = {Mayr, Ernst},
	year = {1982},
}

@article{tajima1984estimation,
	title = {Estimation of evolutionary distance between nucleotide sequences.},
	volume = {1},
	number = {3},
	journal = {Molecular biology and evolution},
	author = {Tajima, Fumio and Nei, Masatoshi},
	year = {1984},
	pages = {269--285},
}

@article{10.2307/1715117,
	title = {Organisms and {Molecules} in {Evolution}},
	volume = {146},
	issn = {00368075, 10959203},
	url = {http://www.jstor.org/stable/1715117},
	number = {3651},
	journal = {Science},
	author = {Simpson, George Gaylord},
	year = {1964},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1535--1538},
}

@article{Rambaut1998,
	title = {Estimating divergence dates from molecular sequences.},
	volume = {15},
	issn = {0737-4038},
	url = {http://dx.doi.org/10.1093/oxfordjournals.molbev.a025940},
	abstract = {The ability to date the time of divergence between lineages using molecular data provides the opportunity to answer many important questions in evolutionary biology. However, molecular dating techniques have previously been criticized for failing to adequately account for variation in the rate of molecular evolution. We present a maximum-likelihood approach to estimating divergence times that deals explicitly with the problem of rate variation. This method has many advantages over previous approaches including the following: (1) a rate constancy test excludes data for which rate heterogeneity is detected; (2) date estimates are generated with confidence intervals that allow the explicit testing of hypotheses regarding divergence times; and (3) a range of sequences and fossil dates are used, removing the reliance on a single calculated calibration rate. We present tests of the accuracy of our method, which show it to be robust to the effects of some modes of rate variation. In addition, we test the effect of substitution model and length of sequence on the accuracy of the dating technique. We believe that the method presented here offers solutions to many of the problems facing molecular dating and provides a platform for future improvements to such analyses.},
	number = {4},
	journal = {Molecular Biology and Evolution},
	author = {Rambaut, A and Bromham, L},
	month = apr,
	year = {1998},
	pages = {442--448},
	annote = {10.1093/oxfordjournals.molbev.a025940},
	annote = {10.1093/oxfordjournals.molbev.a025940},
}

@article{Sanderson1997,
	title = {A {Nonparametric} {Approach} of {Rate} {Constancy} to {Estimating} {Divergence} {Times} in the {Absence}},
	volume = {14},
	issn = {0737-4038},
	doi = {10.1093/oxfordjournals.molbev.a025731},
	abstract = {A new method for estimating divergence times when evolutionary rates are variable across lineages is proposed. The method, called nonparametric rate smoothing (NPRS), relies on minimization of ancestor-descendant local rate changes and is motivated by the likelihood that evolutionary rates are autocorrelated in time. Fossil information pertaining to minimum and/or maximum ages of nodes in a phylogeny is incorporated into the algorithms by constrained optimization techniques. The accuracy of NPRS was examined by comparison to a clock-based maxi- mum-likelihood method in computer simulations. NPRS provides more accurate estimates of divergence times when (1) sequence lengths are sufficiently long, (2) rates are truly nonclocklike, and (3) rates are moderately to highly autocorrelated in time. The algorithms were applied to estimate divergence times in seed plants based on data from the chloroplast rbcL gene. Both constrained and unconstrained NPRS methods tended to produce divergence time estimates more consistent with paleobotanical evidence than did clock-based estimates.},
	number = {12},
	journal = {Molecular Biology and Evolution},
	author = {Sanderson, Michael J},
	year = {1997},
	note = {ISBN: 0737-4038},
	pages = {1218--1231},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\3RY3H3VX\\Sanderson 1997.pdf:application/pdf},
}

@article{Felsenstein1981,
	title = {Evolutionary trees from {DNA} sequences: {A} maximum likelihood approach},
	volume = {17},
	issn = {1432-1432},
	url = {https://doi.org/10.1007/BF01734359},
	doi = {10.1007/BF01734359},
	abstract = {The application of maximum likelihood techniques to the estimation of evolutionary trees from nucleic acid sequence data is discussed. A computationally feasible method for finding such maximum likelihood estimates is developed, and a computer program is available. This method has advantages over the traditional parsimony algorithms, which can give misleading results if rates of evolution differ in different lineages. It also allows the testing of hypotheses about the constancy of evolutionary rates by likelihood ratio tests, and gives rough indication of the error of the estimate of the tree.},
	number = {6},
	journal = {Journal of Molecular Evolution},
	author = {Felsenstein, Joseph},
	year = {1981},
	pages = {368--376},
}

@article{Mccann2015,
	title = {Digital {Receipt}},
	author = {Mccann, Kristine},
	year = {2015},
	note = {ISBN: 0002321026073},
	pages = {2014},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\NEN7EWRX\\receipt_BIOSCI733 jbuc045 A1.pdf.pdf:application/pdf},
}

@article{Cook2016,
	title = {The {European} {Bioinformatics} {Institute} in 2016: {Data} growth and integration},
	volume = {44},
	issn = {13624962},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/26673705},
	doi = {10.1093/nar/gkv1352},
	abstract = {New technologies are revolutionising biological re- search and its applications by making it easier and cheaper to generate ever-greater volumes and types of data. In response, the services and infrastruc- ture of the European Bioinformatics Institute (EMBL- EBI, www.ebi.ac.uk) are continually expanding: total disk capacity increases significantly every year to keep pace with demand (75 petabytes as of Decem- ber 2015), and interoperability between resources remains a strategic priority. Since 2014 we have launched two newresources: the European Variation Archive for genetic variation data and EMPIAR for two-dimensional electron microscopy data, as well as a Resource Description Framework platform. We also launched the Embassy Cloud service, which al- lows users to run large analyses in a virtual environ- ment next to EMBL-EBI’s vast public data resources.},
	number = {D1},
	urldate = {2018-09-12},
	journal = {Nucleic Acids Research},
	author = {Cook, Charles E and Bergman, Mary Todd and Finn, Robert D and Cochrane, Guy and Birney, Ewan and Apweiler, Rolf},
	month = jan,
	year = {2016},
	pmid = {26673705},
	note = {Publisher: Oxford University Press
ISBN: 13624962 (Electronic)},
	pages = {D20--D26},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\SXW2U44E\\full-text.pdf:application/pdf},
}

@article{Reanney1984,
	title = {Temperature as a determinative factor in the evolution of genetic systems},
	volume = {21},
	issn = {00222844},
	doi = {10.1007/BF02100629},
	abstract = {Heat induces a number of premutational lesions (for example, the deamination of cytosine to uracil) in DNA and RNA. These kinds of errors occur in resting as well as replicating polynucleotides. However, an increase in temperature also raises the probability of copying error occurring in nucleic acids because of increased thermal noise in the replicative machinery. In most modern genetic systems, the majority of heat-induced lesions are efficiently repaired. It follows that the importance of heat-induced error increases as the effectiveness of repair declines. We show in this paper that the error rate of enzymatic polynucleotide copying is expected to increase monotonically with temperature. We also explore the effects of temperature variations on the early evolution of biological information transmission mechanisms.},
	number = {1},
	journal = {Journal of Molecular Evolution},
	author = {Reanney, Darryl C. and Pressing, J.},
	year = {1984},
	pmid = {6442360},
	keywords = {Evolution, Heat, Rates of copy error},
	pages = {72--75},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\L82TLJ8W\\Reanney-Pressing1984.pdf:application/pdf},
}

@techreport{Output,
	title = {{RW} 3 pops based on {BioSci733} \_ rightwh . nxs + {RWamov}},
	author = {Output, N-Migrate},
	pages = {1--10},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\XNJGRBIK\\outfile.pdf:application/pdf},
}

@article{Shannon1948,
	title = {A {Mathematical} {Theory} of {Communication}},
	volume = {5},
	issn = {15591662},
	url = {http://portal.acm.org/citation.cfm?doid=584091.584093%5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6773024},
	doi = {10.1002/j.1538-7305.1948.tb01338.x},
	abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange band-width for signal-to-noise ratio has intensified the interest in a general theory of communication.{\textbackslash}n},
	number = {3},
	journal = {Bell System Technical Journal},
	author = {Shannon, Claude Edmund},
	year = {1948},
	pmid = {9230594},
	note = {arXiv: chao-dyn/9411012
ISBN: 0252725484},
	pages = {3},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\9ZWE6WMN\\shannon1949.pdf:application/pdf},
}

@techreport{N-MigrateOutput,
	title = {Example : {Microsatellite} data set},
	author = {{N-Migrate Output}},
	note = {ISBN: 1881505405},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\5KQ5SG8U\\outfile-ml-saved.pdf:application/pdf},
}

@techreport{Output2013a,
	title = {Example : {Microsatellite} data set},
	author = {Output, N-Migrate},
	year = {2013},
	pages = {1--15},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\VDIIZBE9\\outfile-bayes.pdf:application/pdf},
}

@techreport{Output2013b,
	title = {Example : {Microsatellite} data set},
	author = {Output, N-Migrate},
	year = {2013},
	pages = {1--15},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\8XVAAYKX\\outfile-bayes-saved.pdf:application/pdf},
}

@article{Excoffier2010,
	title = {An {Integrated} {Software} {Package} for {Population} {Genetics} {Data} {Analysis}},
	volume = {10},
	issn = {1755-0998},
	doi = {10.1111/j.1755-0998.2010.02847.x},
	abstract = {Arlequin ver 3.0 is a software package integrating several basic and advanced methods for population genetics data analysis, like the computation of standard genetic diversity indices, the estimation of allele and haplotype frequencies, tests of departure from linkage equilibrium, departure from selective neutrality and demographic equilibrium, estimation or parameters from past population expansions, and thorough analyses of population subdivision under the AMOVA framework. Arlequin 3 introduces a completely new graphical interface written in C++, a more robust semantic analysis of input files, and two new methods: a Bayesian estimation of gametic phase from multi-locus genotypes, and an estimation of the parameters of an instantaneous spatial expansion from DNA sequence polymorphism. Arlequin can handle several data types like DNA sequences, microsatellite data, or standard multi-locus genotypes. A Windows version of the software is freely available on http://cmpg.unibe.ch/software/arlequin3.},
	journal = {Molecular Ecology Resources},
	author = {Excoffier, Laurent and Lischer, H E L},
	year = {2010},
	pmid = {21565059},
	note = {arXiv: 1011.1669v3
ISBN: 1755-0998},
	pages = {564--567},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\N4KDN4LE\\Arlequin35.pdf:application/pdf},
}

@article{Boeke2018,
	title = {{GP}-write : {A} {Grand} {Challenge} {Project} to {Build} and {Test} {Genomes} in {Living} {Cells}},
	number = {May},
	author = {Boeke, Jef D. and Church, George M. and Hessel, Andrew and Kelley, Nancy J},
	year = {2018},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\WQ676D75\\2018_GPwrite SWG.pdf:application/pdf},
}

@article{Drake1976,
	title = {The {Biochemistry} of {Mutagenesis}},
	volume = {45},
	journal = {Annu. Rev. Biochem.},
	author = {Drake, John W. and Baltz, Richard H.},
	year = {1976},
	pages = {11--37},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\RWKI48XY\\Drake Baltz1976.pdf:application/pdf},
}

@techreport{Output2013,
	title = {Example : {Microsatellite} data set},
	author = {Output, N-Migrate},
	year = {2013},
	pages = {1--15},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\U52ZV4NA\\outfile-ml.pdf:application/pdf},
}

@patent{Nielsen1996,
	title = {Peptide {Nucleic} {Acids}},
	abstract = {A novel class of compounds, known as peptide nucleic acids, bind complementary ssDNA and RNA strands more strongly than a corresponding DNA. The peptide nucleic acids generally comprise ligands such as naturally occurring DNA bases attached to a peptide backbone through a suit able linker.},
	author = {Nielsen, Peter E and Buchardt, Ole and Egholm, Michael and Berg, Rolf H.},
	year = {1996},
	doi = {10.1126/science.1174577},
	note = {Publication Title: US Patent 5,539,082
ISBN: 978-1-62703-552-1},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\D3SXYIPK\\Peptide Nucleic Acid patent.pdf:application/pdf},
}

@article{Kricker1990,
	title = {Heat mutagenesis in bacteriophage {T4}: {Another} walk down the transversion pathway},
	volume = {172},
	issn = {00219193},
	doi = {10.1128/jb.172.6.3037-3039.1990},
	abstract = {Heat induces transversions (as well as transitions) at G-C base pairs in bacteriophage T4. The target base for transversions is guanine,which is converted to a product which is sometimes replicated and transcribed as a pyrimidine.A model for this process is proposed in which the deoxyguanosine glycosidic bond migrates from N9 to N2: the resulting deoxyneoguanosine may pair with normal guanine to produce G-C leads to C-G transversions.},
	number = {6},
	journal = {Journal of Bacteriology},
	author = {Kricker, M. C. and Drake, John W.},
	year = {1990},
	pmid = {1069305},
	pages = {3037--3039},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\MMX68EMC\\Kricker Drake 1990.pdf:application/pdf},
}

@inproceedings{Furrer2018,
	address = {Milpitas, CA},
	title = {Timing {Recovery} {For} {Low}-{SNR} {Magnetic} {Tape} {Recording} {Channels}},
	booktitle = {The 29th {Magnetic} {Recording} {Conference} ({TMRC})},
	author = {Furrer, Simeon and Lantz, Mark A},
	year = {2018},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\FZJM4FW2\\Furrer Lantz 2014.pdf:application/pdf},
}

@article{Birkelund2017,
	title = {{PhD} {Thesis} {Tape} as {Primary} {Storage} for {Large} {Scientific} {Data} {Sets}},
	author = {Birkelund, Klaus and Jensen, Abildgaard},
	year = {2017},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\EXE9RNBC\\Jensen 2017.pdf:application/pdf},
}

@article{Roberts2000,
	title = {Beyond {Moore}'s {Law}: {Internet} {Growth} {Trends}},
	volume = {33},
	issn = {00189162},
	url = {http://ieeexplore.ieee.org/document/963131/},
	doi = {10.1109/2.963131},
	abstract = {To keep pace with the Internet's growth, the maximum speed of core{\textbackslash}nrouters and switches must increase at the same rate. In a study{\textbackslash}nconducted in 1969, the author analyzed 39 scientific computers released{\textbackslash}nor planned for release from 1958 to 1972 to determine optimal computer{\textbackslash}nreplacement strategy (http://www.ziplink.net/lroberts/Forecast69.htm).{\textbackslash}nThis study looked at the trend of CPU throughput per dollar and{\textbackslash}npredicted that computer performance would double every 18.6 months.{\textbackslash}nUpdating the study using data for 1999 PCs shows that the trend over 41{\textbackslash}nyears is a doubling of computer performance every 21 months, a{\textbackslash}nremarkably small correction. A similar study tracking the costs from the{\textbackslash}nfirst ARPA packet switches in 1969 to the most modern routers and ATM{\textbackslash}nswitches in 1999 confirms that packet switches have followed the same{\textbackslash}ntrend as computers, with performance per dollar doubling every 21{\textbackslash}nmonths. Although the computer performance rate predicted in the updated{\textbackslash}n1969 study is similar to Moore's law, the trends are not identical. It{\textbackslash}nwould appear that both the performance per dollar for computers and the{\textbackslash}nserial interface speed for communications are increasing at 94 percent{\textbackslash}nof the yearly growth rate of semiconductor performance. We can use this{\textbackslash}ninformation about performance and cost trends to predict the cost of{\textbackslash}ncomputers and communications and to understand the Internet traffic{\textbackslash}ngrowth. Keeping up with these trends will be a major engineering{\textbackslash}nchallenge},
	number = {1},
	urldate = {2018-09-12},
	journal = {Computer},
	author = {Roberts, Lawrence G.},
	year = {2000},
	note = {ISBN: 0018-9162},
	pages = {117--119},
}

@article{Marr2015,
	title = {Big {Data}: 20 {Mind}-{Boggling} {Facts} {Everyone} {Must} {Read}},
	url = {https://www.forbes.com/sites/bernardmarr/2015/09/30/big-data-20-mind-boggling-facts-everyone-must-read/#76d3b30917b1},
	journal = {Forbes},
	author = {Marr, Bernard},
	year = {2015},
}

@misc{Switch2018,
	title = {Switch {Data} {Center} - {The} {Citidel}},
	url = {https://www.switch.com/tahoe-reno},
	urldate = {2018-09-12},
	author = {{Switch}},
	year = {2018},
}

@article{Zwolenski2014,
	title = {The {Digital} {Universe}: {Rich} {Data} and the {Increasing} {Value} of the {Internet} of {Things}},
	volume = {2},
	number = {3},
	journal = {The Australian Journal of Telecommunications and the Digital Economy},
	author = {Zwolenski, Matt and Weatherill, Lee},
	year = {2014},
	pages = {1--9},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\GFTKTAGK\\Zwolenski Weatherill 2014.pdf:application/pdf},
}

@misc{Oracle2018,
	title = {{SuperCluster} {M8} {\textbar} {Oracle}},
	url = {https://www.oracle.com/engineered-systems/supercluster/supercluster-m8/index.html},
	urldate = {2018-09-12},
	author = {{Oracle}},
	year = {2018},
}

@misc{Morgan2018,
	title = {Data storage lifespans: {How} long will media really last?},
	url = {https://blog.storagecraft.com/data-storage-lifespan/},
	urldate = {2018-09-12},
	journal = {StorageCraft},
	author = {Morgan, Casey},
	year = {2018},
}

@inproceedings{Williams2008,
	address = {Bern, Switzerland},
	title = {Predicting {Archival} {Life} of {Removable} {Hard} {Disk} {Drives}},
	isbn = {978-0-89208-277-3},
	url = {http://www.ingentaconnect.com/content/ist/ac/2008/00002008/00000001/art00038},
	booktitle = {The {Imaging} {Science} \& {Technology} (digital) {Archiving} {Conference}},
	author = {Williams, Paul and Rosenthal, David S. H. and Roussopoulos, Mema and Georgis, Steve},
	year = {2008},
	pages = {188--192},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\LM6GYI7D\\ISandT2008.pdf:application/pdf},
}

@misc{Millenniata2018,
	title = {M-{DISC} {Technology} {\textbar} {M}-{DISC}},
	url = {http://www.mdisc.com/mdisc-technology/},
	urldate = {2018-09-12},
	author = {{Millenniata}},
	year = {2018},
}

@misc{IGCSEICT,
	title = {Comparison of {Storage} {Media}},
	url = {https://www.ictlounge.com/html/comparison_of_storage_media.htm},
	urldate = {2018-09-12},
	author = {{IGCSE ICT}},
}

@misc{Zetta2016,
	title = {Advances in {Data} {Storage} {Technology}: {A} {Timeline}},
	url = {https://www.zetta.net/about/blog/history-data-storage-technology},
	urldate = {2018-09-12},
	author = {{Zetta}},
	year = {2016},
}

@article{Stojanovic2003,
	title = {A deoxyribozyme-based molecular automaton},
	volume = {21},
	issn = {10870156},
	doi = {10.1038/nbt862},
	abstract = {We describe a molecular automaton, called MAYA, which encodes a version of the game of tic-tac-toe and interactively competes against a human opponent. The automaton is a Boolean network of deoxyribozymes that incorporates 23 molecular-scale logic gates and one constitutively active deoxyribozyme arrayed in nine wells (3x3) corresponding to the game board. To make a move, MAYA carries out an analysis of the input oligonucleotide keyed to a particular move by the human opponent and indicates a move by fluorescence signaling in a response well. The cycle of human player input and automaton response continues until there is a draw or a victory for the automaton. The automaton cannot be defeated because it implements a perfect strategy.},
	number = {9},
	journal = {Nature Biotechnology},
	author = {Stojanovic, Milan N. and Stefanovic, Darko},
	year = {2003},
	pmid = {12923549},
	note = {ISBN: 1087-0156},
	pages = {1069--1074},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\LWXTLBQP\\Stojanovic 2003.pdf:application/pdf},
}

@article{Adleman1994,
	title = {Molecular {Computation} of {Solutions} to {Combinatorial} {Problems}},
	volume = {266},
	number = {5187},
	journal = {Science},
	author = {Adleman, Leonard M},
	year = {1994},
	pages = {1021--1024},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\V8XJAGWF\\Adleman.pdf:application/pdf},
}

@article{Meyer2014,
	title = {A mitochondrial genome sequence of a hominin from {Sima} de los {Huesos}},
	volume = {505},
	issn = {00280836},
	doi = {10.1038/nature12788},
	abstract = {Excavations of a complex of caves in the Sierra de Atapuerca in northern Spain have unearthed hominin fossils that range in age from the early Pleistocene to the Holocene. One of these sites, the 'Sima de los Huesos' ('pit of bones'), has yielded the world's largest assemblage of Middle Pleistocene hominin fossils, consisting of at least 28 individuals dated to over 300,000 years ago. The skeletal remains share a number of morphological features with fossils classified as Homo heidelbergensis and also display distinct Neanderthal-derived traits. Here we determine an almost complete mitochondrial genome sequence of a hominin from Sima de los Huesos and show that it is closely related to the lineage leading to mitochondrial genomes of Denisovans, an eastern Eurasian sister group to Neanderthals. Our results pave the way for DNA research on hominins from the Middle Pleistocene.},
	number = {7483},
	journal = {Nature},
	author = {Meyer, Matthias and Fu, Qiaomei and Aximu-Petri, Ayinuer and Glocke, Isabelle and Nickel, Birgit and Arsuaga, Juan Luis and Martínez, Ignacio and Gracia, Ana and De Castro, José María Bermúdez and Carbonell, Eudald and Pääbo, Svante},
	year = {2014},
	pmid = {24305051},
	note = {ISBN: 1476-4687 (Electronic){\textbackslash}n0028-0836 (Linking)},
	pages = {403--406},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\F8AZ5I3J\\Meyer 2014.pdf:application/pdf},
}

@article{Hoss1996,
	title = {{DNA} damage and {DMA} sequence retrieval from ancient tissues},
	volume = {24},
	issn = {03051048},
	doi = {10.1093/nar/24.7.1304},
	abstract = {Gas chromatography/mass spectrometry (GC/MS) was used to determine the amounts of eight oxidative base modifications in DNA extracted from 11 specimens of bones and soft tissues, ranging in age from 40 to {\textgreater}50 000 years. Among the compounds assayed hydantoin derivatives of pyrimidines were quantitatively dominant. From five of the specimens endogenous ancient DNA sequences could be amplified by PCR. The DNA from these specimens contained substantially lower amounts of hydantoins than the six specimens from which no DNA could be amplified. Other types of damage, e.g. oxidation products of purines, did not correlate with the inability to retrieve DNA sequences. Furthermore, all samples with low amounts of damage and from which DNA could be amplified stemmed from regions where low temperatures have prevailed throughout the burial period of the specimens.},
	number = {7},
	journal = {Nucleic Acids Research},
	author = {Höss, Matthias and Jaruga, Pawel and Zastawny, Tomasz H. and Dizdaroglu, Mirai and Pääbo, Svante},
	year = {1996},
	pmid = {8614634},
	note = {ISBN: 0305-1048 (Print){\textbackslash}r0305-1048 (Linking)},
	pages = {1304--1307},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\JQW6RAIX\\Hoss et al 1996.pdf:application/pdf},
}

@article{Heupink2016,
	title = {Ancient {mtDNA} sequences from the {First} {Australians} revisited},
	volume = {113},
	issn = {0027-8424},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1521066113},
	doi = {10.1073/pnas.1521066113},
	abstract = {The publication in 2001 by Adcock et al. [Adcock GJ, et al. (2001) Proc Natl Acad Sci USA 98(2):537-542] in PNAS reported the recovery of short mtDNA sequences from ancient Australians, including the 42,000-y-old Mungo Man [Willandra Lakes Hominid (WLH3)]. This landmark study in human ancient DNA suggested that an early modern human mitochondrial lineage emerged in Asia and that the theory of modern human origins could no longer be considered solely through the lens of the "Out of Africa" model. To evaluate these claims, we used second generation DNA sequencing and capture methods as well as PCR-based and single-primer extension (SPEX) approaches to reexamine the same four Willandra Lakes and Kow Swamp 8 (KS8) remains studied in the work by Adcock et al. Two of the remains sampled contained no identifiable human DNA (WLH15 and WLH55), whereas the Mungo Man (WLH3) sample contained no Aboriginal Australian DNA. KS8 reveals human mitochondrial sequences that differ from the previously inferred sequence. Instead, we recover a total of five modern European contaminants from Mungo Man (WLH3). We show that the remaining sample (WLH4) contains ∼1.4\% human DNA, from which we assembled two complete mitochondrial genomes. One of these was a previously unidentified Aboriginal Australian haplotype belonging to haplogroup S2 that we sequenced to a high coverage. The other was a contaminating modern European mitochondrial haplotype. Although none of the sequences that we recovered matched those reported by Adcock et al., except a contaminant, these findings show the feasibility of obtaining important information from ancient Aboriginal Australian remains.},
	number = {25},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Heupink, Tim H. and Subramanian, Sankar and Wright, Joanne L. and Endicott, Phillip and Westaway, Michael Carrington and Huynen, Leon and Parson, Walther and Millar, Craig D. and Willerslev, Eske and Lambert, David M.},
	year = {2016},
	pmid = {27274055},
	note = {ISBN: 0027-8424},
	pages = {6892--6897},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\BUPUK579\\Heupink 2015.pdf:application/pdf},
}

@article{Lazaridis2017,
	title = {Genetic origins of the {Minoans} and {Mycenaeans}},
	volume = {548},
	issn = {14764687},
	doi = {10.1038/nature23310},
	abstract = {New genome-wide data for ancient, Bronze Age individuals, including Minoans, Mycenaeans, and southwestern Anatolians, show that Minoans and Mycenaeans were genetically very similar yet distinct, supporting the idea of continuity but not isolation in the history of populations of the Aegean.},
	number = {7666},
	journal = {Nature},
	author = {Lazaridis, Iosif and Mittnik, Alissa and Patterson, Nick and Mallick, Swapan and Rohland, Nadin and Pfrengle, Saskia and Furtwängler, Anja and Peltzer, Alexander and Posth, Cosimo and Vasilakis, Andonis and McGeorge, P. J.P. and Konsolaki-Yannopoulou, Eleni and Korres, George and Martlew, Holley and Michalodimitrakis, Manolis and Özsait, Mehmet and Özsait, Nesrin and Papathanasiou, Anastasia and Richards, Michael and Roodenberg, Songül Alpaslan and Tzedakis, Yannis and Arnott, Robert and Fernandes, Daniel M. and Hughey, Jeffery R. and Lotakis, Dimitra M. and Navas, Patrick A. and Maniatis, Yannis and Stamatoyannopoulos, John A. and Stewardson, Kristin and Stockhammer, Philipp and Pinhasi, Ron and Reich, David and Krause, Johannes and Stamatoyannopoulos, George},
	year = {2017},
	pmid = {28783727},
	note = {arXiv: NIHMS150003
ISBN: 0008-5472 (Print){\textbackslash}r0008-5472 (Linking)},
	pages = {214--218},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\RINAVJLU\\Lazaridis 2017.pdf:application/pdf},
}

@article{Meyer2016,
	title = {Nuclear {DNA} sequences from the {Middle} {Pleistocene} {Sima} de los {Huesos} hominins},
	volume = {531},
	issn = {14764687},
	url = {http://dx.doi.org/10.1038/nature17405},
	doi = {10.1038/nature17405},
	abstract = {A unique assemblage of 28 hominin individuals, found in Sima de los Huesos in the Sierra de Atapuerca in Spain, has recently been dated to approximately 430,000 years ago. An interesting question is how these Middle Pleistocene hominins were related to those who lived in the Late Pleistocene epoch, in particular to Neanderthals in western Eurasia and to Denisovans, a sister group of Neanderthals so far known only from southern Siberia. While the Sima de los Huesos hominins share some derived morphological features with Neanderthals, the mitochondrial genome retrieved from one individual from Sima de los Huesos is more closely related to the mitochondrial DNA of Denisovans than to that of Neanderthals. However, since the mitochondrial DNA does not reveal the full picture of relationships among populations, we have investigated DNA preservation in several individuals found at Sima de los Huesos. Here we recover nuclear DNA sequences from two specimens, which show that the Sima de los Huesos hominins were related to Neanderthals rather than to Denisovans, indicating that the population divergence between Neanderthals and Denisovans predates 430,000 years ago. A mitochondrial DNA recovered from one of the specimens shares the previously described relationship to Denisovan mitochondrial DNAs, suggesting, among other possibilities, that the mitochondrial DNA gene pool of Neanderthals turned over later in their history.  **hi ha vídeo: https://vimeo.com/158883454},
	number = {7595},
	journal = {Nature},
	author = {Meyer, Matthias and Arsuaga, Juan Luis and De Filippo, Cesare and Nagel, Sarah and Aximu-Petri, Ayinuer and Nickel, Birgit and Martínez, Ignacio and Gracia, Ana and De Castro, José María Bermúdez and Carbonell, Eudald and Viola, Bence and Kelso, Janet and Prüfer, Kay and Pääbo, Svante},
	year = {2016},
	pmid = {26976447},
	note = {Publisher: Nature Publishing Group
ISBN: 1476-4687},
	pages = {504--507},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\GL2M8EKU\\Meyer 2016.pdf:application/pdf},
}

@article{Noonan2006,
	title = {Sequencing and analysis of {Neanderthal} genomic {DNA}},
	volume = {314},
	issn = {1095-9203},
	doi = {10.1126/science.1131412},
	abstract = {Our knowledge of Neanderthals is based on a limited number of remains and artifacts from which we must make inferences about their biology, behavior, and relationship to ourselves. Here, we describe the characterization of these extinct hominids from a new perspective, based on the development of a Neanderthal metagenomic library and its high-throughput sequencing and analysis. Several lines of evidence indicate that the 65,250 base pairs of hominid sequence so far identified in the library are of Neanderthal origin, the strongest being the ascertainment of sequence identities between Neanderthal and chimpanzee at sites where the human genomic sequence is different. These results enabled us to calculate the human-Neanderthal divergence time based on multiple randomly distributed autosomal loci. Our analyses suggest that on average the Neanderthal genomic sequence we obtained and the reference human genome sequence share a most recent common ancestor {\textasciitilde}706,000 years ago, and that the human and Neanderthal ancestral populations split {\textasciitilde}370,000 years ago, before the emergence of anatomically modern humans. Our finding that the Neanderthal and human genomes are at least 99.5\% identical led us to develop and successfully implement a targeted method for recovering specific ancient DNA sequences from metagenomic libraries. This initial analysis of the Neanderthal genome advances our understanding of the evolutionary relationship of Homo sapiens and Homo neanderthalensis and signifies the dawn of Neanderthal genomics.},
	number = {5802},
	journal = {Science},
	author = {Noonan, James P and Coop, Graham and Kudaravalli, Sridhar and Smith, Doug and Krause, Johannes and Alessi, Joe and Chen, Feng and Platt, Darren and {Svante Pääbo} and Pritchard, Jonathan K and Rubin, Edward M},
	year = {2006},
	pages = {1113--1118},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\3II9G5RW\\Noonan 2006.pdf:application/pdf},
}

@article{ElEzzi2010,
	title = {Long-term stability of thyroid hormones and {DNA} in blood spots kept under varying storage conditions},
	volume = {52},
	issn = {13288067},
	doi = {10.1111/j.1442-200X.2010.03101.x},
	abstract = {Congenital hypothyroidism is screened using blood spotted on filter paper that may be transported from remote areas to central testing facilities. However, storage conditions and transportation may affect sample quality.},
	number = {4},
	journal = {Pediatrics International},
	author = {El Ezzi, Asmahan A. and El-Saidi, Mohammed A. and Kuddus, Ruhul H.},
	year = {2010},
	pmid = {20202157},
	keywords = {congenital hypothyroidism, DNA extraction, dried blood spot, polymerase chain reaction, radioimmunoassay, thyrotropin, thyroxin},
	pages = {631--639},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\EM46JJV4\\Ezzi_et_al-2010-Pediatrics_International.pdf:application/pdf},
}

@article{Krings1997,
	title = {Neandertal {DNA} sequences and the origin of modern humans.},
	volume = {90},
	issn = {614911677},
	url = {http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&id=9230299&retmode=ref&cmd=prlinks%5Cnpapers3://publication/doi/10.1016/S0092-8674(00)80310-4},
	doi = {10.1016/S0092-8674(00)80310-4},
	abstract = {DNA was extracted from the Neandertal-type specimen found in 1856 in western Germany. By sequencing clones from short overlapping PCR products, a hitherto unknown mitochondrial (mt) DNA sequence was determined. Multiple controls indicate that this sequence is endogenous to the fossil. Sequence comparisons with human mtDNA sequences, as well as phylogenetic analyses, show that the Neandertal sequence falls outside the variation of modern humans. Furthermore, the age of the common ancestor of the Neandertal and modern human mtDNAs is estimated to be four times greater than that of the common ancestor of human mtDNAs. This suggests that Neandertals went extinct without contributing mtDNA to modern humans.},
	number = {1},
	journal = {Cell},
	author = {Krings, M and Stone, Anne C and Schmitz, R W and Krainitzki, H and Stoneking, M and Paabo, S},
	year = {1997},
	pmid = {9230299},
	note = {ISBN: 0092-8674 (Print)},
	pages = {19--30},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\NV4XWB92\\Krings 1997.pdf:application/pdf},
}

@article{Roder2010,
	title = {Impact of long-term storage on stability of standard {DNA} for nucleic acid-based methods},
	volume = {48},
	issn = {00951137},
	doi = {10.1128/JCM.01230-10},
	abstract = {Real-time PCR is dependent upon a calibration function for quantification. While long-term storage of standards saves cost and time, solutions of DNA are prone to degradation. We present here the benchmark treatment for preservation of DNA standards, involving storage in 50\% glycerol-double-distilled water, whereby a deviation of 0.2 threshold cycle (C(T)) values resulted after 100 days of storage.},
	number = {11},
	journal = {Journal of Clinical Microbiology},
	author = {Röder, Barbara and Frühwirth, Karin and Vogl, Claus and Wagner, Martin and Rossmanith, Peter},
	year = {2010},
	pmid = {20810770},
	note = {ISBN: 1098-660X (Electronic){\textbackslash}r0095-1137 (Linking)},
	pages = {4260--4262},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\EHV89FQ2\\Roder2010.pdf:application/pdf},
}

@article{Anthonappa2013,
	title = {Evaluation of the long-term storage stability of saliva as a source of human {DNA}},
	volume = {17},
	issn = {14326981},
	doi = {10.1007/s00784-012-0871-5},
	abstract = {OBJECTIVES: The objectives of this paper are to determine the storage stability of saliva at 37 °C over an 18-month period, and its influence on the DNA yield, purity, PCR protocols and genotyping efficacy.{\textbackslash}n{\textbackslash}nMATERIALS AND METHODS: Of the 60 participants, blood samples were obtained from 10 and saliva from 50. Samples were subjected to different storage conditions: DNA extracted immediately; DNA extracted following storage at 37 °C for 1, 6, 12 and 18 months. Subsequently, DNA yield, OD(260/280) and OD(260/230) ratios were measured. The isolated DNA was used to amplify exons 0-7 of the RUNX2 gene and subsequently sequenced. Furthermore, 25 SNPs were genotyped.{\textbackslash}n{\textbackslash}nRESULTS: The mean DNA yield, OD(260/280) and OD(260/230) ratios obtained from blood were 67.4 ng/μl, 1.8 ± 0.05 and 1.8 ± 0.4 respectively. DNA yield obtained from saliva was significantly higher than blood (p {\textless} 0.0001), ranging from 97.4 to 125.8 ng/μl while the OD(260/280) ratio ranged from 1.8 ± 0.13 to 1.9 ± 0.1. The success rates for the 25 SNPs ranged from 98 to 100 \% for blood and 96-99 \% for saliva samples with the genotype frequencies in Hardy-Weinberg equilibrium ({\textgreater}0.01).{\textbackslash}n{\textbackslash}nCONCLUSIONS: Saliva can be stored at 37 °C for 18 months without compromising its quality and ability to endure genetic analyses.{\textbackslash}n{\textbackslash}nCLINICAL RELEVANCE: Saliva is a viable source of human DNA to facilitate the feasibility of large-scale genetic studies.},
	number = {7},
	journal = {Clinical Oral Investigations},
	author = {Anthonappa, Robert P. and King, Nigel M. and Rabie, A. Bakr M.},
	year = {2013},
	pmid = {23103961},
	note = {ISBN: 0078401208},
	keywords = {DNA, Blood, RUNX2, Saliva},
	pages = {1719--1725},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\UT9R9ALU\\Anthonappa2013_Article_EvaluationOfTheLong-termStorag.pdf:application/pdf},
}

@misc{OpenXtra2018,
	title = {Recommended {Server} {Room} {Temperature}},
	url = {https://www.openxtra.co.uk/kb/recommended-server-room-temperature.html},
	urldate = {2018-09-14},
	author = {{Open Xtra}},
	year = {2018},
}

@misc{CropTrust2018,
	title = {Svalbard {Global} {Seed} {Vault}},
	url = {https://www.croptrust.org/our-work/svalbard-global-seed-vault/},
	urldate = {2018-09-14},
	author = {{Crop Trust}},
	year = {2018},
}

@article{Vijayaraghavan2010,
	title = {Long-term structural and chemical stability of {DNA} in hydrated ionic liquids},
	volume = {49},
	issn = {14337851},
	doi = {10.1002/anie.200906610},
	abstract = {DNA was sol. and exhibited long-term stability in hydrated ionic liqs. based on choline dihydrogenphosphate, choline nitrate, or choline lactate. [on SciFinder(R)]},
	number = {9},
	journal = {Angewandte Chemie - International Edition},
	author = {Vijayaraghavan, Ranganathan and Izgorodin, Aleksey and Ganesh, Venkatraman and Surianarayanan, Mahadevan and MacFarlane, Douglas R.},
	year = {2010},
	pmid = {20108297},
	note = {ISBN: 1433-7851},
	keywords = {DNA, Fluorescence, Helical structures, Ionic liquids, Solubility},
	pages = {1631--1633},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\YAG7V3XR\\Vijayaraghavan_et_al-2010-Angewandte_Chemie_%28International_ed._in_English%29.pdf:application/pdf},
}

@article{Bonnet2009,
	title = {Chain and conformation stability of solid-state {DNA}: {Implications} for room temperature storage},
	volume = {38},
	issn = {03051048},
	doi = {10.1093/nar/gkp1060},
	abstract = {There is currently wide interest in room temperature storage of dehydrated DNA. However, there is insufficient knowledge about its chemical and structural stability. Here, we show that solid-state DNA degradation is greatly affected by atmospheric water and oxygen at room temperature. In these conditions DNA can even be lost by aggregation. These are major concerns since laboratory plastic ware is not airtight. Chain-breaking rates measured between 70 degrees C and 140 degrees C seemed to follow Arrhenius' law. Extrapolation to 25 degrees C gave a degradation rate of about 1-40 cuts/10(5) nucleotides/century. However, these figures are to be taken as very tentative since they depend on the validity of the extrapolation and the positive or negative effect of contaminants, buffers or additives. Regarding the secondary structure, denaturation experiments showed that DNA secondary structure could be preserved or fully restored upon rehydration, except possibly for small fragments. Indeed, below about 500 bp, DNA fragments underwent a very slow evolution (almost suppressed in the presence of trehalose) which could end in an irreversible denaturation. Thus, this work validates using room temperature for storage of DNA if completely protected from water and oxygen.},
	number = {5},
	journal = {Nucleic Acids Research},
	author = {Bonnet, Jacques and Colotte, Marthe and Coudy, Delphine and Couallier, Vincent and Portier, Joseph and Morin, Bénédicte and Tuffet, Sophie},
	year = {2009},
	pmid = {19969539},
	note = {ISBN: 1362-4962 (Electronic) 0305-1048 (Linking)},
	pages = {1531--1546},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\W55SY489\\Bonnet 2009.pdf:application/pdf},
}

@misc{DARPA2017,
	title = {Turning to {Chemistry} for {New} “{Computing}” {Concepts}},
	url = {https://www.darpa.mil/news-events/2017-03-23},
	urldate = {2018-09-14},
	author = {{DARPA}},
	year = {2017},
}

@misc{IlluminaInc.2018,
	title = {Sequencing and array-based solutions for genetic research},
	url = {https://www.illumina.com/},
	urldate = {2018-09-14},
	author = {{Illumina Inc.}},
	year = {2018},
}

@misc{PacificBiosciences2018,
	title = {Home},
	url = {https://www.pacb.com/},
	urldate = {2018-09-14},
	author = {{Pacific Biosciences}},
	year = {2018},
}

@misc{ThermoFisherScientific2018,
	title = {Home},
	url = {https://www.thermofisher.com/us/en/home.html},
	urldate = {2018-09-14},
	author = {{ThermoFisher Scientific}},
	year = {2018},
}

@article{Ranger2017,
	title = {Microsoft is buying another 10 million strands of {DNA} for storage research},
	url = {www.zdnet.com/article/microsoft-is-buying-another-10-million-strands-of-dna-for-storage-research/},
	journal = {ZDNet},
	author = {Ranger, Steve},
	month = apr,
	year = {2017},
}

@misc{IlluminaInc.2018a,
	title = {Sequencing {Platform} {Comparison} {Tool}},
	url = {https://www.illumina.com/systems/sequencing-platforms/comparison-tool.html},
	urldate = {2018-09-14},
	author = {{Illumina Inc.}},
	year = {2018},
}

@article{Tilley2017,
	title = {{HPE} {Has} {Constructed} {The} {Largest} {Single}-{Memory} {Computer} {System} {Ever} {Built}},
	url = {www.forbes.com/sites/aarontilley/2017/05/16/hpe-160-terabytes-memory/#5e24ccf8383f},
	journal = {Forbes},
	author = {Tilley, Aaron},
	month = may,
	year = {2017},
}

@article{Mowbray2017,
	title = {Efficient classification of strings using regular expressions},
	author = {Mowbray, Miranda and Horne, William and Rao, Prasad},
	year = {2017},
	keywords = {bioinformatics, network security, pattern matching, regular expressions, s},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ZLP8724D\\m-api-2c3099cc-0be6-7fad-598b-cc90b12a3e71.pdf:application/pdf},
}

@misc{Wetterstrand2018,
	title = {{DNA} {Sequencing} {Costs}},
	url = {www.genome.gov/sequencingcostsdata},
	journal = {The NHGRI Genome Sequencing Program (GSP)},
	author = {Wetterstrand, KA},
	year = {2018},
	note = {Publisher: NIH: National Human Genome Research Institute},
}

@article{Zavodna2014,
	title = {The accuracy, feasibility and challenges of sequencing short tandem repeats using next-generation sequencing platforms},
	volume = {9},
	issn = {19326203},
	doi = {10.1371/journal.pone.0113862},
	abstract = {To date we have little knowledge of how accurate next-generation sequencing (NGS) technologies are in sequencing repetitive sequences beyond known limitations to accurately sequence homopolymers. Only a handful of previous reports have evaluated the potential of NGS for sequencing short tandem repeats (microsatellites) and no empirical study has compared and evaluated the performance of more than one NGS platform with the same dataset. Here we examined yeast microsatellite variants from both long-read (454-sequencing) and short-read (Illumina) NGS platforms and compared these to data derived through Sanger sequencing. In addition, we investigated any locus-specific biases and differences that might have resulted from variability in microsatellite repeat number, repeat motif or type of mutation. Out of 112 insertion/deletion variants identified among 45 microsatellite amplicons in our study, we found 87.5\% agreement between the 454-platform and Sanger sequencing in frequency of variant detection after Benjamini-Hochberg correction for multiple tests. For a subset of 21 microsatellite amplicons derived from Illumina sequencing, the results of short-read platform were highly consistent with the other two platforms, with 100\% agreement with 454-sequencing and 93.6\% agreement with the Sanger method after Benjamini-Hochberg correction. We found that the microsatellite attributes copy number, repeat motif and type of mutation did not have a significant effect on differences seen between the sequencing platforms. We show that both long-read and short-read NGS platforms can be used to sequence short tandem repeats accurately, which makes it feasible to consider the use of these platforms in high-throughput genotyping. It appears the major requirement for achieving both high accuracy and rare variant detection in microsatellite genotyping is sufficient read depth coverage. This might be a challenge because each platform generates a consistent pattern of non-uniform sequence coverage, which, as our study suggests, may affect some types of tandem repeats more than others.},
	number = {12},
	journal = {PLoS ONE},
	author = {Zavodna, Monika and Bagshaw, Andrew and Brauning, Rudiger and Gemmell, Neil J.},
	year = {2014},
	pmid = {25436869},
	note = {ISBN: 1932-6203 (Electronic){\textbackslash}r1932-6203 (Linking)},
	pages = {1--14},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\PU8TF8LM\\NGS STR 2014.pdf:application/pdf},
}

@techreport{Birren2001,
	title = {Initial sequencing and analysis of the human genome {International} {Human} {Genome} {Sequencing} {Consortium}* {The} {Sanger} {Centre}: {Beijing} {Genomics} {Institute}/{Human} {Genome} {Center}},
	url = {www.nature.com},
	abstract = {The human genome holds an extraordinary trove of information about human development, physiology, medicine and evolution. Here we report the results of an international collaboration to produce and make freely available a draft sequence of the human genome. We also present an initial analysis of the data, describing some of the insights that can be gleaned from the sequence. The rediscovery of Mendel's laws of heredity in the opening weeks of the 20th century 1±3 sparked a scienti®c quest to understand the nature and content of genetic information that has propelled biology for the last hundred years. The scienti®c progress made falls naturally into four main phases, corresponding roughly to the four quarters of the century. The ®rst established the cellular basis of heredity: the chromosomes. The second de®ned the molecular basis of heredity: the DNA double helix. The third unlocked the informa-tional basis of heredity, with the discovery of the biological mechanism by which cells read the information contained in genes and with the invention of the recombinant DNA technologies of cloning and sequencing by which scientists can do the same. The last quarter of a century has been marked by a relentless drive to decipher ®rst genes and then entire genomes, spawning the ®eld of genomics. The fruits of this work already include the genome sequences of 599 viruses and viroids, 205 naturally occurring plasmids, 185 organelles, 31 eubacteria, seven archaea, one fungus, two animals and one plant. Here we report the results of a collaboration involving 20 groups from the United States, China to produce a draft sequence of the human genome. The draft genome sequence was generated from a physical map covering more than 96\% of the euchromatic part of the human genome and, together with additional sequence in public databases, it covers about 94\% of the human genome. The sequence was produced over a relatively short period, with coverage rising from about 10\% to more than 90\% over roughly ®fteen months. The sequence data have been made available without restriction and updated daily throughout the project. The task ahead is to produce a ®nished sequence, by closing all gaps and resolving all ambiguities. Already about one billion bases are in ®nal form and the task of bringing the vast majority of the sequence to this standard is now straightforward and should proceed rapidly. The sequence of the human genome is of interest in several respects. It is the largest genome to be extensively sequenced so far, being 25 times as large as any previously sequenced genome and eight times as large as the sum of all such genomes. It is the ®rst vertebrate genome to be extensively sequenced. And, uniquely, it is the genome of our own species. Much work remains to be done to produce a complete ®nished sequence, but the vast trove of information that has become available through this collaborative effort allows a global perspective on the human genome. Although the details will change as the sequence is ®nished, many points are already clear. X The genomic landscape shows marked variation in the distribution of a number of features, including genes, transposable elements, GC content, CpG islands and recombination rate. This gives us important clues about function. For example, the developmentally important HOX gene clusters are the most repeat-poor regions of the human genome, probably re¯ecting the very complex coordinate regulation of the genes in the clusters. X There appear to be about 30,000±40,000 protein-coding genes in the human genomeÐonly about twice as many as in worm or ¯y. However, the genes are more complex, with more alternative splicing generating a larger number of protein products. X The full set of proteins (thèproteome') encoded by the human genome is more complex than those of invertebrates. This is due in part to the presence of vertebrate-speci®c protein domains and motifs (an estimated 7\% of the total), but more to the fact that vertebrates appear to have arranged pre-existing components into a richer collection of domain architectures. X Hundreds of human genes appear likely to have resulted from horizontal transfer from bacteria at some point in the vertebrate lineage. Dozens of genes appear to have been derived from trans-posable elements. X Although about half of the human genome derives from trans-posable elements, there has been a marked decline in the overall activity of such elements in the hominid lineage. DNA transposons appear to have become completely inactive and long-terminal repeat (LTR) retroposons may also have done so. X The pericentromeric and subtelomeric regions of chromosomes are ®lled with large recent segmental duplications of sequence from elsewhere in the genome. Segmental duplication is much more frequent in humans than in yeast, ¯y or worm. X Analysis of the organization of Alu elements explains the long-standing mystery of their surprising genomic distribution, and suggests that there may be strong selection in favour of preferential retention of Alu elements in GC-rich regions and that thesèsel®sh' elements may bene®t their human hosts. X The mutation rate is about twice as high in male as in female meiosis, showing that most mutation occurs in males. X Cytogenetic analysis of the sequenced clones con®rms suggestions that large GC-poor regions are strongly correlated with`darkwith`dark G-bands' in karyotypes. X Recombination rates tend to be much higher in distal regions (around 20 megabases (Mb)) of chromosomes and on shorter chromosome arms in general, in a pattern that promotes the occurrence of at least one crossover per chromosome arm in each meiosis. X More than 1.4 million single nucleotide polymorphisms (SNPs) in the human genome have been identi®ed. This collection should allow the initiation of genome-wide linkage disequilibrium mapping of the genes in the human population. In this paper, we start by presenting background information on the project and describing the generation, assembly and evaluation of the draft genome sequence. We then focus on an initial analysis of the sequence itself: the broad chromosomal landscape; the repeat elements and the rich palaeontological record of evolutionary and biological processes that they provide; the human genes and proteins and their differences and similarities with those of other articles 860 NATURE {\textbar} VOL 409 {\textbar} 15 FEBRUARY 2001 {\textbar} www.nature.com},
	urldate = {2018-09-15},
	author = {Birren, Bruce and Nusbaum, Chad and Zody, Michael C and Baldwin, Jennifer and Devon, Keri and Dewar, Ken and Doyle, Michael and FitzHugh, William and Funke, Roel and Gage, Diane and Harris, Katrina and Heaford, Andrew and Howland, John and Kann, Lisa and Lehoczky, Jessica and LeVine, Rosie and McEwan, Paul and McKernan, Kevin and Meldrim, James and Mesirov, Jill P and Miranda, Cher and Morris, William and Naylor, Jerome and Raymond, Christina and Rosetti, Mark and Santos, Ralph and Sheridan, Andrew and Sougnez, Carrie and Stange-Thomann, Nicole and Stojanovic, Nikola and Subramanian, Aravind and Wyman, Dudley and Rogers, Jane and Sulston, John and Ainscough, Rachael and Beck, Stephan and Bentley, David and Burton, John and Clee, Christopher and Carter, Nigel and Coulson, Alan and Deadman, Rebecca and Deloukas, Panos and Dunham, Andrew and Dunham, Ian and Durbin, Richard and French, Lisa and Grafham, Darren and Gregory, Simon and Hubbard, Tim and Humphray, Sean and Hunt, Adrienne and Jones, Matthew and Lloyd, Christine and McMurray, Amanda and Matthews, Lucy and Mercer, Simon and Milne, Sarah and Mullikin, James C and Mungall, Andrew and Plumb, Robert and Ross, Mark and Shownkeen, Ratna and Sims, Sarah and Waterston, Robert H and Wilson, Richard K and Hillier, LaDeana W and McPherson, John D and Marra, Marco A and Mardis, Elaine R and Fulton, Lucinda A and Chinwalla, Asif T and Pepin, Kymberlie H and Gish, Warren R and Chissoe, Stephanie L and Wendl, Michael C and Delehaunty, Kim D and Miner, Tracie L and Delehaunty, Andrew and Kramer, Jason B and Cook, Lisa L and Fulton, Robert S and Johnson, Douglas L and Minx, Patrick J and Clifton, Sandra W and Hawkins, Trevor and Branscomb, Elbert and Predki, Paul and Richardson, Paul and Wenning, Sarah and Slezak, Tom and Doggett, Norman and Cheng, Jan-Fang and Olsen, Anne and Lucas, Susan and Elkin, Christopher and Uberbacher, Edward and Frazier, Marvin and Gibbs, Richard A and Muzny, Donna M and Scherer, Steven E and Bouck, John B and Sodergren, Erica J and Worley, Kim C and Rives, Catherine M and Gorrell, James H and Metzker, Michael L and Naylor, Susan L and Kucherlapati, Raju S and Nelson, David L and Weinstock, George M and Sakaki, Yoshiyuki and Fujiyama, Asao and Hattori, Masahira and Yada, Tetsushi and Toyoda, Atsushi and Itoh, Takehiko and Kawagoe, Chiharu and Watanabe, Hidemi and Totoki, Yasushi and Taylor, Todd and Umr-, CNRS and Weissenbach, Jean and Heilig, Roland and Saurin, William and Artiguenave, Francois and Brottier, Philippe and Bruls, Thomas and Pelletier, Eric and Robert, Catherine and Wincker, Patrick and Smith, Douglas R and Doucette-Stamm, Lynn and Ruben, Marc and Weinstock, Keith and Mei Lee, Hong and Dubois, JoAnn and Â Rosenthal, Andre and Platzer, Matthias and Nyakatura, Gerald and Taudien, Stefan and Rump, Andreas and Yang, Huanming and Yu, Jun and Wang, Jian and Huang, Guyang and Gu, Jun and Genome Technology Center, Stanford and Davis, Ronald W and Federspiel, Nancy A and Pia Abola, A and Proctor, Michael J and Human Genome Center, Stanford and Myers, Richard M and Schmutz, Jeremy and Dickson, Mark and Grimwood, Jane and Cox, David R and Olson, Maynard V and Kaul, Rajinder and Raymond, Christopher and Evans, Glen A and Athanasiou, Maria and Schultz, Roger and Roe, Bruce A and Chen, Feng and Pan, Huaqin and Ramser, Juliane and Lehrach, Hans and Reinhardt, Richard and Richard McCombie, W and de la Bastide, Melissa and Dedhia, Neilay and Blo È cker, Helmut and Hornischer, Klaus and Nordsiek, Gabriele and Agarwala, Richa and Aravind, L and Bailey, Jeffrey A and Bateman, Alex and Birney, Ewan and Bork, Peer and Brown, Daniel G and Burge, Christopher B and Cerutti, Lorenzo and Chen, Hsiu-Chuan and Church, Deanna and Clamp, Michele and Copley, Richard R and Doerks, Tobias and Eddy, Sean R and Eichler, Evan E and Furey, Terrence S and Galagan, James and R Gilbert, James G and Harmon, Cyrus and Hayashizaki, Yoshihide and Haussler, David and Hermjakob, Henning and Hokamp, Karsten and Jang, Wonhee and Steven Johnson, L and Jones, Thomas A and Kasif, Simon and Kaspryzk, Arek and Kennedy, Scot and James Kent, W and Kitts, Paul and Koonin, Eugene V and Korf, Ian and Kulp, David and Lancet, Doron and Lowe, Todd M and McLysaght, Aoife and Mikkelsen, Tarjei and Moran, John V and Mulder, Nicola and Pollara, Victor J and Ponting, Chris P and Schuler, Greg and È rg Schultz, Jo and Slater, Guy and A Smit, Arian F and Stupka, Elia and Szustakowki, Joseph and Thierry-Mieg, Danielle and Thierry-Mieg, Jean and Wagner, Lukas and Wallis, John and Wheeler, Raymond and Williams, Alan and Wolf, Yuri I and Wolfe, Kenneth H and Yang, Shiaw-Pyng and Yeh, Ru-Fang},
	year = {2001},
	note = {Publication Title: NATURE
Volume: 409},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\TZUFLDBT\\full-text.pdf:application/pdf},
}

@misc{OxfordNanopore2018,
	title = {Oxford {Nanopore} {Sequencing} {Products}},
	url = {https://nanoporetech.com/products},
	urldate = {2018-09-15},
	author = {{Oxford Nanopore}},
	year = {2018},
}

@misc{Carlson2017,
	title = {Guesstimating the {Size} of the {Global} {Array} {Synthesis} {Market}},
	url = {http://www.synthesis.cc/synthesis/2017/8/guesstimating-the-size-of-the-global-array-synthesis-market?rq=storage},
	urldate = {2018-09-16},
	author = {Carlson, Rob},
	year = {2017},
}

@article{Biskeborn2018,
	title = {{TMR} tape drive for a 15 {TB} cartridge},
	volume = {8},
	issn = {2158-3226},
	url = {http://aip.scitation.org/doi/10.1063/1.5007788},
	doi = {10.1063/1.5007788},
	abstract = {This paper highlights the development of tunnel magnetoresistive (TMR) sensors for magnetic tape recording applications. This has led to the introduction of a tape drives supporting a 15 TB native tape cartridge, currently the highest capacity available. Underscoring this development is the fact that the TMR sensors must run in continual contact with the tape media. This is contrasted with modern hard disk drive (hdd) sensors, which fly above the disk platters. Various challenges encountered in developing and deploying TMR are presented. In addition, advances to the write transducer are also discussed. Lastly, the authors show that future density scaling for tape recording, unlike that for hdd, is not facing limits imposed by photolithography or superparamagnetic physics, suggesting that cartridge capacity improvements of 4 to 6x will be achieved in the next 4 to 8 years.},
	number = {5},
	urldate = {2018-09-16},
	journal = {AIP Advances},
	author = {Biskeborn, Robert G. and Fontana, Robert E. and Lo, Calvin S. and Czarnecki, W. Stanley and Liang, Jason and Iben, Icko E. T. and Decad, Gary M. and Hipolito, Venus A.},
	month = may,
	year = {2018},
	note = {Publisher: AIP Publishing LLC},
	keywords = {disc drives, hard discs, magnetic recording, magnetic sensors, magnetic tape storage, magnetoresistive devices, tape recorders, tunnelling magnetoresistance},
	pages = {056511},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\HWXAKBEB\\full-text.pdf:application/pdf},
}

@article{Mining2005,
	title = {{STATS784} {FC} {Terms} {Test} {Answers} {Instructions} : {Surname} : {First} name :},
	number = {x},
	author = {Mining, Statistical Data},
	year = {2005},
	pages = {1--10},
	file = {tt2013.pdf:C\:\\Users\\jstacey\\Zotero\\storage\\ECADBVDJ\\tt2013.pdf:application/pdf;tt2014.pdf:C\:\\Users\\jstacey\\Zotero\\storage\\WQQNXSAU\\tt2014.pdf:application/pdf},
}

@article{Baker1999,
	title = {Distribution and diversity of {mtDNA} lineages among southern{\textbackslash}nright whales ({Eubalaena} australis) from {Australia} and {New} {Zealand}},
	volume = {134},
	issn = {0025-3162},
	url = {http://www.springerlink.com/openurl.asp?genre=article&id=doi:10.1007/s002270050519},
	doi = {10.1007/s002270050519},
	abstract = {Abstract\&nbsp;\&nbsp; Using a biopsy dart system, samples of skin tissue were collected from southern right whales (Eubalaena australis) in 1995 on two wintering grounds, southwest Australia (n=20) and the Auckland Islands of New Zealand (n=20); and on offshore feeding grounds at Latitudes 40 to 43, south of Western Australia (n=5). A variable section of the mitochondrial DNA control-region (289 nucleotides) was amplified and sequenced from these 45 individuals (21 males, 20 females and 4 of unknown sex), distinguishing a total of seven unique sequences (i.e. mtDNA haplotypes). Two haplotypes were found on both wintering grounds (including a common type representing 45\% of each sample), and five types were unique to only one wintering ground. An analysis of variance adapted for molecular information revealed significant genetic differentiation between the two wintering grounds (p=0.017). The feeding-ground sample was too small for statistical comparison with the wintering grounds, but included two haplotypes found only in the Auckland Islands as well as the common haplotype found on both wintering grounds. The nucleotide diversity and differentiation of mtDNA among the right whales was similar to that among humpback whales (Megaptera novaeangliae) from the same regions (Baker et al. 1998), but haplotype diversity was significantly reduced, perhaps as a result of more intensive hunting during the last century and continued illegal hunting during this century.},
	number = {1},
	journal = {Marine Biology},
	author = {Baker, C S and Patenaude, N. J. and Bannister, J L and Robins, J and Kato, H},
	year = {1999},
	keywords = {GENETICS, RIGHT WHALE},
	pages = {1--7},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\Y93AYJ7U\\Baker1999.pdf:application/pdf},
}

@article{Of2000,
	title = {{ANALYSIS} {OF} {MITOCHONDRIAL} {DNA} {DIVERSITY} {WITHIN} {AND} {BETWEEN} {NORTH}},
	volume = {16},
	number = {July},
	author = {Of, Analysis and Dna, Mitochondrial and Atlantic, South and Whales, Right},
	year = {2000},
	pages = {545--558},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\WSAD7EZD\\Malik_et_al-2000-Marine_Mammal_Science.pdf:application/pdf},
}

@article{Carroll2011a,
	title = {Population structure and individual movement of southern right whales around {New} {Zealand} and {Australia}},
	volume = {432},
	issn = {0171-8630},
	url = {http://www.int-res.com/prepress/m09145.html},
	doi = {10.3354/meps09145},
	abstract = {During the last 2 centuries, southern right whales Eubalaena australis were hunted to near extinction, and an estimated 150000 were killed by pre-industrial whaling in the 19th century and ille- gal Soviet whaling in the 20th century. Here we focus on the coastal calving grounds of Australia and New Zealand (NZ), where previous work suggests 2 genetically distinct stocks of southern right whales are recovering. Historical migration patterns and spatially variable patterns of recovery suggest each of these stocks are subdivided into 2 stocks: (1) NZ, comprising NZ subantarctic (NZSA) and mainland NZ (MNZ) stocks; and (2) Australia, comprising southwest and southeast stocks. We expand upon previous work to investigate population subdivision by analysing over 1000 samples collected at 6 locations across NZ and Australia, although sample sizes were small from some locations. Mitochondrial DNA (mtDNA) control region haplotypes (500 bp) and microsatellite genotypes (13 loci) were used to identify 707 indi- vidual whales and to test for genetic differentiation. For the first time, we documented the movement of 7 individual whales between the NZSA and MNZ based on the matching of multilocus genotypes. Given the current and historical evidence, we hypothesise that individuals from the NZ subantarctic are slowly recolonising MNZ, where a former calving ground was extirpated. We also suggest that southeast Aus- tralian right whales represent a remnant stock, distinct from the southwest Australian stock, based on significant differentiation in mtDNA haplotype frequencies (FST = 0.15, p {\textless} 0.01; ΦST = 0.12, p = 0.02) and contrasting patterns of recovery. In comparison with significant differences in mtDNA haplotype frequencies found between the 3 proposed stocks (overall FST = 0.07, ΦST = 0.12, p {\textless} 0.001), we found no significant differentiation in microsatellite loci (overall FST = 0.004, G’ST = 0.019, p = 0.07), suggesting ongoing or recent historical reproductive interchange.},
	number = {1998},
	journal = {Marine Ecology Progress Series},
	author = {Carroll, E. and Patenaude, N. J. and Alexander, A. and Steel, Debbie and Harcourt, Robert G. and Childerhouse, S. and Smith, S. and Bannister, J. and Constantine, Rochelle and Baker, Charles Scott},
	year = {2011},
	note = {ISBN: 0022701117},
	keywords = {Eubalaena australis, Microsatellite, mtDNA, Population structure, Southern right whale},
	pages = {257--268},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\5AIWHG5Q\\Carroll 2011_supp.pdf:application/pdf},
}

@article{Risso2017,
	title = {{EDASeq} : {Exploratory} {Data} {Analysis} and {Nor}- malization for {RNA}-{Seq}},
	author = {Risso, Davide},
	year = {2017},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\9MFN5CCW\\EDASeq Vignette DE analysis.pdf:application/pdf},
}

@article{Baker2012,
	title = {Genome-enabled transcriptomics reveals archaeal populations that drive nitrification in a deep-sea hydrothermal plume},
	volume = {6},
	issn = {17517362},
	doi = {10.1038/ismej.2012.64},
	abstract = {Ammonia-oxidizing Archaea (AOA) are among the most abundant microorganisms in the oceans and have crucial roles in biogeochemical cycling of nitrogen and carbon. To better understand AOA inhabiting the deep sea, we obtained community genomic and transcriptomic data from ammonium-rich hydrothermal plumes in the Guaymas Basin (GB) and from surrounding deep waters of the Gulf of California. Among the most abundant and active lineages in the sequence data were marine group I (MGI) Archaea related to the cultured autotrophic ammonia-oxidizer, Nitrosopumilus maritimus. Assembly of MGI genomic fragments yielded 2.9 Mb of sequence containing seven 16S rRNA genes (95.4-98.4\% similar to N. maritimus), including two near-complete genomes and several lower-abundance variants. Equal copy numbers of MGI 16S rRNA genes and ammonia monooxygenase genes and transcription of ammonia oxidation genes indicates that all of these genotypes actively oxidize ammonia. De novo genomic assembly revealed the functional potential of MGI populations and enhanced interpretation of metatranscriptomic data. Physiological distinction from N. maritimus is evident in the transcription of novel genes, including genes for urea utilization, suggesting an alternative source of ammonia. We were also able to determine which genotypes are most active in the plume. Transcripts involved in nitrification were more prominent in the plume and were among the most abundant transcripts in the community. These unique data sets reveal populations of deep-sea AOA thriving in the ammonium-rich GB that are related to surface types, but with key genomic and physiological differences.},
	number = {12},
	journal = {ISME Journal},
	author = {Baker, Brett J. and Lesniewski, Ryan A. and Dick, Gregory J.},
	year = {2012},
	pmid = {22695863},
	note = {ISBN: 1751-7370 (Electronic){\textbackslash}n1751-7362 (Linking)},
	keywords = {ammonia, archaea, metagenomics, metatranscriptomics, nitrification},
	pages = {2269--2279},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ETN5IMHH\\Baker 2012 Transcriptomics archaeal nitrification in deep-sea hydrothermal plume.pdf:application/pdf},
}

@article{Moritz2004,
	title = {{DNA} barcoding: {Promise} and pitfalls},
	volume = {2},
	issn = {15449173},
	doi = {10.1371/journal.pbio.0020354},
	abstract = {Fixed and invariant characteristiclike a on a the general utility of mtDNA across different online; Stoeckle M (2003) Taxonomy, and the},
	number = {10},
	journal = {PLoS Biology},
	author = {Moritz, Craig and Cicero, Carla},
	year = {2004},
	pmid = {15486587},
	note = {ISBN: 1544-9173{\textbackslash}r1545-7885},
	file = {Moritz Cicero 2004.PDF:C\:\\Users\\jstacey\\Zotero\\storage\\2ELEJ5HU\\Moritz Cicero 2004.PDF:application/pdf},
}

@article{Essays1994,
	title = {Defining ‘ {Evolutionarily} {Significant} {Units} ’},
	author = {Essays, T H Issue},
	year = {1994},
	pages = {373--375},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\HIK78MLY\\Moritz 1994.pdf:application/pdf},
}

@article{Dunn2003,
	title = {Keeping taxonomy based in morphology [3]},
	volume = {18},
	issn = {01695347},
	doi = {10.1016/S0169-5347(03)00094-6},
	abstract = {In most countries, the scientific community simply is not prepared to allocate significant resources to taxonomic research of poorly known organism groups, not even from funds dedicated to biodiversity research [Statistics on Swedish},
	number = {6},
	journal = {Trends in Ecology and Evolution},
	author = {Dunn, Christopher P.},
	year = {2003},
	pmid = {2374},
	note = {ISBN: 0169-5347},
	pages = {270--271},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\FE2WKPT8\\Baker et al 2003.pdf:application/pdf},
}

@article{Rosenbaum2000,
	title = {World-wide genetic differentiation of {Eubalaena}: {Questioning} the number of right whale species},
	volume = {9},
	issn = {09621083},
	doi = {10.1046/j.1365-294X.2000.01066.x},
	abstract = {Few studies have examined systematic relationships of right whales (Eubalaena spp.) since the original species descriptions, even though they are one of the most endangered large whales. Little morphological evidence exists to support the current species designa-tions for Eubalaena glacialis in the northern hemisphere and E. australis in the southern hemisphere. Differences in migratory behaviour or antitropical distribution between right whales in each hemisphere are considered a barrier to gene flow and maintain the current species distinctions and geographical populations. However, these distinctions between populations have remained controversial and no study has included an analysis of all right whales from the three major ocean basins. To address issues of genetic dif-ferentiation and relationships among right whales, we have compiled a database of mito-chondrial DNA control region sequences from right whales representing populations in all three ocean basins that consist of: western North Atlantic E. glacialis , multiple geographically distributed populations of E. australis and the first molecular analysis of historical and recent samples of E. glacialis from the western and eastern North Pacific Ocean. Diagnostic characters, as well as phylogenetic and phylogeographic analyses, support the possibility that three distinct maternal lineages exist in right whales, with North Pacific E. glacialis being more closely related to E. australis than to North Atlantic E. glacialis . Our genetic results provide unequivocal character support for the two usually recognized species and a third distinct genetic lineage in the North Pacific under the Phylogenetic Species Concept, as well as levels of genetic diversity among right whales world-wide.},
	number = {11},
	journal = {Molecular Ecology},
	author = {Rosenbaum, H. C. and Brownell, R. L. and Brown, M. W. and Schaeff, C. and Portway, V. and White, B. N. and Malik, S. and Pastene, L. A. and Patenaude, N. J. and Baker, C. S. and Goto, M. and Best, P. B. and Clapham, P. J. and Hamilton, P. and Moore, M. and Payne, R. and Rowntree, V. and Tynan, C. T. and Bannister, J. L. and Desalle, R.},
	year = {2000},
	pmid = {11091315},
	note = {ISBN: 0962-1083},
	keywords = {Cetaceans, Conservation, Phylogeny, Right whales, Taxonomy},
	pages = {1793--1802},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\3J36JZWQ\\Rosenbaum_et_al-2000-Molecular_Ecology.pdf:application/pdf},
}

@article{Rosenbaum1997,
	title = {An effective method for isolating {DNA} from historical specimens of baleen},
	volume = {6},
	issn = {09621083},
	doi = {10.1046/j.1365-294X.1997.00230.x},
	abstract = {DNA was isolated from an early twentieth century museum specimen of northern right whale baleen. A system of stringent controls and a novel set of cetacean specific primers eliminated contamination from external sources and ensured the authenticity of the results. Sequence analysis revealed that there were informative nucleotide positions between the museum specimen and extant members of the population and closely related species. The results indicate that museum specimens of baleen can be used to assess historical genetic population structure of the great whales.},
	number = {7},
	journal = {Molecular Ecology},
	author = {Rosenbaum, H. C. and Egan, M. G. and Clapham, P. J. and Brownell, R. L. and Desalle, R.},
	year = {1997},
	pmid = {9226948},
	note = {ISBN: 0962-1083},
	keywords = {Ancient DNA, Mitochondrial DNA, Cetacea, Control region, Eubalaena glacialis, Historical population genetics, Museum specimens},
	pages = {677--681},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\YS8ZKVX2\\Rosenbaum_et_al-1997-Molecular_Ecology.pdf:application/pdf},
}

@article{Rossberg2013,
	title = {Are there species smaller than 1 mm ? {Are} there species smaller than 1 mm ?},
	number = {July},
	author = {Rossberg, Axel G and Rogers, Tim and Mckane, Alan J and Rossberg, Axel G and Rogers, Tim and Mckane, Alan J},
	year = {2013},
	keywords = {evolution, ecology, theoretical biology},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\MLIIVJGC\\Rossberg 2013.pdf:application/pdf},
}

@article{Patenaude2007,
	title = {Mitochondrial {DNA} diversity and population structure among southern right whales ({Eubalaena} australis)},
	volume = {98},
	issn = {00221503},
	doi = {10.1093/jhered/esm005},
	abstract = {The population structure and mitochondrial (mt) DNA diversity of southern right whales (Eubalaena australis) are described from 146 individuals sampled on 4 winter calving grounds (Argentina, South Africa, Western Australia, and the New Zealand sub-Antarctic) and 2 summer feeding grounds (South Georgia and south of Western Australia). Based on a consensus region of 275 base pairs of the mtDNA control region, 37 variable sites defined 37 unique haplotypes, of which only one was shared between regional samples of the Indo-Pacific and South Atlantic Oceans. Phylogenetic reconstruction of the southern right whale haplotypes revealed 2 distinct clades that differed significantly in frequencies between oceans. An analysis of molecular variance confirmed significant overall differentiation among the 4 calving grounds at both the haplotype and the nucleotype levels (FST = 0.159; \{Phi\}ST = 0.238; P {\textless} 0.001). Haplotype diversity was significantly lower in the Indo-Pacific (h = 0.701 \{+/-\} 0.037) compared with the South Atlantic (h = 0.948 \{+/-\} 0.013), despite a longer history of exploitation and larger catches in the South Atlantic. In fact, the haplotype diversity in the Indo-Pacific basin was similar to that of the North Atlantic right whale that currently numbers about 300 animals. Multidimensional scaling of genetic differentiation suggests that gene flow occurred primarily between adjacent calving grounds within an ocean basin, with mixing of lineages from different calving grounds occurring on feeding grounds},
	number = {2},
	journal = {Journal of Heredity},
	author = {Patenaude, N. J. and Portway, Vicky A. and Schaeff, Cathy M. and Bannister, John L. and Best, Peter B. and Payne, Roger S. and Rowntree, Vicky J. and Rivarola, Mariana and Baker, C. Scott},
	year = {2007},
	pmid = {1569},
	note = {ISBN: 0022-1503},
	pages = {147--157},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\9ULLWBNZ\\Patnaude 2007.pdf:application/pdf},
}

@article{Guide2013,
	title = {Metagenome {Assembly} {Guide}},
	author = {Guide, Metagenome Assembly},
	year = {2013},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\6F7DW28Y\\Metagenome assembly guide v1.0.pdf:application/pdf},
}

@article{Mutschler2015,
	title = {Freeze-thaw cycles as drivers of complex ribozyme assembly},
	volume = {7},
	issn = {17554349},
	doi = {10.1038/nchem.2251},
	abstract = {The emergence of an RNA catalyst capable of self-replication is considered a key transition in the origin of life. However, how such replicase ribozymes emerged from the pools of short RNA oligomers arising from prebiotic chemistry and non-enzymatic replication is unclear. Here we show that RNA polymerase ribozymes can assemble from simple catalytic networks of RNA oligomers no longer than 30 nucleotides. The entropically disfavoured assembly reaction is driven by iterative freeze-thaw cycles, even in the absence of external activation chemistry. The steep temperature and concentration gradients of such cycles result in an RNA chaperone effect that enhances the otherwise only partially realized catalytic potential of the RNA oligomer pool by an order of magnitude. Our work outlines how cyclic physicochemical processes could have driven an expansion of RNA compositional and phenotypic complexity from simple oligomer pools.},
	number = {6},
	journal = {Nature Chemistry},
	author = {Mutschler, Hannes and Wochner, Aniela and Holliger, Philipp},
	year = {2015},
	pmid = {25991529},
	note = {ISBN: 1755-4330},
	pages = {502--508},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\67WTZ9ZC\\Mutschler-et-al_2015_with-guide.pdf:application/pdf},
}

@article{Lehmann2007,
	title = {Molecular basis of {RNA}-dependent {RNA} polymerase {II} activity},
	volume = {450},
	issn = {14764687},
	doi = {10.1038/nature06290},
	abstract = {RNA polymerase (Pol) II catalyses DNA-dependent RNA synthesis during gene transcription. There is, however, evidence that Pol II also possesses RNA-dependent RNA polymerase (RdRP) activity. Pol II can use a homopolymeric RNA template, can extend RNA by several nucleotides in the absence of DNA, and has been implicated in the replication of the RNA genomes of hepatitis delta virus (HDV) and plant viroids. Here we show the intrinsic RdRP activity of Pol II with only pure polymerase, an RNA template-product scaffold and nucleoside triphosphates (NTPs). Crystallography reveals the template-product duplex in the site occupied by the DNA-RNA hybrid during transcription. RdRP activity resides at the active site used during transcription, but it is slower and less processive than DNA-dependent activity. RdRP activity is also obtained with part of the HDV antigenome. The complex of transcription factor IIS (TFIIS) with Pol II can cleave one HDV strand, create a reactive stem-loop in the hybrid site, and extend the new RNA 3' end. Short RNA stem-loops with a 5' extension suffice for activity, but their growth to a critical length apparently impairs processivity. The RdRP activity of Pol II provides a missing link in molecular evolution, because it suggests that Pol II evolved from an ancient replicase that duplicated RNA genomes.},
	number = {7168},
	journal = {Nature},
	author = {Lehmann, Elisabeth and Brueckner, Florian and Cramer, Patrick},
	year = {2007},
	pmid = {18004386},
	note = {ISBN: 1476-4687 (Electronic)},
	pages = {445--449},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\DPVWS4IN\\Lehmann_E_Nature_2007_RNA-Pol.pdf:application/pdf},
}

@article{Takeuchi2018f,
	title = {{BIOINF703} : {Origin} of {Genomes}},
	author = {Takeuchi, Nobuto},
	year = {2018},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\TZZMMI65\\lecture01-hide-181007.pdf:application/pdf},
}

@article{Takeuchi2018d,
	title = {{BIOINF703} : {Origin} of {Genomes}},
	author = {Takeuchi, Nobuto},
	year = {2018},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\W4BFVMSR\\lecture02-hide-181005.pdf:application/pdf},
}

@article{Nielsen2013,
	title = {This {PDF} file includes:},
	author = {Nielsen, Per H},
	year = {2013},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\JVLZD6TJ\\Metagenomics - Albertsen 2013 Genome binning SOM-1.pdf:application/pdf},
}

@article{Albertsen2013,
	title = {Genome sequences of rare, uncultured bacteria obtained by differential coverage binning of multiple metagenomes},
	volume = {31},
	issn = {10870156},
	doi = {10.1038/nbt.2579},
	abstract = {Reference genomes are required to understand the diverse roles of microorganisms in ecology, evolution, human and animal health, but most species remain uncultured. Here we present a sequence composition-independent approach to recover high-quality microbial genomes from deeply sequenced metagenomes. Multiple metagenomes of the same community, which differ in relative population abundances, were used to assemble 31 bacterial genomes, including rare ({\textless}1\% relative abundance) species, from an activated sludge bioreactor. Twelve genomes were assembled into complete or near-complete chromosomes. Four belong to the candidate bacterial phylum TM7 and represent the most complete genomes for this phylum to date (relative abundances, 0.06-1.58\%). Reanalysis of published metagenomes reveals that differential coverage binning facilitates recovery of more complete and higher fidelity genome bins than other currently used methods, which are primarily based on sequence composition. This approach will be an important addition to the standard metagenome toolbox and greatly improve access to genomes of uncultured microorganisms.},
	number = {6},
	journal = {Nature Biotechnology},
	author = {Albertsen, Mads and Hugenholtz, Philip and Skarshewski, Adam and Nielsen, Kåre L. and Tyson, Gene W. and Nielsen, Per H.},
	year = {2013},
	pmid = {23707974},
	note = {ISBN: 1546-1696 (Electronic){\textbackslash}r1087-0156 (Linking)},
	pages = {533--538},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\SPVWNITB\\Metagenomics - Albertsen 2013 Genome binning by differential coverage binning-1.pdf:application/pdf},
}

@article{Bendall2016,
	title = {Genome-wide selective sweeps and gene-specific sweeps in natural bacterial populations},
	volume = {10},
	issn = {17517370},
	url = {http://dx.doi.org/10.1038/ismej.2015.241},
	doi = {10.1038/ismej.2015.241},
	abstract = {Multiple models describe the formation and evolution of distinct microbial phylogenetic groups. These evolutionary models make different predictions regarding how adaptive alleles spread through populations and how genetic diversity is maintained. Processes predicted by competing evolutionary models, for example, genome-wide selective sweeps vs gene-specific sweeps, could be captured in natural populations using time-series metagenomics if the approach were applied over a sufficiently long time frame. Direct observations of either process would help resolve how distinct microbial groups evolve. Here, from a 9-year metagenomic study of a freshwater lake (2005–2013), we explore changes in single-nucleotide polymorphism (SNP) frequencies and patterns of gene gain and loss in 30 bacterial populations. SNP analyses revealed substantial genetic heterogeneity within these populations, although the degree of heterogeneity varied by {\textgreater}1000-fold among populations. SNP allele frequencies also changed dramatically over time within some populations. Interestingly, nearly all SNP variants were slowly purged over several years from one population of green sulfur bacteria, while at the same time multiple genes either swept through or were lost from this population. These patterns were consistent with a genome-wide selective sweep in progress, a process predicted by the ‘ecotype model’ of speciation but not previously observed in nature. In contrast, other populations contained large, SNP-free genomic regions that appear to have swept independently through the populations prior to the study without purging diversity elsewhere in the genome. Evidence for both genome-wide and gene-specific sweeps suggests that different models of bacterial speciation may apply to different populations coexisting in the same environment.},
	number = {7},
	journal = {ISME Journal},
	author = {Bendall, Matthew L. and Stevens, Sarah L.R. and Chan, Leong Keat and Malfatti, Stephanie and Schwientek, Patrick and Tremblay, Julien and Schackwitz, Wendy and Martin, Joel and Pati, Amrita and Bushnell, Brian and Froula, Jeff and Kang, Dongwan and Tringe, Susannah G. and Bertilsson, Stefan and Moran, Mary A. and Shade, Ashley and Newton, Ryan J. and McMahon, Katherine D. and Malmstrom, Rex R.},
	year = {2016},
	pmid = {26744812},
	note = {Publisher: Nature Publishing Group
ISBN: 1751-7362},
	pages = {1589--1601},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\N7CK3PA9\\Bendall 2016 Genome-wide selective sweeps and gene-specific sweeps in natural bacterial populations.pdf:application/pdf},
}

@article{Sharon2013,
	title = {Time series community genomics analysis reveals rapid shifts in bacterial species, strains, and phage during infant gut colonization},
	volume = {23},
	issn = {10889051},
	doi = {10.1101/gr.142315.112},
	abstract = {The gastrointestinal microbiome undergoes shifts in species and strain abundances, yet dynamics involving closely related microorganisms remain largely unknown because most methods cannot resolve them. We developed new metagenomic methods and utilized them to track species and strain level variations in microbial communities in 11 fecal samples collected from a premature infant during the first month of life. Ninety six percent of the sequencing reads were assembled into scaffolds of {\textgreater}500 bp in length that could be assigned to organisms at the strain level. Six essentially complete (∼99\%) and two near-complete genomes were assembled for bacteria that comprised as little as 1\% of the community, as well as nine partial genomes of bacteria representing as little as 0.05\%. In addition, three viral genomes were assembled and assigned to their hosts. The relative abundance of three Staphylococcus epidermidis strains, as well as three phages that infect them, changed dramatically over time. Genes possibly related to these shifts include those for resistance to antibiotics, heavy metals, and phage. At the species level, we observed the decline of an early-colonizing Propionibacterium acnes strain similar to SK137 and the proliferation of novel Propionibacterium and Peptoniphilus species late in colonization. The Propionibacterium species differed in their ability to metabolize carbon compounds such as inositol and sialic acid, indicating that shifts in species composition likely impact the metabolic potential of the community. These results highlight the benefit of reconstructing complete genomes from metagenomic data and demonstrate methods for achieving this goal.},
	number = {1},
	journal = {Genome Research},
	author = {Sharon, Itai and Morowitz, Michael J. and Thomas, Brian C. and Costello, Elizabeth K. and Relman, David A. and Banfield, Jillian F.},
	year = {2013},
	pmid = {22936250},
	note = {ISBN: 1549-5469 (Electronic){\textbackslash}n1088-9051 (Linking)},
	pages = {111--120},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\KAKIAXVF\\Metagenomics - Sharon 2013 Time series community genomics of infant gut.pdf:application/pdf},
}

@article{Baker2015,
	title = {Genomic resolution of linkages in carbon, nitrogen, and sulfur cycling among widespread estuary sediment bacteria},
	volume = {3},
	issn = {2049-2618},
	url = {http://www.microbiomejournal.com/content/3/1/14},
	doi = {10.1186/s40168-015-0077-6},
	abstract = {BACKGROUND: Estuaries are among the most productive habitats on the planet. Bacteria in estuary sediments control the turnover of organic carbon and the cycling of nitrogen and sulfur. These communities are complex and primarily made up of uncultured lineages, thus little is known about how ecological and metabolic processes are partitioned in sediments.{\textbackslash}n{\textbackslash}nRESULTS: De novo assembly and binning resulted in the reconstruction of 82 bacterial genomes from different redox regimes of estuary sediments. These genomes belong to 23 bacterial groups, including uncultured candidate phyla (for example, KSB1, TA06, and KD3-62) and three newly described phyla (White Oak River (WOR)-1, WOR-2, and WOR-3). The uncultured phyla are generally most abundant in the sulfate-methane transition (SMTZ) and methane-rich zones, and genomic data predict that they mediate essential biogeochemical processes of the estuarine environment, including organic carbon degradation and fermentation. Among the most abundant organisms in the sulfate-rich layer are novel Gammaproteobacteria that have genes for the oxidation of sulfur and the reduction of nitrate and nitrite. Interestingly, the terminal steps of denitrification (NO3 to N2O and then N2O to N2) are present in distinct bacterial populations.{\textbackslash}n{\textbackslash}nCONCLUSIONS: This dataset extends our knowledge of the metabolic potential of several uncultured phyla. Within the sediments, there is redundancy in the genomic potential in different lineages, often distinct phyla, for essential biogeochemical processes. We were able to chart the flow of carbon and nutrients through the multiple geochemical layers of bacterial processing and reveal potential ecological interactions within the communities.},
	number = {1},
	journal = {Microbiome},
	author = {Baker, Brett J and Lazar, Cassandre Sara and Teske, Andreas P and Dick, Gregory J},
	year = {2015},
	pmid = {25922666},
	note = {Publisher: ???
ISBN: 2049-2618},
	keywords = {anaerobic respiration, candidate phyla, carbon, Carbon, estuary, Estuary, metagenome, Metagenome, nitrogen, Nitrogen, sediment, Sediment, sulfur, Sulfur, Estuary,Sediment,Metagenome,Sulfur,Nitrogen,Carbon},
	pages = {14},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\QCEI795D\\Baker 2015 Genomic linkages in C-N-S cycling in estuary sediment.pdf:application/pdf},
}

@article{Sciences2004,
	title = {Community structure and metabolism through reconstruction of microbial genomes from the environment {Acid} {Mine} {Drainage} ( {AMD} ) {AMD} in {Fireway} region of {Richmond} mine at {Iron} {Mountain} , {CA} {Pink} {Biofilm}},
	volume = {428},
	number = {March},
	author = {Sciences, Planetary and Creek, Walnut},
	year = {2004},
	pages = {1--6},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\X7EYLCHL\\Tyson 2004 Community genomics.pdf:application/pdf},
}

@article{Kantor2013,
	title = {Small genomes and sparse metabolisms of sediment-associated bacteria from four candidate phyla},
	volume = {4},
	issn = {21612129},
	doi = {10.1128/mBio.00708-13},
	abstract = {UNLABELLED: Cultivation-independent surveys of microbial diversity have revealed many bacterial phyla that lack cultured representatives. These lineages, referred to as candidate phyla, have been detected across many environments. Here, we deeply sequenced microbial communities from acetate-stimulated aquifer sediment to recover the complete and essentially complete genomes of single representatives of the candidate phyla SR1, WWE3, TM7, and OD1. All four of these genomes are very small, 0.7 to 1.2 Mbp, and have large inventories of novel proteins. Additionally, all lack identifiable biosynthetic pathways for several key metabolites. The SR1 genome uses the UGA codon to encode glycine, and the same codon is very rare in the OD1 genome, suggesting that the OD1 organism could also transition to alternate coding. Interestingly, the relative abundance of the members of SR1 increased with the appearance of sulfide in groundwater, a pattern mirrored by a member of the phylum Tenericutes. All four genomes encode type IV pili, which may be involved in interorganism interaction. On the basis of these results and other recently published research, metabolic dependence on other organisms may be widely distributed across multiple bacterial candidate phyla.{\textbackslash}n{\textbackslash}nIMPORTANCE: Few or no genomic sequences exist for members of the numerous bacterial phyla lacking cultivated representatives, making it difficult to assess their roles in the environment. This paper presents three complete and one essentially complete genomes of members of four candidate phyla, documents consistently small genome size, and predicts metabolic capabilities on the basis of gene content. These metagenomic analyses expand our view of a lifestyle apparently common across these candidate phyla.},
	number = {5},
	journal = {mBio},
	author = {Kantor, Rose S. and Wrighton, Kelly C. and Handley, Kim M. and Sharon, Itai and Hug, Laura A. and Castelle, Cindy J. and Thomas, Brian C. and Banfield, Jillian F.},
	year = {2013},
	pmid = {24149512},
	note = {ISBN: 2150-7511 (Electronic)},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\4GVDATRA\\Metagenomics - Kantor 2013 Small Genomes and Sparse Metabolisms of Sediment-Associated Bacteria.pdf:application/pdf},
}

@article{Robinson2009,
	title = {{edgeR}: {A} {Bioconductor} package for differential expression analysis of digital gene expression data},
	volume = {26},
	issn = {14602059},
	doi = {10.1093/bioinformatics/btp616},
	abstract = {Summary: It is expected that emerging digital gene expression (DGE) technologies will overtake microarray technologies in the near future for many functional genomics applications. One of the fundamental data analysis tasks, especially for gene expression studies, involves determining whether there is evidence that counts for a transcript or exon are significantly different across experimental conditions. edgeR is a Bioconductor software package for examining differential expression of replicated count data. An over-dispersed Poisson model is used to account for both biological and technical variability. Empirical Bayes methods are used to moderate the degree of over-dispersion across transcripts, improving the reliability of inference. The methodology can be used even with the most minimal levels of replication, provided at least one phenotype or experimental condition is replicated. The software may have other applications beyond sequencing data, such as proteome peptide count data. Availability: The package is freely available under the LGPL licence from the Bioconductor web site (http://bioconductor.org). Contact: mrobinson@wehi.edu.au},
	number = {1},
	journal = {Bioinformatics},
	author = {Robinson, Mark D. and McCarthy, Davis J. and Smyth, Gordon K.},
	year = {2009},
	pmid = {19910308},
	note = {ISBN: 1367-4811 (Electronic){\textbackslash}n1367-4803 (Linking)},
	pages = {139--140},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\6Y9TQBHY\\Robinson 2010 edgeR - a Bioconductor package for differential expression analysis.pdf:application/pdf},
}

@article{Dick2009,
	title = {Community-wide analysis of microbial genome sequence signatures},
	volume = {10},
	issn = {14747596},
	doi = {10.1186/gb-2009-10-8-r85},
	abstract = {BACKGROUND: Analyses of DNA sequences from cultivated microorganisms have revealed genome-wide, taxa-specific nucleotide compositional characteristics, referred to as genome signatures. These signatures have far-reaching implications for understanding genome evolution and potential application in classification of metagenomic sequence fragments. However, little is known regarding the distribution of genome signatures in natural microbial communities or the extent to which environmental factors shape them.{\textbackslash}n{\textbackslash}nRESULTS: We analyzed metagenomic sequence data from two acidophilic biofilm communities, including composite genomes reconstructed for nine archaea, three bacteria, and numerous associated viruses, as well as thousands of unassigned fragments from strain variants and low-abundance organisms. Genome signatures, in the form of tetranucleotide frequencies analyzed by emergent self-organizing maps, segregated sequences from all known populations sharing {\textless} 50 to 60\% average amino acid identity and revealed previously unknown genomic clusters corresponding to low-abundance organisms and a putative plasmid. Signatures were pervasive genome-wide. Clusters were resolved because intra-genome differences resulting from translational selection or protein adaptation to the intracellular (pH approximately 5) versus extracellular (pH approximately 1) environment were small relative to inter-genome differences. We found that these genome signatures stem from multiple influences but are primarily manifested through codon composition, which we propose is the result of genome-specific mutational biases.{\textbackslash}n{\textbackslash}nCONCLUSIONS: An important conclusion is that shared environmental pressures and interactions among coevolving organisms do not obscure genome signatures in acid mine drainage communities. Thus, genome signatures can be used to assign sequence fragments to populations, an essential prerequisite if metagenomics is to provide ecological and biochemical insights into the functioning of microbial communities.},
	number = {8},
	journal = {Genome Biology},
	author = {Dick, Gregory J. and Andersson, Anders F. and Baker, Brett J. and Simmons, Sheri L. and Thomas, Brian C. and Yelton, A. Pepper and Banfield, Jillian F.},
	year = {2009},
	pmid = {19698104},
	note = {ISBN: 1474-760X},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\GGK6XLE2\\Metagenomics - Dick 2009 Metagenomics and tetranucleotide freqs.pdf:application/pdf},
}

@article{Genomics,
	title = {Using metagenomics to understand complex interactions between organism and environment},
	author = {Genomics, Environmental},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\5YRVV87X\\Bioinf-703-2018-metagenomics-intro-slides.pdf:application/pdf},
}

@article{Bentley2004,
	title = {Comparative {Genomic} {Structure} of {Prokaryotes}},
	volume = {38},
	issn = {0066-4197},
	url = {http://www.annualreviews.org/doi/10.1146/annurev.genet.38.072902.094318},
	doi = {10.1146/annurev.genet.38.072902.094318},
	abstract = {▪ Abstract Recent advances in DNA-sequencing technologies have made available an enormous resource of data for the study of bacterial genomes. The broad sample of complete genomes currently available allows us to look at variation in the gross features and characteristics of genomes while the detail of the sequences reveal some of the mechanisms by which these genomes evolve. This review aims to describe bacterial genome structures according to current knowledge and proposed hypotheses. We also describe examples where mechanisms of genome evolution have acted in the adaptation of bacterial species to particular niches.},
	number = {1},
	journal = {Annual Review of Genetics},
	author = {Bentley, Stephen D. and Parkhill, Julian},
	year = {2004},
	pmid = {15568993},
	note = {ISBN: 0066-4197 (Print){\textbackslash}n0066-4197 (Linking)},
	keywords = {evolution, abstract recent advances in, an enormous resource of, available, available allows us to, bacteria, data for the study, dna-sequencing technologies have made, genome, look at variation in, of bacterial genomes, of complete genomes currently, rearrangement, structure, the broad sample, the gross},
	pages = {771--791},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\GDNYNA2M\\Bentley 2004 Comparative genomic structure of prokaryotes REVIEW.pdf:application/pdf},
}

@article{Handley2017,
	title = {Metabolic and spatio-taxonomic response of uncultivated seafloor bacteria following the {Deepwater} {Horizon} oil spill},
	volume = {11},
	issn = {17517370},
	doi = {10.1038/ismej.2017.110},
	abstract = {Metabolic and spatio-taxonomic response of uncultivated seafloor bacteria following the Deepwater Horizon oil spill},
	number = {11},
	journal = {ISME Journal},
	author = {Handley, K. M. and Piceno, Y. M. and Hu, P. and Tom, L. M. and Mason, O. U. and Andersen, G. L. and Jansson, J. K. and Gilbert, J. A.},
	year = {2017},
	pmid = {28777379},
	note = {arXiv: http://dx.doi.org/10.1101/084707.
ISBN: 1751-7362 1751-7370},
	pages = {2569--2583},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\984QT3EB\\Handley et al 2017 Metabolic response to DWH - ISME J.pdf:application/pdf},
}

@article{Chen2014,
	title = {of digital gene expression data {User} ’ s {Guide}},
	number = {September},
	author = {Chen, Yunshun and Mccarthy, Davis and Robinson, Mark and Smyth, Gordon K},
	year = {2014},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\4TLSCPRY\\edgeR-UsersGuide 2008 - revised April 2018.pdf:application/pdf},
}

@article{huelsenbeck2001mrbayes,
	title = {{MRBAYES}: {Bayesian} inference of phylogenetic trees},
	volume = {17},
	number = {8},
	journal = {Bioinformatics},
	author = {Huelsenbeck, John P and Ronquist, Fredrik},
	year = {2001},
	note = {Publisher: Oxford University Press},
	pages = {754--755},
}

@article{excoffier2005arlequin,
	title = {Arlequin (version 3.0): an integrated software package for population genetics data analysis},
	volume = {1},
	journal = {Evolutionary bioinformatics},
	author = {Excoffier, Laurent and Laval, Guillaume and Schneider, Stefan},
	year = {2005},
	note = {Publisher: SAGE Publications Sage UK: London, England},
	pages = {117693430500100003},
}

@article{doi:10.1093/sysbio/syy032,
	title = {Posterior {Summarization} in {Bayesian} {Phylogenetics} {Using} {Tracer} 1.7},
	volume = {67},
	url = {http://dx.doi.org/10.1093/sysbio/syy032},
	doi = {10.1093/sysbio/syy032},
	number = {5},
	journal = {Systematic Biology},
	author = {Rambaut, Andrew and Drummond, Alexei J and Xie, Dong and Baele, Guy and Suchard, Marc A},
	year = {2018},
	pages = {901--904},
}

@article{10.1371/journal.pcbi.1003537,
	title = {{BEAST} 2: {A} {Software} {Platform} for {Bayesian} {Evolutionary} {Analysis}},
	volume = {10},
	url = {https://doi.org/10.1371/journal.pcbi.1003537},
	doi = {10.1371/journal.pcbi.1003537},
	abstract = {We present a new open source, extensible and flexible software platform for Bayesian evolutionary analysis called BEAST 2. This software platform is a re-design of the popular BEAST 1 platform to correct structural deficiencies that became evident as the BEAST 1 software evolved. Key among those deficiencies was the lack of post-deployment extensibility. BEAST 2 now has a fully developed package management system that allows third party developers to write additional functionality that can be directly installed to the BEAST 2 analysis platform via a package manager without requiring a new software release of the platform. This package architecture is showcased with a number of recently published new models encompassing birth-death-sampling tree priors, phylodynamics and model averaging for substitution models and site partitioning. A second major improvement is the ability to read/write the entire state of the MCMC chain to/from disk allowing it to be easily shared between multiple instances of the BEAST software. This facilitates checkpointing and better support for multi-processor and high-end computing extensions. Finally, the functionality in new packages can be easily added to the user interface (BEAUti 2) by a simple XML template-based mechanism because BEAST 2 has been re-designed to provide greater integration between the analysis engine and the user interface so that, for example BEAST and BEAUti use exactly the same XML file format.},
	number = {4},
	journal = {PLOS Computational Biology},
	author = {Bouckaert, Remco R. and Heled, Joseph and Kühnert, Denise and Vaughan, Tim and Wu, Chieh-Hsi and Xie, Dong and Suchard, Marc A and Rambaut, Andrew and Drummond, Alexei J.},
	year = {2014},
	note = {Publisher: Public Library of Science},
	pages = {1--6},
}

@misc{rambaut2016tracer,
	title = {Tracer v1. 6. 2014},
	author = {Rambaut, Andrew and Suchard, Marc A and Xie, D and Drummond, Alexei J},
	year = {2016},
}

@article{Guarracino2010,
	title = {Noninvasive {Ventilation} for {Awake} {Percutaneous} {Aortic} {Valve} {Implantation} in {High}-{Risk} {Respiratory} {Patients}: {A} {Case} {Series}.},
	volume = {294},
	issn = {1532-8422},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/20857278},
	doi = {10.1053/j.jvca.2010.06.032},
	number = {24},
	journal = {Journal of cardiothoracic and vascular anesthesia},
	author = {Guarracino, Fabio and Cabrini, Luca and Baldassarri, Rubia and Petronio, Sonia and De Carlo, Marco and Covello, Remo Daniel and Landoni, Giovanni and Gabbrielli, Luciano and Ambrosino, Nicolino},
	year = {2010},
	pmid = {20829068},
	note = {arXiv: 1305.4210
ISBN: 0892-0915 (Print){\textbackslash}r0892-0915 (Linking)},
	pages = {3124--3130},
}

@article{Takeuchi2018a,
	title = {{BIOINF703} : {Origin} of {Genomes}},
	author = {Takeuchi, Nobuto},
	year = {2018},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\93T8PF2X\\lecture01-show-181007.pdf:application/pdf},
}

@article{tamura1993estimation,
	title = {Estimation of the number of nucleotide substitutions in the control region of mitochondrial {DNA} in humans and chimpanzees.},
	volume = {10},
	number = {3},
	journal = {Molecular biology and evolution},
	author = {Tamura, Koichiro and Nei, Masatoshi},
	year = {1993},
	pages = {512--526},
}

@article{Mate2011,
	title = {Coastal, offshore, and migratory movements of {South} {African} right whales revealed by satellite telemetry},
	volume = {27},
	issn = {08240469},
	url = {http://doi.wiley.com/10.1111/j.1748-7692.2010.00412.x},
	doi = {10.1111/j.1748-7692.2010.00412.x},
	number = {3},
	urldate = {2018-10-18},
	journal = {Marine Mammal Science},
	author = {Mate, B. R. and Best, P. B. and Lagerquist, B. A. and Winsor, M. H.},
	month = jul,
	year = {2011},
	note = {Publisher: Wiley/Blackwell (10.1111)},
	keywords = {Eubalaena australis, Antarctic Polar Front, Argos telemetry, foraging, migration, movements, South Africa, southern right whale, Subtropical Convergence},
	pages = {455--476},
}

@article{Bannister2001,
	title = {Status of southern right whales ({Eubalaena} australis) off {Australia}},
	volume = {2},
	abstract = {The history of Australian right whaling is briefly reviewed. Most catching took place in the first half of the 19th century, with a peak in the 1830s, involving bay whaling by locals and visiting whaleships in winter and whaling offshore in the summer. In the early 20th century, right whales were regarded as at least very rare, if not extinct. The first published scientific record for Australian waters in the 20th century was a sighting near Albany, Western Australia, in 1955. Increasing sightings close to the coast in winter and spring led to annual aerial surveys off southern Western Australia from 1976. To allow for possible effects of coastwise movements, coverage was extended into South Australian waters from 1993. Evidence from 19th century pelagic catch locations, recent sightings surveys, 1960s Soviet catch data and photographically-identified individuals is beginning to confirm earlier views about likely seasonal movements to and from warm water coastal breeding grounds and colder water feeding grounds. Increase rates of ca 7-13\% have been observed since 1983. Some effects of different breeding female cohort strength are now beginning to appear. A minimum population size of ca 700 for the period 1995-97 is suggested for the bulk of the 'Australian' population, i.e. animals approaching the ca 2,000km of coast between Cape Leeuwin, Western Australia and Ceduna, South Australia.},
	journal = {Journal of Cetacean Research \& Management Special Issue.},
	author = {Bannister, John},
	year = {2001},
	pmid = {200200312983},
	note = {ISBN: 1561-073X},
	keywords = {Cetaceans, Cetacea, [00512] General biology - Conservation and resourc, [07002] Behavioral biology - General and comparati, [07003] Behavioral biology - Animal behavior, [07502] Ecology: environmental biology - General a, [07508] Ecology: environmental biology - Animal, [07516] Ecology: environmental biology - Wildlife, [62800] Animal distribution, [85805] Balaenidae, Balaenidae: Animals, Behavior, Biogeography: Population Studies, Chordates, distrib, Eubalaena australis: southern right whale, historical whaling, Mammalia, Mammals, population size, population status, seasonal movements, Vertebrata, Wildlife Management: Conservation, [85805] Balaenidae, Cetacea, Mammalia, Vertebrata,, Balaenidae: Animals, Cetaceans, Chordates, Mammals, Eubalaena australis: southern right whale, distrib},
	pages = {103--110},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\2NRMP6AJ\\RS278_SI_2p103_110Bannister.pdf:application/pdf},
}

@article{Takeuchi2018b,
	title = {{BIOINF703} : {Origin} of {Genomes}},
	author = {Takeuchi, Nobuto},
	year = {2018},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\G2V873BY\\lecture04-show-181015.pdf:application/pdf},
}

@article{Takeuchi2018,
	title = {{BIOINF703} : {Origin} of {Genomes}},
	author = {Takeuchi, Nobuto},
	year = {2018},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\J42TEW5U\\lecture02-show-181009.pdf:application/pdf},
}

@article{Takeuchi2018g,
	title = {{BIOINF703} : {Origin} of {Genomes}},
	author = {Takeuchi, Nobuto},
	year = {2018},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\2M97NRE3\\lecture03-show-181010.pdf:application/pdf},
}

@article{Nature2005,
	title = {How to construct a {Nature} summary paragraph},
	volume = {118},
	issn = {0365110X},
	doi = {10.1107/S0365110X59001529},
	abstract = {Nature guide to authors: Summary paragraph for Letters},
	number = {May},
	journal = {Nature},
	author = {{Nature}},
	year = {2005},
	pmid = {12891355},
	note = {ISBN: 0365-110X},
	pages = {2005},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\YB659P9H\\2c_Summary_para.pdf:application/pdf},
}

@article{Takeuchia,
	title = {Lab 6 {Guide} 1},
	author = {Takeuchi, Nobuto},
	pages = {1--3},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\PQEXJ9I9\\guide.pdf:application/pdf},
}

@article{Takeuchi2018c,
	title = {{BIOINF703} : {Origin} of {Genomes}},
	author = {Takeuchi, Nobuto},
	year = {2018},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\QDG872JD\\lecture05-show-181016.pdf:application/pdf},
}

@article{Takeuchi,
	title = {Lab 6 {Guide} 2},
	number = {1},
	author = {Takeuchi, Nobuto},
	pages = {1--7},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\IZEG8N27\\guide2.pdf:application/pdf},
}

@article{Carol2015a,
	title = {{IL}-16 represses {HIV}-1 promoter activity.},
	volume = {158},
	number = {1},
	journal = {J Immunol.},
	author = {Maciaszek, JW and Parada, NA and Cruikshank, WW and Center, DM and Kornfeld, H and Viglianti, GA},
	year = {1997},
	pages = {5--8},
}

@inproceedings{Heckel2017,
	title = {Fundamental limits of {DNA} storage systems},
	isbn = {978-1-5090-4096-4},
	doi = {10.1109/ISIT.2017.8007106},
	abstract = {Due to its longevity and enormous information density, DNA is an attractive medium for archival storage. In this work, we study the fundamental limits and tradeoffs of DNA-based storage systems under a simple model, motivated by current technological constraints on DNA synthesis and sequencing. Our model captures two key distinctive aspects of DNA storage systems: (1) the data is written onto many short DNA molecules that are stored in an unordered way and (2) the data is read by randomly sampling from this DNA pool. Under this model, we characterize the storage capacity, and show that a simple index-based coding scheme is optimal.},
	booktitle = {{IEEE} {International} {Symposium} on {Information} {Theory} - {Proceedings}},
	author = {Heckel, Reinhard and Shomorony, Ilan and Ramchandran, Kannan and Tse, David N.C.},
	year = {2017},
	note = {ISSN: 21578095},
}

@article{Takahashi2019,
	title = {Demonstration of {End}-to-{End} {Automation} of {DNA} {Data} {Storage}},
	volume = {9},
	issn = {20452322},
	doi = {10.1038/s41598-019-41228-8},
	abstract = {We developed a complete end-to-end DNA data storage device. The device enables the encoding of data, which is then written to a DNA oligonucleotide using a custom DNA synthesizer, pooled for liquid storage, and read using a nanopore sequencer and a novel, minimal preparation protocol. We demonstrate an automated 5-byte write, store, and read cycle with the ability to expand as new technology is available.},
	number = {1},
	journal = {Scientific Reports},
	author = {Takahashi, Christopher N. and Nguyen, Bichlien H. and Strauss, Karin and Ceze, Luis},
	year = {2019},
}

@article{Lu2016,
	title = {Oxford {Nanopore} {MinION} {Sequencing} and {Genome} {Assembly}},
	volume = {14},
	issn = {22103244},
	doi = {10.1016/j.gpb.2016.05.004},
	abstract = {The revolution of genome sequencing is continuing after the successful second-generation sequencing (SGS) technology. The third-generation sequencing (TGS) technology, led by Pacific Biosciences (PacBio), is progressing rapidly, moving from a technology once only capable of providing data for small genome analysis, or for performing targeted screening, to one that promises high quality de novo assembly and structural variation detection for human-sized genomes. In 2014, the MinION, the first commercial sequencer using nanopore technology, was released by Oxford Nanopore Technologies (ONT). MinION identifies DNA bases by measuring the changes in electrical conductivity generated as DNA strands pass through a biological pore. Its portability, affordability, and speed in data production makes it suitable for real-time applications, the release of the long read sequencer MinION has thus generated much excitement and interest in the genomics community. While de novo genome assemblies can be cheaply produced from SGS data, assembly continuity is often relatively poor, due to the limited ability of short reads to handle long repeats. Assembly quality can be greatly improved by using TGS long reads, since repetitive regions can be easily expanded into using longer sequencing lengths, despite having higher error rates at the base level. The potential of nanopore sequencing has been demonstrated by various studies in genome surveillance at locations where rapid and reliable sequencing is needed, but where resources are limited.},
	number = {5},
	journal = {Genomics, Proteomics and Bioinformatics},
	author = {Lu, Hengyun and Giordano, Francesca and Ning, Zemin},
	year = {2016},
}

@article{Newman2019,
	title = {High density {DNA} data storage library via dehydration with digital microfluidic retrieval},
	volume = {10},
	issn = {20411723},
	doi = {10.1038/s41467-019-09517-y},
	abstract = {© 2019, The Author(s). DNA promises to be a high density data storage medium, but physical storage poses a challenge. To store large amounts of data, pools must be physically isolated so they can share the same addressing scheme. We propose the storage of dehydrated DNA spots on glass as an approach for scalable DNA data storage. The dried spots can then be retrieved by a water droplet using a digital microfluidic device. Here we show that this storage schema works with varying spot organization, spotted masses of DNA, and droplet retrieval dwell times. In all cases, the majority of the DNA was retrieved and successfully sequenced. We demonstrate that the spots can be densely arranged on a microfluidic device without significant contamination of the retrieval. We also demonstrate that 1 TB of data could be stored in a single spot of DNA and successfully retrieved using this method.},
	number = {1},
	journal = {Nature Communications},
	author = {Newman, Sharon and Stephenson, Ashley P. and Willsey, Max and Nguyen, Bichlien H. and Takahashi, Christopher N. and Strauss, Karin and Ceze, Luis},
	year = {2019},
}

@article{HosseinTabatabaeiYazdi2017,
	title = {Portable and {Error}-{Free} {DNA}-{Based} {Data} {Storage}},
	volume = {7},
	issn = {20452322},
	doi = {10.1038/s41598-017-05188-1},
	abstract = {DNA-based data storage is an emerging nonvolatile memory technology of potentially unprecedented density, durability, and replication efficiency. The basic system implementation steps include synthesizing DNA strings that contain user information and subsequently retrieving them via high-throughput sequencing technologies. Existing architectures enable reading and writing but do not offer random-access and error-free data recovery from low-cost, portable devices, which is crucial for making the storage technology competitive with classical recorders. Here we show for the first time that a portable, random-access platform may be implemented in practice using nanopore sequencers. The novelty of our approach is to design an integrated processing pipeline that encodes data to avoid costly synthesis and sequencing errors, enables random access through addressing, and leverages efficient portable sequencing via new iterative alignment and deletion error-correcting codes. Our work represents the only known random access DNA-based data storage system that uses error-prone nanopore sequencers, while still producing error-free readouts with the highest reported information rate/density. As such, it represents a crucial step towards practical employment of DNA molecules as storage media.},
	number = {1},
	journal = {Scientific Reports},
	author = {Hossein TabatabaeiYazdi, S. M. and Gabrys, Ryan and Milenkovic, Olgica},
	year = {2017},
}

@inproceedings{Blawat2016,
	title = {Forward error correction for {DNA} data storage},
	volume = {80},
	doi = {10.1016/j.procs.2016.05.398},
	abstract = {We report on a strong capacity boost in storing digital data in synthetic DNA. In principle, synthetic DNA is an ideal media to archive digital data for very long times because the achievable data density and longevity outperforms today's digital data storage media by far. On the other hand, neither the synthesis, nor the amplification and the sequencing of DNA strands can be performed error-free today and in the foreseeable future. In order to make synthetic DNA available as digital data storage media, specifically tailored forward error correction schemes have to be applied. For the purpose of realizing a DNA data storage, we have developed an efficient and robust forward-error-correcting scheme adapted to the DNA channel. We based the design of the needed DNA channel model on data from a proof-of-concept conducted 2012 by a team from the Harvard Medical School [1]. Our forward error correction scheme is able to cope with all error types of today's DNA synthesis, amplification and sequencing processes, e.g. insertion, deletion, and swap errors. In a successful experiment, we were able to store and retrieve error-free 22 MByte of digital data in synthetic DNA recently. The found residual error probability is already in the same order as it is in hard disk drives and can be easily improved further. This proves the feasibility to use synthetic DNA as longterm digital data storage media.},
	booktitle = {Procedia {Computer} {Science}},
	author = {Blawat, Meinolf and Gaedke, Klaus and Hütter, Ingo and Chen, Xiao Ming and Turczyk, Brian and Inverso, Samuel and Pruitt, Benjamin W. and Church, George M.},
	year = {2016},
	note = {ISSN: 18770509},
}

@article{Choi2019,
	title = {High information capacity {DNA}-based data storage with augmented encoding characters using degenerate bases},
	volume = {9},
	issn = {20452322},
	doi = {10.1038/s41598-019-43105-w},
	abstract = {DNA-based data storage has emerged as a promising method to satisfy the exponentially increasing demand for information storage. However, practical implementation of DNA-based data storage remains a challenge because of the high cost of data writing through DNA synthesis. Here, we propose the use of degenerate bases as encoding characters in addition to A, C, G, and t, which augments the amount of data that can be stored per length of DNA sequence designed (information capacity) and lowering the amount of DNA synthesis per storing unit data. Using the proposed method, we experimentally achieved an information capacity of 3.37 bits/character. The demonstrated information capacity is more than twice when compared to the highest information capacity previously achieved. the proposed method can be integrated with synthetic technologies in the future to reduce the cost of DNA-based data storage by 50\%. The annual demand for digital data storage is expected to surpass the supply of silicon in 2040, assuming that all data are stored in flash memory for instant access 1. Considering the massive accumulation of digital data, the development of alternative storage methods is essential. One alternative is DNA-based data storage, which converts the binary digital data of 0 and 1 into the quaternary encoding nucleotides A, C, G, and T, synthesizes the sequence, and stores the data 2,3. This concept 2-10 is attractive due to two main advantages: the high physical information density of petabytes of data per gram, and the durability as the storage lasts for centuries without energy input. Due to these advantages, DNA-based data storage is expected to supplement the increasing demand for digital data storage, especially for archival data that are not frequently accessed. Since DNA-based data storage was proposed, the major goal was to improve data to DNA encoding algorithms 9,10 or error correction algorithms 4,6,7,9,10 to reduce data error or loss considering the biochemical properties while handling DNA. These previous studies on encoding algorithms showed 100\% reconstruction of the data from DNA while using library of 100 to 200nt length oligonucleotides. To correct the synthesis errors and recover the dropped data fragments during DNA amplification, the library of oligonucleotides that contains 1300 copies of each designed sequences were required 10 , with the developed algorithms. The next step towards the practical use of DNA-based data storage is to reduce the cost of storing the data. The cost of DNA-based data storage is categorized into the cost of data writing through DNA synthesis and the cost of data reading through DNA sequencing. Among these two costs, the cost of data writing is predominant because it is tens of thousands times more expensive per unit DNA than that of reading. However, previous studies have shown that DNA can be put to practical use as a backup storage medium only when the cost of the data writing is},
	number = {1},
	journal = {Scientific Reports},
	author = {Choi, Yeongjae and Ryu, Taehoon and Lee, Amos C. and Choi, Hansol and Lee, Hansaem and Park, Jaejun and Song, Suk Heung and Kim, Seojoo and Kim, Hyeli and Park, Wook and Kwon, Sunghoon},
	year = {2019},
}

@inproceedings{Stewart2018,
	title = {A content-addressable {DNA} database with learned sequence encodings},
	volume = {11145 LNCS},
	isbn = {978-3-030-00029-5},
	doi = {10.1007/978-3-030-00030-1_4},
	abstract = {© Springer Nature Switzerland AG 2018. We present strand and codeword design schemes for a DNA database capable of approximate similarity search over a multidimensional dataset of content-rich media. Our strand designs address cross-talk in associative DNA databases, and we demonstrate a novel method for learning DNA sequence encodings from data, applying it to a dataset of tens of thousands of images. We test our design in the wetlab using one hundred target images and ten query images, and show that our database is capable of performing similarity-based enrichment: on average, visually similar images account for 30\% of the sequencing reads for each query, despite making up only 10\% of the database.},
	booktitle = {Lecture {Notes} in {Computer} {Science} (including subseries {Lecture} {Notes} in {Artificial} {Intelligence} and {Lecture} {Notes} in {Bioinformatics})},
	author = {Stewart, Kendall and Chen, Yuan Jyue and Ward, David and Liu, Xiaomeng and Seelig, Georg and Strauss, Karin and Ceze, Luis},
	year = {2018},
	note = {ISSN: 16113349},
}

@inproceedings{Limbachiya2016,
	title = {On optimal family of codes for archival {DNA} storage},
	isbn = {978-1-4673-8308-0},
	doi = {10.1109/IWSDA.2015.7458386},
	abstract = {DNA based storage systems received attention by many researchers. This includes archival and re-writable random access DNA based storage systems. In this work, we have developed an efficient technique to encode the data into DNA sequence by using non-linear families of ternary codes. In particular, we proposes an algorithm to encode data into DNA with high information storage density and better error correction using a sub code of Golay code. Theoretically, 115 exabytes (EB) data can be stored in one gram of DNA by our method.},
	booktitle = {7th {International} {Workshop} on {Signal} {Design} and {Its} {Applications} in {Communications}, {IWSDA} 2015},
	author = {Limbachiya, Dixita and Dhameliya, Vijay and Khakhar, Madhav and Gupta, Manish K.},
	year = {2016},
}

@article{Rang2018,
	title = {From squiggle to basepair: {Computational} approaches for improving nanopore sequencing read accuracy},
	volume = {19},
	issn = {1474760X},
	doi = {10.1186/s13059-018-1462-9},
	abstract = {Nanopore sequencing is a rapidly maturing technology delivering long reads in real time on a portable instrument at low cost. Not surprisingly, the community has rapidly taken up this new way of sequencing and has used it successfully for a variety of research applications. A major limitation of nanopore sequencing is its high error rate, which despite recent improvements to the nanopore chemistry and computational tools still ranges between 5\% and 15\%. Here, we review computational approaches determining the nanopore sequencing error rate. Furthermore, we outline strategies for translation of raw sequencing data into base calls for detection of base modifications and for obtaining consensus sequences.},
	number = {1},
	journal = {Genome Biology},
	author = {Rang, Franka J. and Kloosterman, Wigard P. and de Ridder, Jeroen},
	year = {2018},
}

@article{Shipman2016,
	title = {Molecular recordings by directed {CRISPR} spacer acquisition},
	volume = {353},
	issn = {10959203},
	doi = {10.1126/science.aaf1175},
	abstract = {Copyright 2016 by the American Association for the Advancement of Science. All rights reserved. The ability to write a stable record of identified molecular events into a specific genomic locus would enable the examination of long cellular histories and have many applications, ranging from developmental biology to synthetic devices.We show that the type I-E CRISPR (clustered regularly interspaced short palindromic repeats)-Cas  system of Escherichia coli can mediate acquisition of defined pieces of synthetic DNA.We harnessed this feature to generate records of specific DNA sequences into a population of bacterial genomes.We then applied directed evolution so as to alter the recognition of a protospacer adjacent motif by the Cas1-Cas2 complex, which enabled recording in two modes simultaneously. We used this system to reveal aspects of spacer acquisition, fundamental to the CRISPR-Cas adaptation process. These results lay the foundations of a multimodal intracellular recording device.},
	number = {6298},
	journal = {Science},
	author = {Shipman, Seth L. and Nivala, Jeff and Macklis, Jeffrey D. and Church, George M.},
	year = {2016},
}

@article{Gabrys2017,
	title = {Asymmetric {Lee} {Distance} {Codes} for {DNA}-{Based} {Storage}},
	volume = {63},
	issn = {00189448},
	doi = {10.1109/TIT.2017.2700847},
	abstract = {We consider a new family of codes, termed asymmetric Lee distance codes, that arise in the design and implementation of DNA-based storage systems and systems with parallel string transmission protocols. The codewords are defined over a quaternary alphabet, although the results carry over to other alphabet sizes; furthermore, symbol confusability is dictated by their underlying binary representation. Our contributions are two-fold. First, we demonstrate that the new distance represents a linear combination of the Lee and Hamming distance and derive upper bounds on the size of the codes under this metric based on linear programming techniques. Second, we propose a number of code constructions which imply lower bounds.},
	number = {8},
	journal = {IEEE Transactions on Information Theory},
	author = {Gabrys, Ryan and Kiah, Han Mao and Milenkovic, Olgica},
	year = {2017},
}

@article{Lee2019,
	title = {Terminator-free template-independent enzymatic {DNA} synthesis for digital information storage},
	volume = {10},
	issn = {20411723},
	doi = {10.1038/s41467-019-10258-1},
	abstract = {DNA is an emerging medium for digital data and its adoption can be accelerated by synthesis processes specialized for storage applications. Here, we describe a de novo enzymatic synthesis strategy designed for data storage which harnesses the template-independent polymerase terminal deoxynucleotidyl transferase (TdT) in kinetically controlled conditions. Information is stored in transitions between non-identical nucleotides of DNA strands. To produce strands representing user-defined content, nucleotide substrates are added iteratively, yielding short homopolymeric extensions whose lengths are controlled by apyrase-mediated substrate degradation. With this scheme, we synthesize DNA strands carrying 144 bits, including addressing, and demonstrate retrieval with streaming nanopore sequencing. We further devise a digital codec to reduce requirements for synthesis accuracy and sequencing coverage, and experimentally show robust data retrieval from imperfectly synthesized strands. This work provides distributive enzymatic synthesis and information-theoretic approaches to advance digital information storage in DNA.},
	number = {1},
	journal = {Nature Communications},
	author = {Lee, Henry H. and Kalhor, Reza and Goela, Naveen and Bolot, Jean and Church, George M.},
	year = {2019},
}

@article{Carmean2019,
	title = {{DNA} {Data} {Storage} and {Hybrid} {Molecular}-{Electronic} {Computing}},
	volume = {107},
	issn = {00189219},
	doi = {10.1109/JPROC.2018.2875386},
	abstract = {Moore's law may be slowing, but our ability to manipulate molecules is improving faster than ever. DNA could provide alternative substrates for computing and storage as existing ones approach physical limits. In this paper, we explore the implications of this trend in computer architecture. We present a computer systems perspective on molecular processing and storage, positing a hybrid molecular-electronic architecture that plays to the strengths of both domains. We cover the design and implementation of all stages of the pipeline: encoding, DNA synthesis, system integration with digital microfluidics, DNA sequencing (including emerging technologies such as nanopores), and decoding. We first draw on our experience designing a DNA-based archival storage system, which includes the largest demonstration to date of DNA digital data storage of over three billion nucleotides encoding over 400 MB of data. We then propose a more ambitious hybrid-electronic design that uses a molecular form of near-data processing for massive parallelism. We present a model that demonstrates the feasibility of these systems in the near future. We think the time is ripe to consider molecular storage seriously and explore system designs and architectural implications.},
	number = {1},
	journal = {Proceedings of the IEEE},
	author = {Carmean, Douglas and Ceze, Luis and Seelig, Georg and Stewart, Kendall and Strauss, Karin and Willsey, Max},
	year = {2019},
}

@inproceedings{Goela2017,
	title = {Encoding movies and data in {DNA} storage},
	isbn = {978-1-5090-2529-9},
	doi = {10.1109/ITA.2016.7888163},
	abstract = {Focusing on error-correction methods and codes, a systems level design is presented for encoding movies and digital information in DNA storage. A source of data (e.g., movies, audio) is compressed, efficiently encoded with redundant information, modulated, and stored in multiple DNA oligonucleotide strands. The goal is to decode the source from the DNA reliably even in the presence of diverse errors introduced by DNA synthesis, PCR amplification, and DNA sequencing processes.},
	booktitle = {2016 {Information} {Theory} and {Applications} {Workshop}, {ITA} 2016},
	author = {Goela, Naveen and Bolot, Jean},
	year = {2017},
}

@article{Mehta2018,
	title = {Bacterial {Genome} {Containing} {Chimeric} {DNA}-{RNA} {Sequences}},
	volume = {140},
	issn = {15205126},
	doi = {10.1021/jacs.8b07046},
	abstract = {Almost five decades ago Crick, Orgel, and others proposed the RNA world hypothesis. Subsequent studies have raised the possibility that RNA might be able to support both genotype and phenotype, and the function of RNA templates has been studied in terms of evolution, replication, and catalysis. Recently, we engineered strains of E. coli in which a large fraction of 2′-deoxycytidine in the genome is substituted with the modified base 5-hydroxymethyl-2′-deoxycytidine. We now report the generation of mutant strains derived from these engineered bacteria that show significant (∼40−50\%) ribonu-cleotide content in their genome. We have begun to characterize the properties of these chimeric genomes and the corresponding strains to determine the circumstances under which E. coli can incorporate ribonucleotides into its genome and herein report our initial observations.},
	number = {36},
	journal = {Journal of the American Chemical Society},
	author = {Mehta, Angad P. and Wang, Yiyang and Reed, Sean A. and Supekova, Lubica and Javahishvili, Tsotne and Chaput, John C. and Schultz, Peter G.},
	year = {2018},
	pages = {11464--11473},
}

@article{Poole2006,
	title = {Getting from an {RNA} world to modern cells just got a little easier},
	volume = {28},
	issn = {02659247},
	doi = {10.1002/bies.20367},
	abstract = {Our understanding of the early steps in the evolution of life is hampered by a Catch-22: Darwinian selection leading to longer genomes requires as prerequisite increased replicative fidelity. Yet a genome at capacity cannot increase in size; it will be catastrophically mutated out of existence if fidelity has not already increased. Traditionally the problem has been considered for genotypes but can be down-sized if multiple genotypes specify the same phenotype. Kun and colleagues put empirical meat on theoretical bone by analysing ribozyme mutagenesis data, concluding that modest replication fidelities could permit a primordial genome with up to 100 genes.},
	number = {2},
	journal = {BioEssays},
	author = {Poole, Anthony M.},
	year = {2006},
	pmid = {16435297},
	note = {ISBN: 0265-9247 (Print){\textbackslash}r0265-9247 (Linking)},
	pages = {105--108},
}

@article{Zenkin2006,
	title = {Transcript-assisted transcriptional proofreading},
	volume = {313},
	issn = {00368075},
	doi = {10.1126/science.1127422},
	abstract = {Fidelity of template-dependent nucleic acid synthesis is the main determinant of stable heredity and error-free gene expression. The mechanism (or mechanisms) ensuring fidelity of transcription by DNA-dependent RNA polymerases (RNAPs) is not fully understood. Here, we show that the 3' end-proximal nucleotide of the nascent transcript stimulates hydrolysis of the penultimate phosphodiester bond by providing active groups and coordination bonds to the RNAP active center. This stimulation is much higher in the case of misincorporated nucleotide. We show that during transcription elongation, the hydrolytic reaction stimulated by misincorporated nucleotides proofreads most of the misincorporation events and thus serves as an intrinsic mechanism of transcription fidelity.},
	number = {5786},
	journal = {Science},
	author = {Zenkin, Nikolay and Yuzenkova, Yulia and Severinov, Konstantin},
	year = {2006},
	pmid = {16873663},
	note = {ISBN: 1095-9203 (Electronic)},
	pages = {518--520},
}

@article{Beerli2007,
	title = {Comment on "{Population} {Size} {Does} {Not} {Influence} {Mitochondrial} {Genetic} {Diversity} in {Animals}" -- {Mulligan} et al. 314 (5804): 1390a -- {Science}},
	volume = {314},
	url = {papers3://publication/uuid/2DF4C5A0-2C73-4953-8993-ACE86A678C2E},
	number = {December},
	author = {Beerli, Peter},
	year = {2007},
	pages = {2--4},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\D2BSRJ4H\\mulligan et al.pdf:application/pdf},
}

@article{Takeuchi2018e,
	title = {{BIOINF703} : {Origin} of {Genomes}},
	author = {Takeuchi, Nobuto},
	year = {2018},
}

@article{Koshkin1998,
	title = {The {University} of {Auckland} - {Loading}...},
	abstract = {... Also shown is the locked N-type conformation of an LNA nucleoside. ... The Danish Natural Science Research Council, The Danish Technical Research Council, and Exiqon A/S, Denmark are thanked for financial support. ... (17) Egli, M. Antisense Nucleic  Acid Drug DeV. ... {\textbackslash}n},
	journal = {Journal of the …},
	author = {Koshkin, A A and Nielsen, P and Meldgaard, M},
	year = {1998},
	pages = {1--2},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\8KCIB8KQ\\m-api-39a7a3d5-1559-f8af-3a3c-9662243e1f21.pdf:application/pdf},
}

@article{Neigel2002,
	title = {Is {F} {ST} obsolete ?},
	number = {Slatkin 1985},
	author = {Neigel, Joseph E},
	year = {2002},
	keywords = {dispersal, population structure, f st, gene flow, nm},
	pages = {167--173},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\I266KZSY\\Is_FST_obsolete.pdf:application/pdf},
}

@article{Hilbert2011a,
	title = {The {World}'s {Technological} {Capacity} to {Store}, {Communicate}, and {Compute} {Information}},
	url = {http://science.sciencemag.org/content/early/2011/02/09/science.1200970.abstract},
	doi = {10.1126/science.1200970},
	abstract = {We estimate the world's technological capacity to store, communicate, and compute information, tracking 60 analog and digital technologies during the period from 1986 to 2007. In 2007, humankind was able to store 2.9 x 10 20 optimally compressed bytes, communicated almost 2 x 10 21 bytes, and carry out 6.4 x 10 18 instructions per second on general-purpose computers. General-purpose computing capacity grew at an annual rate of 58\%. The world's capacity for bidirectional telecommunication grew at 28\% per year, closely followed by the increase in globally stored information (23\%). Humankind's capacity for unidirectional information diffusion through broadcasting channels has experienced comparatively modest annual growth (6\%). Telecommunication has been dominated by digital technologies since 1990 (99.9\% in digital format in 2007) and the majority of our technological memory has been in digital format since the early 2000s (94\% digital in 2007). Leading social scientists have recognized that we are living through an age in which "the generation of wealth, the exercise of power, and the creation of cultural codes came to depend on the technological capacity of societies and individuals, with information technologies as the core of this capacity" (1). Despite this insight, most evaluations of society's technological capacity to handle information are based on either qualitative assessments or indirect approximations, such as the stock of installed devices or the economic value of related products and services (2-9). Previous work Some pioneering studies have taken a more direct approach to quantify the amount of information that society processes with its information and communication technologies (ICT). Following pioneering work in Japan (10), Pool (11) estimated the growth trends of the "amount of words" transmitted by 17 major communications media in the United States from 1960 to 1977. This study was the first to show empirically the declining relevance of print media with respect to electronic media. In 1997, Lesk (12) asked "how much information is there in the world?" and presented a brief outline on how to go about estimating the global information storage capacity. A group of researchers at the University of California, at Berkeley, took up the measurement challenge between 2000 and 2003 (13). Their focus on "uniquely created" information resulted in the conclusion that "most of the total volume of new information flows is derived from the volume of voice telephone traffic, most of which is unique content" (97\%); as broadcasted television and most information storage mainly consists of duplicate information, these omnipresent categories contributed relatively little. A storage company hired a private sector research firm (International Data Corporation, IDC) to estimate the global hardware capacity of digital ICT for the years 2007-2008 (14). For digital storage, IDC estimates that in 2007 "all the empty or usable space on hard drives, tapes, CDs, DVDs, and memory (volatile and nonvolatile) in the market equaled 264 exabytes" (14). During 2008, an industry and university collaboration explicitly focused on information consumption (15), measured in hardware capacity, words, and hours. The results are highly reliant on media time-budget studies, which estimate how many hours people interact with a media device. Not surprisingly, the result obtained with this methodology was that computer games and movies represent 99.2\% of the total amount of data "consumed". Scope of our exercise To reconcile these different results, we focus on the world's technological capacity to handle information. We do not account for uniqueness of information, since it is very difficult to differentiate between truly new and merely recombined, duplicate information. Instead we assume that all information has some relevance for some individual. Aside from the traditional focus on the transmission through space (communication) and time (storage), we also consider the computation of information. We define storage as the},
	journal = {Science},
	author = {Hilbert, Martin and López, Priscila},
	month = feb,
	year = {2011},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\7LSDUJJD\\Hilbert, López - 2011 - The World's Technological Capacity to Store, Communicate, and Compute Information.pdf:application/pdf},
}

@article{Eddy2004d,
	title = {What is a hidden {Markov} model?},
	volume = {22},
	issn = {1087-0156},
	url = {papers2://publication/uuid/9BFE5458-7C45-4E6E-9F95-48D5CE4CF032},
	doi = {10.1038/nbt1004-1315},
	abstract = {Statistical models called hidden Markov models are a recurring theme in computational biology. What are hidden Markov models, and why are they so useful for so many different problems?},
	number = {10},
	journal = {Nature Biotechnology},
	author = {Eddy, Sean R.},
	year = {2004},
	pmid = {15470472},
	note = {arXiv: NIHMS150003
ISBN: 1087-0156},
	keywords = {Markov model hidden Markov Model chaien de Markov},
	pages = {1315--1316},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\5EHFAPA8\\Eddy - 2004 - What is a hidden Markov model.pdf:application/pdf},
}

@article{Mortazavi2008,
	title = {Mapping and quantifying mammalian transcriptomes by {RNA}-{Seq}},
	volume = {5},
	issn = {15487091},
	doi = {10.1038/nmeth.1226},
	abstract = {We have mapped and quantified mouse transcriptomes by deeply sequencing them and recording how frequently each gene is represented in the sequence sample (RNA-Seq). This provides a digital measure of the presence and prevalence of transcripts from known and previously unknown genes. We report reference measurements composed of 41-52 million mapped 25-base-pair reads for poly(A)-selected RNA from adult mouse brain, liver and skeletal muscle tissues. We used RNA standards to quantify transcript prevalence and to test the linear range of transcript detection, which spanned five orders of magnitude. Although {\textgreater}90\% of uniquely mapped reads fell within known exons, the remaining data suggest new and revised gene models, including changed or additional promoters, exons and 3' untranscribed regions, as well as new candidate microRNA precursors. RNA splice events, which are not readily measured by standard gene expression microarray or serial analysis of gene expression methods, were detected directly by mapping splice-crossing sequence reads. We observed 1.45 x 10(5) distinct splices, and alternative splices were prominent, with 3,500 different genes expressing one or more alternate internal splices.},
	number = {7},
	journal = {Nature Methods},
	author = {Mortazavi, Ali and Williams, Brian A. and McCue, Kenneth and Schaeffer, Lorian and Wold, Barbara},
	year = {2008},
	pmid = {18516045},
	note = {arXiv: 1111.6189v1
ISBN: 1548-7105 (Electronic){\textbackslash}r1548-7091 (Linking)},
	pages = {621--628},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\TA5Z9WI4\\Mortazavi et al. - 2008 - Mapping and quantifying mammalian transcriptomes by RNA-Seq.pdf:application/pdf},
}

@article{jackson_multi-state_2018,
	title = {Multi-state modelling with {R}: the msm package (long vignette)},
	abstract = {The multi-state Markov model is a useful way of describing a process in which an individual moves through a series of states in continuous time. The msm package for R allows a general multi-state model to be fitted to longitudinal data. Data often consist of observations of the process at arbitrary times, so that the exact times when the state changes are unobserved. For example, the progression of chronic diseases is often described by stages of severity, and the state of the patient may only be known at doctor or hospital visits. Features of msm include the ability to model transition rates and hidden Markov output models in terms of covariates, and the ability to model data with a variety of observation schemes, including censored states. Hidden Markov models, in which the true path through states is only observed through some error-prone marker, can also be fitted. The observation is generated, conditionally on the underly- ing states, via some distribution. An example is a screening misclassification model in which states are observed with error. More generally, hidden Markov models can have a continuous response, with some arbitrary distribution, conditionally on the underlying state. This manual introduces the theory behind multi-state Markov and hidden Markov models, and gives a tutorial in the typical use of the msm package, illustrated by some typical applications to modelling chronic diseases.},
	journal = {Journal of Statistical Software},
	author = {Jackson, Christopher H},
	year = {2018},
	pmid = {18286417},
}

@article{yachie_stabilizing_2008,
	title = {Stabilizing synthetic data in the {DNA} of living organisms},
	issn = {18725325},
	doi = {10.1007/s11693-008-9020-5},
	abstract = {Data-encoding synthetic DNA, inserted into the genome of a living organism, is thought to be more robust than the current media. Because the living genome is duplicated and copied into new generations, one of the merits of using DNA material is long-term data storage within heritable media. A disadvantage of this approach is that encoded data can be unexpectedly broken by mutation, deletion, and insertion of DNA, which occurs naturally during evolution and prolongation, or laboratory experiments. For this reason, several information theory-based approaches have been developed as an error check of broken DNA data in order to achieve data durability. These approaches cannot efficiently recover badly damaged data- encoding DNA. We recently developed a DNA data-storage approach based on the multiple sequence alignment method to achieve a high level of data durability. In this paper, we overview this technology and discuss strategies for optimal application of this approach.},
	journal = {Systems and Synthetic Biology},
	author = {Yachie, Nozomu and Ohashi, Yoshiaki and Tomita, Masaru},
	year = {2008},
	keywords = {DNa data storage, Error check, Error correction, Genetically modified organism (GMO), Polymerase chain reaction (PCR), Sequence alignment},
}

@article{borkowski_record_2016,
	title = {On the record with {E}. coli {DNA}},
	issn = {0036-8075},
	doi = {10.1126/science.aah4438},
	abstract = {Synthetic biology uses DNA programs to add new functions into living cells. By expressing transcription factors, microbes such as Escherichia coli can be made to perform complex computational logic (1) and pass “memories” of selected events between generations (2). But DNA is more than just the source code for protein expression programs; DNA can be used as the storage medium for information. In synthetic biology, the use of DNA for in vivo information storage was first realized in 2009 with a synthetic genetic program that enabled E. coli to count events by using a recombinase to rearrange DNA in response to an input (3). With multiple recombinases, this technology could be used to store 1.375 bytes of information in a living E. coli (4). As reported by Shipman et al. (5) on page 463 of this issue, and by Roquet et al. (6), in vivo encoding of information into DNA is pushed even further, using either genome editing to store dozens of bytes of data, or employing multiple recombinases to realize “state machines” inside living cells.},
	journal = {Science},
	author = {Borkowski, Olivier and Gilbert, Charlie and Ellis, Tom},
	year = {2016},
}

@inproceedings{debata_coding_2012,
	title = {A coding theoretic model for error-detecting in {DNA} sequences},
	doi = {10.1016/j.proeng.2012.06.216},
	abstract = {A major problem in communication engineering system is the transmitting of information from source to receiver over a noisy channel. To check the error in information digits many error detecting and correcting codes have been developed. The main aim of these error correcting codes is to encode the information digits and decode these digits to detect and correct the common errors in transmission. This information theory concept helps to study the information transmission in biological systems and extend the field of coding theory into the biological domain. In the cellular level, the information in DNA is transformed into proteins. The sequence of bases like Adenine (A), Thymine (T), Guanine (G) and Cytosine (C) in DNA may be considered as digital codes which transmit genetic information. This paper shows the existence of any form error detecting code in the DNA structure, by encoding the DNA sequences using Hamming code. © 2012 Published by Elsevier Ltd.},
	booktitle = {Procedia {Engineering}},
	author = {Debata, Prajna Paramita and Mishra, Debahuti and Shaw, Kailash and Mishra, Sashikala},
	year = {2012},
	note = {ISSN: 18777058},
	keywords = {Biological systems, Coding theory, Encoding DNA sequences, Error detecting and correcting codes, Hamming code, Information transmission},
}

@article{limbachiya_family_2018,
	title = {Family of {Constrained} {Codes} for {Archival} {DNA} {Data} {Storage}},
	issn = {15582558},
	doi = {10.1109/LCOMM.2018.2861867},
	abstract = {DNA-based data storage systems have evolved as a solution to accommodate data explosion. In this letter, some properties of DNA codewords that are essential for an archival DNA storage are considered for the design of codes. Constraint-based DNA codes, which avoid runs of nucleotides, have fixed GC-weight, and a specific minimum distance is presented. An altruistic algorithm that enumerates DNA codewords with the above constraints is provided. A theoretical bound on such DNA codewords is obtained. This bound is tight when there is no minimum distance constraint.},
	journal = {IEEE Communications Letters},
	author = {Limbachiya, Dixita and Gupta, Manish K. and Aggarwal, Vaneet},
	year = {2018},
	keywords = {DNA storage, constrained codes, DNA codes},
}

@article{bornholt_toward_2017,
	title = {Toward a {DNA}-{Based} {Archival} {Storage} {System}},
	issn = {02721732},
	doi = {10.1109/MM.2017.70},
	abstract = {Storing data in DNA molecules offers extreme density and durability advantages that can mitigate exponential growth in data storage needs. This article presents a DNA-based archival storage system, performs wet lab experiments to show its feasibility, and identifies technology trends that point to increasing practicality.},
	journal = {IEEE Micro},
	author = {Bornholt, James and Lopez, Randolph and Carmean, Douglas M. and Ceze, Luis and Seelig, Georg and Strauss, Karin},
	year = {2017},
	keywords = {DNA, Archival storage, molecular storage},
}

@book{xu_principles_2013,
	title = {Principles of statistical genomics},
	isbn = {978-0-387-70807-2},
	abstract = {Statistical genomics is a rapidly developing field, with more and more people involved in this area. However, a lack of synthetic reference books and textbooks in statistical genomics has become a major hurdle on the development of the field. Although many books have been published recently in bioinformatics, most of them emphasize DNA sequence analysis under a deterministic approach. Principles of Statistical Genomics synthesizes the state-of-the-art statistical methodologies (stochastic approaches) applied to genome study. It facilitates understanding of the statistical models and methods behind the major bioinformatics software packages, which will help researchers choose the optimal algorithm to analyze their data and better interpret the results of their analyses. Understanding existing statistical models and algorithms assists researchers to develop improved statistical methods to extract maximum information from their data. Resourceful and easy to use, Principles of Statistical Genomics is a comprehensive reference for researchers and graduate students studying statistical genomics.},
	author = {Xu, Shizhong},
	year = {2013},
	doi = {10.1007/978-0-387-70807-2},
	note = {Publication Title: Principles of Statistical Genomics},
}

@article{Iterson2012,
	title = {Resolving confusion of tongues in statistics and machine learning: {A} primer for biologists and bioinformaticians},
	issn = {16159853},
	doi = {10.1002/pmic.201100395},
	abstract = {Bioinformatics is the field where computational methods from various domains have come together for analysis of biological data. Each domain has introduced its own specific jargon. However, in closely related domains, e.g. machine learning and statistics, concordant and discordant terminology occurs, the later can lead to confusion. This article aims to help solve the confusion of tongues arising from these two closely related domains, which are frequently used in bioinformatics. We provide a short summary of the most commonly applied machine learning and statistical approaches to data analysis in bioinformatics, i.e. classification and statistical hypothesis testing. We explain differences and similarities in common terminology used in various domains, such as precision, recall, sensitivity and true positive rate. This primer can serve as a guide to the terminology used in these fields. © 2012 WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim.},
	journal = {Proteomics},
	author = {Van Iterson, Maarten and Van Haagen, Herman H.H.B.M. and Goeman, Jelle J.},
	year = {2012},
	keywords = {Bioinformatics, Classification, Contingency table, False discovery rate, Hypothesis testing, Sensitivity and specificity},
}

@article{liu_forensic_2020,
	title = {Forensic {STR} allele extraction using a machine learning paradigm},
	volume = {44},
	issn = {18780326},
	doi = {10.1016/j.fsigen.2019.102194},
	abstract = {© 2019 Elsevier B.V. We present a machine learning approach to short tandem repeat (STR) sequence detection and extraction from massively parallel sequencing data called Fragsifier. Using this approach, STRs are detected on each read by first locating the longest repeat stretches followed by locus prediction using k-mers in a machine learning sequence model. This is followed by reference flanking sequence alignment to determine precise STR boundaries. We show that Fragsifier produces genotypes that are concordant with profiles obtained using capillary electrophoresis (CE), and also compared the results with that of STRait Razor and the ForenSeq UAS. The data pre-processing and training of the sequence classifier is readily scripted, allowing the analyst to experiment with different thresholds, datasets and loci of interest, and different machine learning models.},
	journal = {Forensic Science International: Genetics},
	author = {Liu, Y.-Y. and Welch, D. and England, R. and Stacey, J. and Harbison, S.},
	year = {2020},
	keywords = {Bioinformatics, Machine learning, Massively parallel sequencing, STR extraction},
}

@article{england_massively_2015,
	title = {Massively parallel sequencing of {Identifiler} and {PowerPlex}$^{\textrm{®}}$ {Y} amplified forensic samples},
	volume = {5},
	issn = {1875175X},
	doi = {10.1016/j.fsigss.2015.09.084},
	abstract = {© 2015 Elsevier Ireland Ltd In the last few years the cost of massively parallel sequencing has reduced dramatically to the point that it can now be practically considered as a tool in forensic casework. An important consideration for the implementation of any new approach is the ability to remain compatible with previous technology. With this is mind we conducted two sets of experiments to evaluate sequencing the previously amplified products of two commercial forensic STR multiplexes. Samples amplified with AmpFlSTR® Identifiler® and PowerPlex® Y were sequenced on the Illumina© MiSeq and Ion PGM™ Sequencer (Life Technologies). We found it is possible to sequence such amplified DNA and to accurately determine the STR genotype of forensic samples using both platforms. Sequencing these STR loci provided extra information in the form of sequence variation, something that is not possible measuring amplicon length alone. And from these results we begin to characterise the sequence data from a forensic perspective, by looking at sequence variation within the repeats, stutter and heterozygote imbalance.},
	journal = {Forensic Science International: Genetics Supplement Series},
	author = {England, R. and Curnow, N. and Liu, A. and Stacey, J. and Harbison, S.},
	year = {2015},
	keywords = {Massively parallel sequencing, Short tandem repeats},
}

@article{knight_results_2010,
	title = {The results of an experimental indoor hydroponic {Cannabis} growing study, using the '{Screen} of {Green}' ({ScrOG}) method-{Yield}, tetrahydrocannabinol ({THC}) and {DNA} analysis},
	volume = {202},
	issn = {03790738},
	doi = {10.1016/j.forsciint.2010.04.022},
	abstract = {The results of an indoor hydroponic Cannabis growth study are presented. It is intended that this work will be of assistance to those with an interest in determining an estimation of yield and value of Cannabis crops. Three cycles of six plants were grown over a period of 1 year in order to ascertain the potential yield of female flowering head material from such an operation. The cultivation methods used were selected to replicate typical indoor hydroponic Cannabis growing operations, such as are commonly encountered by the New Zealand Police. The plants were also tested to ascertain the percentage of the psychoactive chemical Δ-9 tetrahydrocannabinol (THC) present in the flowering head material, and were genetically profiled by STR analysis. Phenotypic observations are related to the data collected. The inexperience of the growers was evidenced by different problems encountered in each of the three cycles, each of which would be expected to negatively impact the yield and THC data obtained. These data are therefore considered to be conservative. The most successful cycle yielded an average of 881. g (31.1. oz) of dry, groomed female flowering head per plant, and over the whole study the 18 plants yielded a total of 12,360. g (436.0. oz), or an average of 687. g (24.2. oz) of dry head per plant. THC data shows significant intra-plant variation and also demonstrates inter-varietal variation. THC values for individual plants ranged from 4.3 to 25.2\%. The findings of this study and a separate ESR research project illustrate that the potency of Cannabis grown in New Zealand has dramatically increased in recent years. DNA analysis distinguished distinct groups in general agreement with the phenotypic variation observed. One plant however, exhibiting a unique triallelic pattern at two of the five loci tested, while remaining phenotypically indistinguishable from three other plants within the same grow. © 2010 Elsevier Ireland Ltd.},
	number = {1-3},
	journal = {Forensic Science International},
	author = {Knight, G. and Hansen, S. and Connor, M. and Poulsen, H. and McGovern, C. and Stacey, J.},
	year = {2010},
	keywords = {DNA, Cannabis, Hydroponic cultivation, Screen of Green (ScrOG), Tetrahydrocannabinol (THC), Yield},
}

@article{mayer_epigenetics-inspired_2016,
	title = {An {Epigenetics}-{Inspired} {DNA}-{Based} {Data} {Storage} {System}},
	volume = {55},
	issn = {15213773},
	doi = {10.1002/anie.201605531},
	abstract = {Biopolymers are an attractive alternative to store and circulate information. DNA, for example, combines remarkable longevity with high data storage densities and has been demonstrated as a means for preserving digital information. Inspired by the dynamic, biological regulation of (epi)genetic information, we herein present how binary data can undergo controlled changes when encoded in synthetic DNA strands. By exploiting differential kinetics of hydrolytic deamination reactions of cytosine and its naturally occurring derivatives, we demonstrate how multiple layers of information can be stored in a single DNA template. Moreover, we show that controlled redox reactions allow for interconversion of these DNA-encoded layers of information. Overall, such interlacing of multiple messages on synthetic DNA libraries showcases the potential of chemical reactions to manipulate digital information on (bio)polymers.},
	number = {37},
	journal = {Angewandte Chemie - International Edition},
	author = {Mayer, Clemens and McInroy, Gordon R. and Murat, Pierre and Van Delft, Pieter and Balasubramanian, Shankar},
	year = {2016},
}

@article{akram_trends_2018,
	title = {Trends to store digital data in {DNA}: an overview},
	volume = {45},
	issn = {15734978},
	doi = {10.1007/s11033-018-4280-y},
	abstract = {There has been an ascending growth in the capacity of information being generated. The increased production of data in turn has put forward other challenges as well thus, and there is the need to store this information and not only to store it but also to retain it for a prolonged time period. The reliance on DNA as a dense storage medium with high storage capacity and its ability to withstand extreme environmental conditions has increased over the past few years. There have been developments in reading and writing different forms of data on DNA, codes for encrypting data and using DNA as a way of secret writing leading towards new styles like stenography and cryptography. The article outlines different methods adopted for storing digital data on DNA with pros and cons of each method that has been applied plus the advantages and limitations of using DNA as a storage medium.},
	number = {5},
	journal = {Molecular Biology Reports},
	author = {Akram, Fatima and Haq, Ikram ul and Ali, Haider and Laghari, Aiman Tahir},
	year = {2018},
}

@article{Vlek2015,
	title = {Representing the {Quality} of {Crime} {Scenarios} in a {Bayesian} {Network}},
	volume = {279},
	issn = {18798314},
	url = {https://ebooks.iospress.nl/doi/10.3233/978-1-61499-609-5-131},
	doi = {10.3233/978-1-61499-609-5-131},
	abstract = {Bayesian networks have gained popularity as a probabilistic tool for reasoning with legal evidence. However, two common difficulties are (1) the construction and (2) the understanding of a network. In previous work, we proposed to use narrative tools and in particular scenario schemes to assist the construction and the understanding of Bayesian networks for legal cases. We proposed a construction method and a reporting format for explaining or understanding the network. The quality of a scenario, which plays an important role in the narrative approach to evidential reasoning, was not yet included in this method. In this paper, we provide a discussion of what constitutes the quality of a scenario, in terms of the narrative concepts of completeness, consistency and plausibility. We propose a probabilistic interpretation of these concepts, and show how they can be incorporated in our previously proposed method. We also illustrate with an example how these concepts concerning scenario quality can be used to explain or understand a Bayesian network.},
	urldate = {2024-07-05},
	journal = {Frontiers in Artificial Intelligence and Applications},
	author = {Vlek, Charlotte S. and Prakken, Henry and Renooij, Silja and Verheij, Bart},
	year = {2015},
	note = {Publisher: IOS Press
ISBN: 9781614996088},
	keywords = {Bayesian networks, Narrative, Reasoning about legal evidence},
	pages = {131--140},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\GJFK3H8P\\Vleketal2015.pdf:application/pdf},
}

@article{TaylorH2024,
	title = {Development and use of a directed acyclic graph ({DAG}) for conceptual framework and study protocol development exploring relationships between dwelling characteristics and household transmission of {COVID}-19 – {England}, 2020},
	volume = {250},
	issn = {0360-1323},
	doi = {10.1016/J.BUILDENV.2023.111145},
	abstract = {Background: Household settings are high risk for COVID-19 transmission. Understanding transmission factors associated with environmental dwelling characteristics is important in informing public health and building design recommendations. We aimed to develop a directed acyclic graph (DAG) to inform a novel analytical study examining the effect of dwelling environmental characteristics on household transmission of COVID-19. Methods: Key demographic, behavioural and environmental dwelling characteristics were identified by a multidisciplinary team. Using the DAG to visually display risk factors, and using expert knowledge of available datasets we reached a consensus on the factors included and directionality of relationships to build the final conceptual framework. Factors were displayed as nodes and relationships as pathways. Results: Of 34 potential factors, 16 were included in the DAG, with 13 causal and three biasing pathways. Three variables were not measurable using retrospective datasets. The DAG enabled us to select data sources for the pilot study period and to inform the analysis plan. Key exposure nodes were energy efficiency or dwelling age; dwelling type or number of storeys; and dwelling size. We determined direct and proxy confounders which we could adjust for, potential interactions terms we could test in model building, and co-linear variables to omit in the same model. Conclusions: The DAG helped identify key variables and datasets. It prioritised key nodes and pathways to formalise complex relationships between variables. It was pivotal in identifying unobserved variables, confounders, co-linearity and potential interactions. It has supported data selection and design of a retrospective pilot study analysis plan.},
	urldate = {2024-07-07},
	journal = {Building and Environment},
	author = {Taylor, Hannah and Crabbe, Helen and Humphreys, Clare and Dabrera, Gavin and Mavrogianni, Anna and Verlander, Neville Q. and Leonardi, Giovanni S.},
	month = feb,
	year = {2024},
	note = {Publisher: Pergamon},
	keywords = {COVID-19, DAG, Directed Acyclic Graphs, Dwelling characteristics, Environmental exposures, Epidemiology, Housing, Public Health, Study design},
	pages = {111145},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\EW33FWIL\\full-text.pdf:application/pdf},
}

@misc{BenGal2007,
	title = {Bayesian {Networks}},
	abstract = {Adaptive management is an iterative process of gathering new knowledge regarding a system's behavior and monitoring the ecological consequences of management actions to improve management decisions. Although the concept originated in the 1970s, it is rarely actively incorporated into ecological restoration. Bayesian networks (BNs) are emerging as efficient ecological decision-support tools well suited to adaptive management, but examples of their application in this capacity are few. We developed a BN within an adaptive-management framework that focuses on managing the effects of feral grazing and prescribed burning regimes on avian diversity within woodlands of subtropical eastern Australia. We constructed the BN with baseline data to predict bird abundance as a function of habitat structure, grazing pressure, and prescribed burning. Results of sensitivity analyses suggested that grazing pressure increased the abundance of aggressive honeyeaters, which in turn had a strong negative effect on small passerines. Management interventions to reduce pressure of feral grazing and prescribed burning were then conducted, after which we collected a second set of field data to test the response of small passerines to these measures. We used these data, which incorporated ecological changes that may have resulted from the management interventions, to validate and update the BN. The network predictions of small passerine abundance under the new habitat and management conditions were very accurate. The updated BN concluded the first iteration of adaptive management and will be used in planning the next round of management interventions. The unique belief-updating feature of BNs provides land managers with the flexibility to predict outcomes and evaluate the effectiveness of management interventions.},
	journal = {Encyclopedia of Statistics in Quality and Reliability},
	publisher = {Wiley},
	author = {Ben‐Gal, Irad},
	month = dec,
	year = {2007},
	doi = {10.1002/9780470061572.eqr089},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\328FH8JQ\\B.Gal(2008)Encyclopedia_of_Stats_in_Quality&Reliability.pdf:application/pdf},
}

@techreport{HuginWP2016,
	title = {The use of {HUGIN} software for forensic identification problems involving {DNA} evidence},
	url = {www.hugin.com},
	abstract = {This white paper illustrates how to use HUGIN Bayesian network software for forensic identification problems using DNA profiles. Bayesian networks are ideal for expressing, manipulating, and resolving identity questions in a variety of forensic contexts, including identification based on DNA evidence. Using HUGIN software forensic science service organizations can develop and integrate advanced probabilistic evidence calculations in their own systems for analyzing DNA and calculating probabilities of identity. Computations are made quickly, reliably, and efficiently-even in complex cases involving several and possibly mixed traces of DNA, combining evidence from alternative sources.},
	author = {{Hugin Expert A/S}},
	year = {2016},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\QQMBHNIC\\Hugin Forensic-White-Paper.pdf:application/pdf},
}

@article{ChenPollino2012,
	title = {Good practice in {Bayesian} network modelling},
	volume = {37},
	issn = {13648152},
	doi = {10.1016/j.envsoft.2012.03.012},
	abstract = {Bayesian networks (BNs) are increasingly being used to model environmental systems, in order to: integrate multiple issues and system components; utilise information from different sources; and handle missing data and uncertainty. BNs also have a modular architecture that facilitates iterative model development. For a model to be of value in generating and sharing knowledge or providing decision support, it must be built using good modelling practice. This paper provides guidelines to developing and evaluating Bayesian network models of environmental systems, and presents a case study habitat suitability model for juvenile Astacopsis gouldi, the giant freshwater crayfish of Tasmania. The guidelines entail clearly defining the model objectives and scope, and using a conceptual model of the system to form the structure of the BN, which should be parsimonious yet capture all key components and processes. After the states and conditional probabilities of all variables are defined, the BN should be assessed by a suite of quantitative and qualitative forms of model evaluation. All the assumptions, uncertainties, descriptions and reasoning for each node and linkage, data and information sources, and evaluation results must be clearly documented. Following these standards will enable the modelling process and the model itself to be transparent, credible and robust, within its given limitations. © 2012 Elsevier Ltd.},
	journal = {Environmental Modelling and Software},
	author = {Chen, Serena H. and Pollino, Carmel A.},
	month = nov,
	year = {2012},
	keywords = {Bayes network, Bayesian belief network, Ecological models, Good modelling practice, Integration, Model evaluation},
	pages = {134--145},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ZWYTK5JE\\Chen and Pollino 2012.pdf:application/pdf},
}

@article{BiedermanTaroni2012,
	title = {Bayesian networks for evaluating forensic {DNA} profiling evidence: {A} review and guide to literature},
	volume = {6},
	issn = {18724973},
	doi = {10.1016/j.fsigen.2011.06.009},
	abstract = {Almost 30 years ago, Bayesian networks (BNs) were developed in the field of artificial intelligence as a framework that should assist researchers and practitioners in applying the theory of probability to inference problems of more substantive size and, thus, to more realistic and practical problems. Since the late 1980s, Bayesian networks have also attracted researchers in forensic science and this tendency has considerably intensified throughout the last decade. This review article provides an overview of the scientific literature that describes research on Bayesian networks as a tool that can be used to study, develop and implement probabilistic procedures for evaluating the probative value of particular items of scientific evidence in forensic science. Primary attention is drawn here to evaluative issues that pertain to forensic DNA profiling evidence because this is one of the main categories of evidence whose assessment has been studied through Bayesian networks. The scope of topics is large and includes almost any aspect that relates to forensic DNA profiling. Typical examples are inference of source (or, 'criminal identification'), relatedness testing, database searching and special trace evidence evaluation (such as mixed DNA stains or stains with low quantities of DNA). The perspective of the review presented here is not exclusively restricted to DNA evidence, but also includes relevant references and discussion on both, the concept of Bayesian networks as well as its general usage in legal sciences as one among several different graphical approaches to evidence evaluation. © 2011 Elsevier Ireland Ltd. All rights reserved.},
	number = {2},
	journal = {Forensic Science International: Genetics},
	author = {Biedermann, A. and Taroni, F.},
	month = mar,
	year = {2012},
	pmid = {21775236},
	keywords = {Bayesian networks, DNA evidence, Object-oriented Bayesian networks},
	pages = {147--157},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\3W5VW46D\\forensicDNA.pdf:application/pdf},
}

@article{Koeijer2020,
	title = {Combining evidence in complex cases - a practical approach to interdisciplinary casework},
	volume = {60},
	issn = {18764452},
	doi = {10.1016/j.scijus.2019.09.001},
	abstract = {Activity level evaluations, although still a major challenge for many disciplines, bring a wealth of possibilities for a more formal approach to the evaluation of interdisciplinary forensic evidence. This paper proposes a practical methodology for combining evidence from different disciplines within the likelihood ratio framework. Evidence schemes introduced in this paper make the process of combining evidence more insightful and intuitive thereby assisting experts in their interdisciplinairy evaluation and in explaining this process to the courts. When confronted with two opposing scenarios and multiple types of evidence, the likelihood ratio approach allows experts to combine this evidence in a probabilistic manner. Parts of the prosecution and defence scenarios for which forensic science is expected to be informative are identified. For these so called core elements, activity level propositions are formulated. Afterwards evidence schemes are introduced to assist the expert in combining the evidence in a logical manner. Two types of evidence relations are identified: serial and parallel evidence. Practical guidelines are given on how to deal with both types of evidence relations when combining the evidence.},
	number = {1},
	journal = {Science and Justice},
	author = {de Koeijer, Jan A. and Sjerps, Marjan J. and Vergeer, Peter and Berger, Charles E.H.},
	month = jan,
	year = {2020},
	pmid = {31924285},
	note = {Publisher: Forensic Science Society},
	keywords = {Activity level, Combining evidence, Evidence scheme, Parallel evidence, Scenarios, Serial evidence},
	pages = {20--29},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\963VK9HC\\de Koeijer et al 2019.pdf:application/pdf},
}

@article{Zoete2015,
	title = {Modelling crime linkage with {Bayesian} networks},
	volume = {55},
	issn = {18764452},
	doi = {10.1016/j.scijus.2014.11.005},
	abstract = {When two or more crimes show specific similarities, such as a very distinct modus operandi, the probability that they were committed by the same offender becomes of interest. This probability depends on the degree of similarity and distinctiveness. We show how Bayesian networks can be used to model different evidential structures that can occur when linking crimes, and how they assist in understanding the complex underlying dependencies. That is, how evidence that is obtained in one case can be used in another and vice versa. The flip side of this is that the intuitive decision to "unlink" a case in which exculpatory evidence is obtained leads to serious overestimation of the strength of the remaining cases.},
	number = {3},
	journal = {Science and Justice},
	author = {de Zoete, Jacob and Sjerps, Marjan and Lagnado, David and Fenton, Norman},
	month = may,
	year = {2015},
	note = {Publisher: Forensic Science Society},
	keywords = {Bayesian networks, Combining evidence, Case linkage, Crime linkage, Serial crime},
	pages = {209--217},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\HLCFZBPN\\de Zoete et al 2014.pdf:application/pdf},
}

@article{Taylor2016,
	title = {Using sensitivity analyses in {Bayesian} {Networks} to highlight the impact of data paucity and direct future analyses: a contribution to the debate on measuring and reporting the precision of likelihood ratios},
	volume = {56},
	issn = {18764452},
	doi = {10.1016/j.scijus.2016.06.010},
	abstract = {Bayesian networks are being increasingly used to address complex questions of forensic interest. Like all probabilities, those that underlie the nodes within a network rely on structured data and knowledge. Obviously, the more structured data we have, the better. But, in real life, the numbers of experiments that can be carried out are limited. It is thus important to know if/when our knowledge is sufficient and when one needs to perform further experiments to be in a position to report the value of the observations made. To explore the impact of the amount of data that are available for assessing results, we have constructed Bayesian Networks and explored the sensitivity of the likelihood ratios to changes to the data that underlie each node. Bayesian networks are constructed and sensitivity analyses performed using freely available R libraries (gRain and BNlearn). We demonstrate how the analyses can be used to yield information about the robustness provided by the data used to inform the conditional probability table, and also how they can be used to direct further research for maximum effect. By maximum effect, we mean to contribute with the least investment to an increased robustness. In addition, the paper investigates the consequences of the sensitivity analysis to the discussion on how the evidence shall be reported for a given state of knowledge in terms of underpinning data.},
	number = {5},
	journal = {Science and Justice},
	author = {Taylor, Duncan and Hicks, Tacha and Champod, Christophe},
	month = sep,
	year = {2016},
	note = {Publisher: Forensic Science Society},
	keywords = {Bayesian networks, Data, Likelihood ratio, Sensitivity analysis, Source level propositions},
	pages = {402--410},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\Z5WP8BE6\\Taylor et al 2016.pdf:application/pdf},
}

@article{Schaapveld2019,
	title = {Bayesian networks for the interpretation of biological evidence},
	volume = {1},
	issn = {2573-9468},
	doi = {10.1002/wfs2.1325},
	abstract = {In court, it is typical for biological evidence to be reported at a level that only addresses how likely the DNA evidence is if it originated from a particular individual, or individuals. However, there are other questions that could be considered that would be of value in enabling the court, including the jury, to make better informed decisions. For example, although answers to specific questions such as: “Which type of bodily fluid has the DNA originated from?” or, “How was the DNA deposited at the scene?” would be probabilistic in nature, they can be crucial to the outcome of a case. The relationship between the DNA evidence, the source of the DNA and the activity that took place is described in a term called the “hierarchy of propositions.” Currently, such questions are usually answered by scientists subjectively with little to no logical framework to assist them. Bayesian networks have proven to be beneficial in providing logical reasoning by way of a likelihood ratio to help combine subjective, yet, experience‐based, opinions of experts with experimental data when answering questions which can be both complex and uncertain. These networks offer a framework that provides balance, transparency, and robustness in the evaluation of evidence. A current limitation of the use of Bayesian networks includes a lack of understanding of the underlying concepts from both forensic scientists and the courts and consequently a reduced recognition of the potential strengths. This article is categorized under:   Forensic Biology {\textgreater} Interpretation of Biological Evidence},
	number = {3},
	journal = {WIREs Forensic Science},
	author = {Schaapveld, Tayla E. M. and Opperman, Stephanie L. and Harbison, SallyAnn},
	month = may,
	year = {2019},
	note = {Publisher: Wiley},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\IQMQL2LT\\Bayesian_networks_for_the_interpretation_of_biolog.pdf:application/pdf},
}

@article{Aguilera2010,
	title = {Hybrid {Bayesian} network classifiers: {Application} to species distribution models},
	volume = {25},
	issn = {13648152},
	doi = {10.1016/j.envsoft.2010.04.016},
	abstract = {Bayesian networks are one of the most powerful tools in the design of expert systems located in an uncertainty framework. However, normally their application is determined by the discretization of the continuous variables. In this paper the naïve Bayes (NB) and tree augmented naïve Bayes (TAN) models are developed. They are based on Mixtures of Truncated Exponentials (MTE) designed to deal with discrete and continuous variables in the same network simultaneously without any restriction. The aim is to characterize the habitat of the spur-thighed tortoise (Testudo graeca graeca), using several continuous environmental variables, and one discrete (binary) variable representing the presence or absence of the tortoise. These models are compared with the full discrete models and the results show a better classification rate for the continuous one. Therefore, the application of continuous models instead of discrete ones avoids loss of statistical information due to the discretization. Moreover, the results of the TAN continuous model show a more spatially accurate distribution of the tortoise. The species is located in the Doñana Natural Park, and in semiarid habitats. The proposed continuous models based on MTEs are valid for the study of species predictive distribution modelling. © 2010 Elsevier Ltd.},
	number = {12},
	journal = {Environmental Modelling and Software},
	author = {Aguilera, P. A. and Fernández, A. and Reche, F. and Rumí, R.},
	month = dec,
	year = {2010},
	keywords = {Classification, Conservation planning, Hybrid Bayesian networks, Mixtures of truncated exponentials},
	pages = {1630--1639},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\3ZEQC48Q\\Aguilera et al 2010 hybrid BNs.pdf:application/pdf},
}

@article{Tayloretal2018,
	title = {A template for constructing {Bayesian} networks in forensic biology cases when considering activity level propositions},
	volume = {33},
	issn = {18780326},
	doi = {10.1016/j.fsigen.2017.12.006},
	abstract = {The hierarchy of propositions has been accepted amongst the forensic science community for some time. It is also accepted that the higher up the hierarchy the propositions are, against which the scientist are competent to evaluate their results, the more directly useful the testimony will be to the court. Because each case represents a unique set of circumstances and findings, it is difficult to come up with a standard structure for evaluation. One common tool that assists in this task is Bayesian networks (BNs). There is much diversity in the way that BN can be constructed. In this work, we develop a template for BN construction that allows sufficient flexibility to address most cases, but enough commonality and structure that the flow of information in the BN is readily recognised at a glance. We provide seven steps that can be used to construct BNs within this structure and demonstrate how they can be applied, using a case example.},
	journal = {Forensic Science International: Genetics},
	author = {Taylor, Duncan and Biedermann, Alex and Hicks, Tacha and Champod, Christophe},
	month = mar,
	year = {2018},
	pmid = {29275089},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {DNA, Bayesian networks, Data, Likelihood ratio, Activity level propositions, Evidence evaluation},
	pages = {136--146},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\YRLWRDGW\\Taylor et al 2017.pdf:application/pdf},
}

@misc{HuginAPI1990,
	title = {{HUGIN} {API} {REFERENCE} {MANUAL}},
	author = {{Hugin Expert A/S}},
	year = {1990},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\GEEQG3J8\\hugin api-manual.pdf:application/pdf},
}

@article{Uitdehaag2022,
	title = {Use of {Bayesian} networks in forensic soil casework},
	volume = {62},
	issn = {18764452},
	doi = {10.1016/j.scijus.2022.02.005},
	abstract = {Forensic soil comparisons can be of high evidential value in a forensic case, but become complex when multiple methods and factors are considered. Bayesian networks are well suited to support forensic practitioners in complex casework. This study discusses the structure of a Bayesian network, elaborates on the in- and output data and evaluates two examples, one using source level propositions and one using activity level propositions. These examples can be applied as a template to construct a case specific network and can be used to assess sensitivity of the target output to different factors and identify avenues for research.},
	number = {2},
	journal = {Science and Justice},
	author = {Uitdehaag, S. C.A. and Donders, T. H. and Kuiper, I. and Wagner-Cremer, F. and Sjerps, M. J.},
	month = mar,
	year = {2022},
	pmid = {35277237},
	note = {Publisher: Forensic Science Society},
	keywords = {Activity level, Bayesian network, Distance measure, Elemental composition, Evaluate propositions, Palynology, Soil comparison, Source level},
	pages = {229--238},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\3FII2AEK\\Uitdehaag et al 2022.pdf:application/pdf},
}

@article{MarchTaroni2020,
	title = {Bayesian networks and dissonant items of evidence: {A} case study},
	volume = {44},
	issn = {18780326},
	doi = {10.1016/j.fsigen.2019.102172},
	abstract = {The assessment of different items of evidence is a challenging process in forensic science, particularly when the relevant elements support different inferential directions. In this study, a model is developed to assess the joint probative value of three different analyses related to some biological material retrieved on an object of interest in a criminal case. The study shows the ability of probabilistic graphical models, say Bayesian networks, to deal with complex situations, those that one expects to face in real cases. The results obtained by the model show the importance of a conflict measure as an indication of inconsistencies in the model itself. A contamination event alleged by the defense is also introduced in the model to explain and solve the conflict. The study aims to give an insight in the application of a probabilistic model to real criminal cases.},
	journal = {Forensic Science International: Genetics},
	author = {De March, Ilaria and Taroni, Franco},
	month = jan,
	year = {2020},
	pmid = {31629186},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Bayesian networks, DNA evidence, Activity level interpretation, Conflict measure},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\3PAJ7NDF\\March and Taroni 2019.pdf:application/pdf},
}

@techreport{Koller2013,
	title = {Object-{Oriented} {Bayesian} {Networks}},
	abstract = {Bayesian networks provide a modeling language and associated inference algorithm for stochastic domains. They have been successfully applied in a variety of medium-scale applications. However, when faced with a large complex domain, the task of modeling using Bayesian networks begins to resemble the task of pro­ gramming using logical circuits. In this paper, we de­ scribe an object-oriented Bayesian network (OOBN) lan­ guage, which allows complex domains to be described in terms of interrelated objects. We use a Bayesian net­ work fragment to describe the probabilistic relations be­ tween the attributes of an object. These attributes can themsel ves be objects, providing a natural framework for encoding part-of hierarchies. Classes are used to pro­ vide a reusable probabilistic model which can be applied to multiple similar objects. Classes also support inher­ itance of model fragments from a class to a subclass, allowing the common aspects of related classes to be defined only once. Our language has clear declarative semantics: an OOBN can be interpreted as a stochas­ tic functional program, so that it uniquely specifies a probabilistic model. We provide an inference algorithm for OOBNs, and show that much of the structural infor­ mation encoded by an OOBN-particularly the encap­ sulation of variables within an object and the reuse of model fragments in different contexts-can also be used to speed up the inference process.},
	author = {Koller, Daphne and Pfeffer, Avi},
	year = {2013},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\CT87UU3Y\\Koller and Pfeffer 2013.pdf:application/pdf},
}

@article{Samie2022,
	title = {Use of {Bayesian} {Networks} for the investigation of the nature of biological material in casework},
	volume = {331},
	issn = {18726283},
	doi = {10.1016/j.forsciint.2022.111174},
	abstract = {Chemical and staining methods, immunochromatography, spectroscopy, RNA expression or methylation patterns, do not allow to determine the nature of the biological material with certainty. However, to our knowledge, there are few forensic scientists that assess the value of such test results using a probabilistic approach. This is surprising as it would allow account for false positives and false negatives and avoid misleading conclusions. In this paper, we developed three Bayesian Networks (BNs) to assess the presence of blood, saliva and sperm in the recovered material and combine potentially contradictory observations. The approach was successfully tested using 188 traces from proficiency tests. We have implemented an online user-friendly application (https://forensic-genetic.shinyapps.io/BodyFluidsApp/) that allows forensic scientists to assess the value of their results without having to build Bayesian Networks themselves. They can also input their own data, use the application to identify a potential lack of knowledge and report their conclusions regarding the presence of sperm, blood or/and saliva considering uncertainty.},
	journal = {Forensic Science International},
	author = {Samie, Lydie and Champod, Christophe and Delémont, Séverine and Basset, Patrick and Hicks, Tacha and Castella, Vincent},
	month = feb,
	year = {2022},
	pmid = {34999364},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Saliva, Likelihood ratio, Body Fluid detection, Christmas Tree staining, OBTI, PSA, RSID Saliva, Bayes Theorem, Forensic Medicine},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\JHJNQ4CM\\Samie et al 2022.pdf:application/pdf},
}

@article{Cale2016,
	title = {Could {Secondary} {DNA} {Transfer} {Falsely} {Place} {Someone} at the {Scene} of a {Crime}?},
	volume = {61},
	issn = {15564029},
	doi = {10.1111/1556-4029.12894},
	abstract = {The occurrence of secondary DNA transfer has been previously established. However, the transfer of DNA through an intermediary has not been revisited with more sensitive current technologies implemented to increase the likelihood of obtaining results from low-template/low-quality samples. This study evaluated whether this increased sensitivity could lead to the detection of interpretable secondary DNA transfer profiles. After two minutes of hand to hand contact, participants immediately handled assigned knives. Swabbings of the knives with detectable amounts of DNA were amplified with the Identifiler® Plus Amplification Kit and injected on a 3130xl. DNA typing results indicated that secondary DNA transfer was detected in 85\% of the samples. In five samples, the secondary contributor was either the only contributor or the major contributor identified despite never coming into direct contact with the knife. This study demonstrates the risk of assuming that DNA recovered from an object resulted from direct contact.},
	number = {1},
	journal = {Journal of Forensic Sciences},
	author = {Cale, Cynthia M. and Earll, Madison E. and Latham, Krista E. and Bush, Gay L.},
	month = jan,
	year = {2016},
	pmid = {26331369},
	note = {Publisher: Blackwell Publishing Inc.},
	keywords = {Criminalistics, DNA analysis, Forensic casework, Forensic science, Identifiler® Plus, Secondary transfer},
	pages = {196--203},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\HBJPZDMK\\Cale_et_al-2015-Journal_of_Forensic_Sciences.pdf:application/pdf},
}

@inproceedings{Andersen1989,
	title = {{HUGIN}*-a {Shell} for {Building} {Bayesian} {Belief} {Universes} for {Expert} {Systems}},
	abstract = {Causal probabilistic networks have proved to be a useful knowledge representation tool for modelling domains where causal relations in a broad sense are a natural way of relating domain objects and where uncertainty is inherited in these relations. This paper outlines an implementation the HUGIN shell-for handling a domain model expressed by a causal probabilistic network. The only topological restriction imposed on the network is that, it must not contain any directed loops. The approach is illustrated step by step by solving a. genetic breeding problem. A graph representation of the domain model is interactively created by using instances of the basic network components-nodes and arcs-as building blocks. This structure, together with the quantitative relations between nodes and their immediate causes expressed as conditional probabilities, are automatically transformed into a tree structure, a junction tree. Here a computationally efficient and conceptually simple algebra of Bayesian belief universes supports incorporation of new evidence, propagation of information , and calculation of revised beliefs in the states of the nodes in the network. Finally, as an exam ple of a real world application, MUN1N an expert system for electromyography is discussed.},
	booktitle = {Conference: {Proceedings} of the 11th international joint conference on {Artificial} intelligence - {Volume} 2},
	author = {Andersen, Stig K and Olesen, Kristian G and Jensen, Finn V and Jensen, Frank},
	year = {1989},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\YQ9VD96V\\HUGIN-Building-Bayesian-Belief-Universes-for-Expert-Systems.pdf:application/pdf},
}

@misc{belyi_luna_2024,
	title = {Luna: {An} {Evaluation} {Foundation} {Model} to {Catch} {Language} {Model} {Hallucinations} with {High} {Accuracy} and {Low} {Cost}},
	url = {http://arxiv.org/abs/2406.00975},
	abstract = {Retriever Augmented Generation (RAG) systems have become pivotal in enhancing the capabilities of language models by incorporating external knowledge retrieval mechanisms. However, a significant challenge in deploying these systems in industry applications is the detection and mitigation of hallucinations: instances where the model generates information that is not grounded in the retrieved context. Addressing this issue is crucial for ensuring the reliability and accuracy of responses generated by large language models (LLMs) in diverse industry settings. Current hallucination detection techniques fail to deliver accuracy, low latency, and low cost simultaneously. We introduce Luna: a DeBERTA-large (440M) encoder, finetuned for hallucination detection in RAG settings. We demonstrate that Luna outperforms GPT-3.5 and commercial evaluation frameworks on the hallucination detection task, with 97\% and 91\% reduction in cost and latency, respectively. Luna is lightweight and generalizes across multiple industry verticals and out-of-domain data, making it an ideal candidate for industry LLM applications.},
	author = {Belyi, Masha and Friel, Robert and Shao, Shuai and Sanyal, Atindriyo},
	month = jun,
	year = {2024},
	note = {arXiv: 2406.00975},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\KJJETN7T\\Belyi et al 2024 - Luna detection.pdf:application/pdf},
}

@article{Tayloretal2019,
	title = {Using {Bayesian} networks to track {DNA} movement through complex transfer scenarios},
	volume = {42},
	issn = {18780326},
	doi = {10.1016/j.fsigen.2019.06.006},
	abstract = {Trace DNA and the manner in which it is transferred from item to item is a common topic arising in forensic science, both in case evaluations, and in Court testimony. In order to assign the probability of obtaining DNA findings, given competing propositions that specify transfer mechanisms, consideration must be given to a number of factors. Previous work by the authors developed a simple Object-Oriented Bayesian Network (OOBN) that pooled numerous published studies in order to attempt evaluation of trace DNA results given such propositions. In this work, we expand on the previously published OOBN and formalise a number of class networks that can be used together in a logical way to consider DNA movement through complex chains of transfer events. Specifically, we develop an OOBN that considers the two-way transfer of DNA that occurs when two items contact, and allows for the sampling of intermediary items involved in the chain of transfers. The aim is to show that adopting an approach involving basic building blocks, we offer the possibility to tackle complex and various cases for which the OOBN will be obtained by combining their elementary blocks. We conclude with a demonstration of applying the OOBN being applied to a chain of transfers in a case scenario.},
	journal = {Forensic Science International: Genetics},
	author = {Taylor, Duncan and Samie, Lydie and Champod, Christophe},
	month = sep,
	year = {2019},
	pmid = {31234042},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Object-oriented Bayesian networks, Activity level propositions, Background DNA, Contact DNA, DNA persistence, DNA transfer},
	pages = {69--80},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\W8QN3GIC\\Taylor et al 2019.pdf:application/pdf},
}

@misc{Singh2023,
	title = {A {Review} on {Objective}-{Driven} {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2308.10135},
	abstract = {While advancing rapidly, Artificial Intelligence still falls short of human intelligence in several key aspects due to inherent limitations in current AI technologies and our understanding of cognition. Humans have an innate ability to understand context, nuances, and subtle cues in communication, which allows us to comprehend jokes, sarcasm, and metaphors. Machines struggle to interpret such contextual information accurately. Humans possess a vast repository of common-sense knowledge that helps us make logical inferences and predictions about the world. Machines lack this innate understanding and often struggle with making sense of situations that humans find trivial. In this article, we review the prospective Machine Intelligence candidates, a review from Prof. Yann LeCun, and other work that can help close this gap between human and machine intelligence. Specifically, we talk about what's lacking with the current AI techniques such as supervised learning, reinforcement learning, self-supervised learning, etc. Then we show how Hierarchical planning-based approaches can help us close that gap and deep-dive into energy-based, latent-variable methods and Joint embedding predictive architecture methods.},
	author = {Singh, Apoorv},
	month = aug,
	year = {2023},
	note = {arXiv: 2308.10135},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\7R3VM7PX\\Overview of AI 2023.pdf:application/pdf},
}

@article{DNA-TRAC2018,
	title = {Sharing data on {DNA} transfer, persistence, prevalence and recovery: {Arguments} for harmonization and standardization},
	volume = {37},
	issn = {18780326},
	doi = {10.1016/j.fsigen.2018.09.006},
	abstract = {Sharing data between forensic scientists on DNA transfer, persistence, prevalence and recovery (TPPR) is crucial to advance the understanding of these issues in the criminal justice community. We present the results of a collaborative exercise on reporting forensic genetics findings given activity level propositions. This exercise outlined differences in the methodology that was applied by the participating laboratories, as well as limitations to the use of published data on DNA TPPR. We demonstrate how publication of experimental results in scientific journals can be further improved to allow for an adequate use of these data. Steps that can be taken to share and use these data for research and casework purposes are outlined, and the prospects for future sharing of data through publicly accessible databases are discussed. This paper also explores potential avenues to proceed with implementation and is intended to fuel the discussion on sharing data pertaining to DNA TPPR issues. It is further suggested that international standardization and harmonization on these topics will benefit the forensic DNA community as it has been achieved in the past with the harmonization of STR typing systems.},
	journal = {Forensic Science International: Genetics},
	author = {Kokshoorn, Bas and Aarts, Lambertus H.J. and Ansell, Ricky and Connolly, Edward and Drotz, Weine and Kloosterman, Ate D. and McKenna, Louise G. and Szkuta, Bianca and van Oorschot, Roland A.H.},
	month = nov,
	year = {2018},
	pmid = {30273824},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Criminalistics, DNA persistence, DNA transfer, Activity level inference, Data exchange, Database, DNA prevalence, DNA recovery, Forensic biology, Trace DNA},
	pages = {260--269},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\27WIXHMP\\Kokshoorn et al 2018.pdf:application/pdf},
}

@misc{Lewis2020,
	title = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	url = {http://arxiv.org/abs/2005.11401},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	month = may,
	year = {2020},
	note = {arXiv: 2005.11401},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Accepted at NeurIPS 2020},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\KWZ5SEJB\\RAG 2005.pdf:application/pdf},
}

@misc{Tay2020,
	title = {Efficient {Transformers}: {A} {Survey}},
	url = {http://arxiv.org/abs/2009.06732},
	abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
	author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.06732},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\GVQS8KCH\\Transformer survey.pdf:application/pdf},
}

@article{szkuta_transfer_2018,
	title = {Transfer and persistence of non-self {DNA} on hands over time: {Using} empirical data to evaluate {DNA} evidence given activity level propositions},
	volume = {33},
	issn = {18780326},
	doi = {10.1016/j.fsigen.2017.11.017},
	abstract = {Questions relating to how DNA from an individual got to where it was recovered from and the activities associated with its pickup, retention and deposition are increasingly relevant to criminal investigations and judicial considerations. To address activity level propositions, investigators are typically required to assess the likelihood that DNA was transferred indirectly and not deposited through direct contact with an item or surface. By constructing a series of Bayesian networks, we demonstrate their use in assessing activity level propositions derived from a recent legal case involving the alleged secondary transfer of DNA to a surface following a handshaking event. In the absence of data required to perform the assessment, a set of handshaking simulations were performed to obtain probabilities on the persistence of non-self DNA on the hands following a 40 min, 5 h or 8 h delay between the handshake and contact with the final surface (an axe handle). Variables such as time elapsed, and the activities performed and objects contacted between the handshake and contact with the axe handle, were also considered when assessing the DNA results. DNA from a known contributor was transferred to the right hand of an opposing hand-shaker (as a depositor), and could be subsequently transferred to, and detected on, a surface contacted by the depositor 40 min to 5 h post-handshake. No non-self DNA from the known contributor was detected in deposits made 8 h post-handshake. DNA from the depositor was generally detected as the major or only contributor in the profiles generated. Contributions from the known contributor were minor, decreasing in presence and in the strength of support for inclusion as the time between the handshake and transfer event increased. The construction of a series of Bayesian networks based on the case circumstances provided empirical estimations of the likelihood of direct or indirect deposition. The analyses and conclusions presented demonstrate both the complexity of activity level assessments concerning DNA evidence, and the power of Bayesian networks to visualise and explore the issues of interest for a given case.},
	journal = {Forensic Science International: Genetics},
	author = {Szkuta, Bianca and Ballantyne, Kaye N. and Kokshoorn, Bas and van Oorschot, Roland A.H.},
	month = mar,
	year = {2018},
	pmid = {29216581},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Activity level propositions, Bayesian network, DNA transfer, Handshaking},
	pages = {84--97},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\GIBWCL5Z\\Szkuta et al 2017.pdf:application/pdf},
}

@article{gosch_dna_2019,
	title = {On {DNA} transfer: {The} lack and difficulty of systematic research and how to do it better},
	volume = {40},
	issn = {18780326},
	doi = {10.1016/j.fsigen.2019.01.012},
	abstract = {Since DNA from touched items and surfaces (“touch DNA”) can successfully and reliably be analyzed, the question as to how a particular DNA containing sample came to be from where it was recovered is of increasing forensic interest and expert witnesses in court are increasingly challenged to assess for instance whether an incriminatory DNA sample matching to a suspect could have been transferred to the crime scene in an innocent manner and to guess at the probability of such an occurrence. The latter however will frequently entail expressing a subjective probability i.e. simply making a best guess from experience. There is, to the present date, an extensive and complex body of literature on primary, secondary, tertiary and even higher order DNA transfer, its possibility, plausibility, dependency on an array of variables and factors and vast numbers of permutations thereof. However, from our point of view there is a lack of systematic data on DNA transfer with existing research widely varying in quality and relevance. Our aim was, starting from a comprehensive survey of the status quo and appreciating its increasing importance, to in the first part of our review raise consciousness towards the underestimated and insufficiently accounted for complexity of DNA transfer and thus appendant research of forensic scientists serving as expert witnesses in court but also acting in the role of a journal referee to point them to areas of criticism when reviewing a manuscript on DNA transfer. In the second part, we present propositions how to systematize and integrate future research efforts concerning DNA transfer. Also, we present a searchable database providing an extensive overview of the current state of knowledge on DNA transfer, intended to facilitate the identification of relevant studies adding knowledge to a specific question and thus help forensic experts to base their opinion on a broader, more complete and more reproducible selection of studies.},
	journal = {Forensic Science International: Genetics},
	author = {Gosch, Annica and Courts, Cornelius},
	month = may,
	year = {2019},
	pmid = {30731249},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Forensic genetics, DNA transfer, Trace DNA, Touch DNA},
	pages = {24--36},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\57TQ3T7X\\On-DNA-transfer--The-lack-and-difficulty-of-syst_2019_Forensic-Science-Inter.pdf:application/pdf},
}

@article{samie_stabbing_2016,
	title = {Stabbing simulations and {DNA} transfer},
	volume = {22},
	issn = {18780326},
	doi = {10.1016/j.fsigen.2016.02.001},
	abstract = {Technical developments have made it possible to analyze very low amounts of DNA. This has many advantages, but the drawback of this technological progress is that interpretation of the results becomes increasingly complex: the number of mixed DNA profiles increased relatively to single source DNA profiles and stochastic effects in the DNA profile, such as drop-in and drop-out, are more frequently observed. Moreover, the relevance of low template DNA material regarding the activities alleged is not as straightforward as it was a few years ago, when for example large quantities of blood were recovered. The possibility of secondary and tertiary transfer is now becoming an issue. The purpose of this research is twofold: first, to study the transfer of DNA from the handler and secondly, to observe if handlers would transfer DNA from persons closely connected to them. We chose to mimic cases where the offender would attack a person with a knife. As a first approach, we envisaged that the defense would not give an alternative explanation for the origin of the DNA. In our transfer experiments (4 donors, 16 experiments each, 64 traces), 3\% of the traces were single DNA profiles. Most of the time, the DNA profile of the person handling the knife was present as the major profile: in 83\% of the traces the major contributor profile corresponded to the stabber's DNA profile (in single stains and mixtures). Mixture with no clear major/minor fraction (12\%) were observed. 5\% of the traces were considered of insufficient quality (more than 3 contributors, presence of a few minor peaks). In that case, we considered that the stabber's DNA was absent. In our experiments, no traces allowed excluding the stabber, however it must be noted that precautions were taken to minimize background DNA as knives were cleaned before the experiments. DNA profiles of the stabber's colleagues were not observed. We hope that this study will allow for a better understanding of the transfer mechanism and of how to assess and describe results given activity level propositions. In this preliminary research, we have focused on the transfer of DNA on the hand of the person. Besides, more research is needed to assign the probability of the results given an alternative activity proposed by the defense, for instance when the source of the DNA is not contested, but that the activities are.},
	journal = {Forensic Science International: Genetics},
	author = {Samie, Lydie and Hicks, Tacha and Castella, Vincent and Taroni, Franco},
	month = may,
	year = {2016},
	pmid = {26875110},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {DNA, Likelihood ratio, Activity level propositions, Bayesian framework, Knives, STRmix, Transfer},
	pages = {73--80},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\S9NASZGH\\Stabbing-simulations-and-DNA-trans_2016_Forensic-Science-International--Gene.pdf:application/pdf},
}

@article{Lehmann2015,
	title = {Following the transfer of {DNA}: {How} does the presence of background {DNA} affect the transfer and detection of a target source of {DNA}?},
	volume = {19},
	issn = {18780326},
	doi = {10.1016/j.fsigen.2015.05.002},
	abstract = {Abstract DNA transfer is of increasing importance in crime scene situations, partly due to analytical techniques detecting profiles in ever declining amounts of DNA. Whereas the focus has previously been DNA transfer of target sources, the effects of background DNA on transfer and detection of DNA after multiple contact situations have been much less investigated. This study measured the transfer and detection rates of a specific DNA source in the presence of background DNA sources. The presence of background DNA influenced the transfer of DNA differently depending on the combination of biological material and surface type. The detection of a profile from the target DNA decreased after multiple contact situations, due to the reduced total and relative quantity of target DNA, and the increasing complexity of the mixture. The results of this study contribute to a greater understanding of the effects of background DNA sources on DNA transfer and detection.},
	journal = {Forensic Science International: Genetics},
	author = {Lehmann, V. J. and Mitchell, R. J. and Ballantyne, K. N. and Van Oorschot, R. A.H.},
	month = jul,
	year = {2015},
	pmid = {26143222},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {DNA, Forensic science, Background DNA, Transfer, Contamination, Trace},
	pages = {68--75},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\G3N56DIZ\\Following-the-transfer-of-DNA--How-does-the-presence-o_2015_Forensic-Science.pdf:application/pdf},
}

@article{Gill2021,
	title = {An {LR} framework incorporating sensitivity analysis to model multiple direct and secondary transfer events on skin surface},
	volume = {53},
	issn = {18780326},
	doi = {10.1016/j.fsigen.2021.102509},
	abstract = {Bayesian logistic regression is used to model the probability of DNA recovery following direct and secondary transfer and persistence over a 24 h period between deposition and sample collection. Sub-source level likelihood ratios provided the raw data for activity-level analysis. Probabilities of secondary transfer are typically low, and there are challenges with small data-sets with low numbers of positive observations. However, the persistence of DNA over time can be modelled by a single logistic regression for both direct and secondary transfer, except that the time since deposition must be compensated by an offset value for the latter. This simplifies the analysis. Probabilities are used to inform an activity-level Bayesian Network that takes account of alternative propositions e.g. time of assault and time of social activities. The model is extended in order to take account of multiple contacts between person of interest and ’victim’. Variables taken into account include probabilities of direct and secondary transfer, along with background DNA from unknown individuals. The logistic regression analysis is Bayesian — for each analysis, 4000 separate simulations were carried out. Quantile assignments enable calculation of a plausible range of probabilities and sensitivity analysis is used to describe the corresponding variation of LRs that occur when modelled by the Bayesian network. It is noted that there is need for consistent experimental design, and analysis, to facilitate inter-laboratory comparisons. Appropriate recommendations are made. The open-source program written in R-code ALTRaP (Activity Level, Transfer, Recovery and Persistence) enables analysis of complex multiple transfer propositions that are commonplace in cases-work e.g. between those who cohabit. A number of case examples are provided. ALTRaP can be used to replicate the results and can easily be modified to incorporate different sets of data and variables.},
	journal = {Forensic Science International: Genetics},
	author = {Gill, Peter and Bleka, Øyvind and Roseth, Arne and Fonneløp, Ane Elida},
	month = jul,
	year = {2021},
	pmid = {33930816},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Evidence evaluation, Bayesian network, Secondary transfer, ALTRaP, Direct transfer, Likelihood ratio (LR), Mixtures, Likelihood ratio ()},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\PR6Z2QUK\\An-LR-framework-incorporating-sensitivity-analysis-to_2021_Forensic-Science-.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\HUHMRYM2\\S1872497321000478.html:text/html},
}

@article{szkuta_transfer_2017,
	title = {Transfer and persistence of {DNA} on the hands and the influence of activities performed},
	volume = {28},
	issn = {18780326},
	doi = {10.1016/j.fsigen.2017.01.006},
	abstract = {During the evaluation of forensic DNA evidence in court proceedings, the emphasis previously placed on the source of the DNA is progressively shifting to the consideration of the activities resulting in its deposition. While direct contact and deposition may be a likely explanation, alternative scenarios involving DNA transfer through a secondary person or medium are important to consider. Here we assessed whether non-self DNA, indirectly transferred via a handshake, could be detected on surfaces contacted by the opposing hand-shaker after 15 min, and considered the variables affecting its persistence in subsequent contacts. In general, the depositor of the handprint was the major contributor to DNA profiles collected from handprints placed on glass plates. Minor contributions from the opposing hand-shaker (as a known contributor) were detected at a lower rate, decreasing as the number of contacted items increased post-handshake. Delays in deposition also affected the detection of the opposing hand-shaker, with a 15 min delay between handshaking and contact resulting in the reduced presence, and corresponding LRs, of the known contributor. The handprint depositor was excluded from their own handprint on several occasions, including instances where the opposing hand-shaker was not excluded from the same profile. Several factors appeared to strongly influence the detection of both the depositor and contributing individual involved in the handshake. The relative shedding ability of the pair had the largest effect, where good shedders (whether depositor or contributor) could swamp poor to moderate shedders, while the pairing of two moderate or two poor shedders could result in the detection of both individuals. When the deposition of a handprint was delayed, the activities performed by the individual had a substantial effect on the resultant detection of the contributing profile – multiple contacts with the same items increased the likelihood that the known contributor's DNA would be retained and subsequently detected, through the parking and re-transfer of DNA on used items.},
	journal = {Forensic Science International: Genetics},
	author = {Szkuta, Bianca and Ballantyne, Kaye N. and van Oorschot, Roland A.H.},
	month = may,
	year = {2017},
	pmid = {28126692},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {DNA, activity, forensic, handshake, transfer},
	pages = {10--20},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\LFULPW49\\Transfer-and-persistence-of-DNA-on-the-hands-and_2017_Forensic-Science-Inter.pdf:application/pdf},
}

@article{szkuta_dna_2020,
	title = {{DNA} transfer to worn upper garments during different activities and contacts: {An} inter-laboratory study},
	volume = {46},
	issn = {18780326},
	doi = {10.1016/j.fsigen.2020.102268},
	abstract = {Cellular material derived from contact traces can be transferred via many direct and indirect routes, with the manner of contact and the time of transfer (in relation to the alleged crime-event) having an impact on whether DNA is recovered from the surface and a reportable profile generated. In an effort to acquire information on the transfer and recovery of DNA traces from clothing items worn during scenarios commonly encountered in casework, upper garments were worn during a normal working day before individuals were paired to embrace one another (‘contact’), go on an outing together (‘close proximity’), or individually asked to spend a day in another person's environment (‘physical absence’). Each prescribed activity was repeated by sixteen individuals across four countries, and was the last activity performed before the garment was removed. Samples were collected from several areas of the upper garments and processed from DNA extraction through to profiling within the laboratory of the country in which the individual resided. Activities relating to the garment prior to and during wearing, including the prescribed activity, were recorded by the participant and considered during the interpretation of results. In addition to obtaining reference profiles from the wearer and their activity partner, DNA profiles from the wearers’ close associates identified in the questionnaire were obtained to assess the impact of background DNA transferred prior to the prescribed activity. The wearer was typically, but not always, observed as the major contributor to the profiles obtained. DNA from the activity partner was observed on several areas of the garment following the embrace and after temporarily occupying another person's space. Particular areas of the garment were more prone to acquiring the hugging partner or office owner's DNA than others, and whether they were observed as the major or minor component was activity dependent. For each of the pairs, no DNA from the activity partner was acquired by the garments during the outing, even though both participants were in close proximity. This study provides empirical data on the transfer, persistence, prevalence and recovery of DNA from clothing items, and enables a better understanding of the mechanisms which lead to the transfer and detectability of DNA traces in different scenarios.},
	journal = {Forensic Science International: Genetics},
	author = {Szkuta, Bianca and Ansell, Ricky and Boiso, Lina and Connolly, Edward and Kloosterman, Ate D. and Kokshoorn, Bas and McKenna, Louise G. and Steensma, Kristy and van Oorschot, Roland A.H.},
	month = may,
	year = {2020},
	pmid = {32172221},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Activity level, DNA persistence, DNA transfer, DNA prevalence, DNA recovery, Clothing},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\V9WE2NKT\\Szkuta et al 2020.pdf:application/pdf},
}

@article{lang_using_2023,
	title = {Using convolutional neural networks to support examiners in duct tape physical fit comparisons},
	volume = {353},
	issn = {18726283},
	doi = {10.1016/j.forsciint.2023.111884},
	abstract = {This paper describes the construction and use of a machine-learning model to provide objective support for a physical fit examination of duct tapes. We present the ForensicFit package that can preprocess and database raw tape images. Using the processed tape image, we trained a convolutional neural network to compare tape edges and predict membership scores (i.e., fit or non-fit category). A dataset of nearly 2000 tapes and 4000 images was evaluated, including various quality grades: low, medium, and high, as well as two separation methods, scissor-cut and hand-torn. The model predicts medium-quality and high-quality scissor-cut tape more accurately than hand-torn, whereas for low-quality tape predicts the hand-torn tapes more accurately. These results are consistent with previous studies performed on the same datasets by analyst examinations. A method of pixel importance was also implemented to show which pixels are used to make the decision. This method can confirm some fit features that correspond with analyst-identified features, like edge morphology and backing pattern. This pilot study demonstrates the feasibility of computational algorithms to build physical fit databases and automated comparisons using deep neural networks, which can be used as a model for other materials.},
	journal = {Forensic Science International},
	author = {Lang, Logan and Tavadze, Pedram and Prusinowski, Meghan and Andrews, Zachary and Neumann, Cedric and Trejos, Tatiana and Romero, Aldo H.},
	month = dec,
	year = {2023},
	pmid = {37989070},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Machine learning, Convolutional neural networks, Decision tree, Duct tape, Physical fit},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\D92WAVUI\\DuctTapeFit2023.pdf:application/pdf},
}

@book{Bolstad2016,
	title = {Introduction to {Bayesian} {Statistics}},
	publisher = {John Wiley {\textbackslash}\& Sons, Inc.},
	author = {Bolstad, William M. and Curran, James M.},
	year = {2016},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\QN3MA4WZ\\IntroBayesStats.pdf:application/pdf},
}

@article{Spaulding2023,
	title = {Characterization of fracture match associations with automated image processing},
	volume = {342},
	issn = {18726283},
	doi = {10.1016/j.forsciint.2022.111519},
	abstract = {During the examination of trace evidence, often a realignment along the edges of known and questioned items are made to determine if a physical fit is present and if those objects were once one continuous piece or object. Duct tape is an evidence type in which the evaluation of physical fits is often conducted and is regarded as conclusive evidence of an association between the items. The examination and conclusion of a physical fit between edges relies heavily on examiner discretion to identify distinctive features across the edges since there are no statistical approaches or objective methodologies for the comparison. This study developed an automated image processing and comparison method to quantify tape end matches using cross-correlation scores and an empirical approach to the assessment. Characterization of 150 hand torn duct tape end pair physical fits were also conducted where matching and non-matching sample distributions were created. This study also evaluated partial duct tape edges and the influence this has on a comparison. Given the strength associated with a physical fit and the presence of stretching or deformation along the fractured edge, an understanding of the value these samples have is paramount. Furthermore, random match probabilities were calculated based on the correlation scores from the inter-comparisons to model the weight of evidence or strength of association between the edges. Finally, the study demonstrated that not every true match holds the same association strength through score distributions, but the approach is able to distinguish matching and non-matching samples at edge widths greater than 27 \%.},
	journal = {Forensic Science International},
	author = {Spaulding, Jamie S. and Picconatto, Gina M.},
	month = jan,
	year = {2023},
	pmid = {36423360},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Physical fit, Algorithm, Edge similarity score, End matching, Trace evidence},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ATK66JXI\\PysicalFit2023.pdf:application/pdf},
}

@misc{Edge2024,
	title = {From {Local} to {Global}: {A} {Graph} {RAG} {Approach} to {Query}-{Focused} {Summarization}},
	url = {http://arxiv.org/abs/2404.16130},
	abstract = {The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as "What are the main themes in the dataset?", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose a Graph RAG approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed. Our approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that Graph RAG leads to substantial improvements over a na{\textbackslash}"ive RAG baseline for both the comprehensiveness and diversity of generated answers. An open-source, Python-based implementation of both global and local Graph RAG approaches is forthcoming at https://aka.ms/graphrag.},
	author = {Edge, Darren and Trinh, Ha and Cheng, Newman and Bradley, Joshua and Chao, Alex and Mody, Apurva and Truitt, Steven and Larson, Jonathan},
	month = apr,
	year = {2024},
	note = {arXiv: 2404.16130},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\D8T28CM7\\graph RAG 2024.pdf:application/pdf},
}

@misc{Astronomer2024,
	title = {What is a {DAG}? ({Directed} {Acyclic} {Graph}) {\textbar} {Astronomer}},
	url = {https://www.astronomer.io/blog/what-exactly-is-a-dag/},
	urldate = {2024-07-07},
	author = {Gregory, Ben and Wrzosińska, Julia},
	year = {2017},
}

@book{Sridhar2024,
	address = {Cham},
	title = {Bayesian {Network} {Modeling} of {Corrosion}},
	isbn = {978-3-031-56127-6},
	url = {https://link.springer.com/10.1007/978-3-031-56128-3},
	urldate = {2024-07-09},
	publisher = {Springer International Publishing},
	author = {Sridhar, N},
	editor = {Sridhar, Narasi},
	year = {2024},
	doi = {10.1007/978-3-031-56128-3},
}

@article{Shen2020,
	title = {challenges and opportunities with causal {Discovery} {Algorithms}: {Application} to {Alzheimer}'s pathophysiology},
	url = {www.nature.com/scientificreports},
	doi = {10.1038/s41598-020-59669-x},
	abstract = {the Alzheimer's Disease neuroimaging initiative † causal Structure Discovery (cSD) is the problem of identifying causal relationships from large quantities of data through computational methods. With the limited ability of traditional association-based computational methods to discover causal relationships, cSD methodologies are gaining popularity. the goal of the study was to systematically examine whether (i) cSD methods can discover the known causal relationships from observational clinical data and (ii) to offer guidance to accurately discover known causal relationships. We used Alzheimer's disease (AD), a complex progressive disease, as a model because the well-established evidence provides a "gold-standard" causal graph for evaluation. We evaluated two cSD methods, fast causal inference (fci) and fast Greedy equivalence Search (fGeS) in their ability to discover this structure from data collected by the Alzheimer's Disease neuroimaging initiative (ADni). We used structural equation models (which is not designed for cSD) as control. We applied these methods under three scenarios defined by increasing amounts of background knowledge provided to the methods. the methods were evaluated by comparing the resulting causal relationships with the "gold standard" graph that was constructed from literature. Dedicated cSD methods managed to discover graphs that nearly coincided with the gold standard. for best results, cSD algorithms should be used with longitudinal data providing as much prior knowledge as possible.},
	urldate = {2024-07-12},
	author = {Shen, Xinpeng and Ma, Sisi and Vemuri, prashanthi and Simon, Gyorgy},
	year = {2020},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\WZLHEM3W\\full-text.pdf:application/pdf},
}

@article{Lemmer2013,
	title = {Causal {Modeling}},
	url = {https://arxiv.org/abs/1303.1471},
	author = {Lemmer, John F},
	year = {2013},
	note = {Publication Title: arxiv
arXiv: 1303.1471},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\CXBUXAZZ\\Lemmer2013.pdf:application/pdf},
}

@article{GarbolinoTaroni2002,
	title = {Evaluation of scientific evidence using {Bayesian} networks},
	volume = {125},
	issn = {0379-0738},
	doi = {10.1016/S0379-0738(01)00642-9},
	abstract = {Bayesian networks provide a valuable aid for representing epistemic relationships in a body of uncertain evidence. The paper proposes some simple Bayesian networks for standard analysis of patterns of inference concerning scientific evidence, with a discussion of the rationale behind the nets, the corresponding probabilistic formulas, and the required probability assessments. Copyright © 2002 Elsevier Science Ireland Ltd.},
	number = {2-3},
	urldate = {2024-07-12},
	journal = {Forensic Science International},
	author = {Garbolino, Paolo and Taroni, Franco},
	month = feb,
	year = {2002},
	pmid = {11909657},
	note = {Publisher: Elsevier},
	keywords = {Bayesian networks, Likelihood ratio, Bayes theorem, Evidence interpretation},
	pages = {149--155},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\P9I5SAGI\\full-text.pdf:application/pdf},
}

@article{Sinharay2010,
	title = {Continuous {Probability} {Distributions}},
	doi = {10.1016/B978-0-08-044894-7.01720-6},
	abstract = {This article provides an overview of the most extensively used continuous distributions in statistics. Both univariate and multivariate distributions are covered. The distributions covered in this article are: beta, Cauchy, χ2, Dirichlet, exponential, exponential family, F, gamma, lognormal, Student's t, uniform, and Wishart.},
	urldate = {2024-07-16},
	journal = {International Encyclopedia of Education, Third Edition},
	author = {Sinharay, S.},
	month = jan,
	year = {2010},
	note = {Publisher: Elsevier
ISBN: 9780080448947},
	keywords = {Beta distribution, Expectation, Gamma distribution, Mean, Moments, Probability density function, Random number, Random variable, χ 2 distribution},
	pages = {98--102},
}

@article{BiedermannTaroni2006,
	title = {Bayesian networks and probabilistic reasoning about scientific evidence when there is a lack of data},
	volume = {157},
	issn = {0379-0738},
	doi = {10.1016/J.FORSCIINT.2005.09.008},
	abstract = {Bayesian networks (BNs) are a kind of graphical model that formally combines elements of graph and probability theory. BNs are a mathematically and statistically rigorous technique allowing their user to define a pictorial representation of assumed dependencies and influences among a set of variables deemed to be relevant for a particular inferential problem. The formalism allows one to process newly acquired evidence according to the rules of probability calculus. Applications of BNs have been reported in various forensic disciplines. However, there seems to be some reluctance to consider BNs as a more general framework for representing and evaluating sources of uncertainties associated with scientific evidence. Notably, BNs are widely thought of as an essentially numerical method, requiring "exact" numbers with a high "accuracy". The present paper aims to draw the reader's attention to the point that the availability of hard numerical data is not a necessary requirement for using BNs in forensic science. An abstraction of quantitative BNs, known as qualitative probabilistic networks (QPNs), and sensitivity analyses are presented and their potential applications discussed. As a main difference to their quantitative counterpart, QPNs contain qualitative probabilistic relationships instead of numerical relations. Sensitivity analyses consist of varying the probabilities assigned to one or more variables and evaluating the effect on one or more other variables of interest. Both QPNs and sensitivity analyses appear to be useful concepts that permit one to work in contexts with acute lack of numerical data and where reasoning consistent with the laws of probability should nevertheless be performed. © 2005 Elsevier Ireland Ltd. All rights reserved.},
	number = {2-3},
	urldate = {2024-07-17},
	journal = {Forensic Science International},
	author = {Biedermann, A. and Taroni, F.},
	month = mar,
	year = {2006},
	note = {Publisher: Elsevier},
	keywords = {Bayesian networks, Probability, Qualitative probabilistic networks},
	pages = {163--167},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\ZDMIFLRG\\full-text.pdf:application/pdf;PDF:C\:\\Users\\jstacey\\Zotero\\storage\\LCPMGR7K\\Biedermann and Taroni - 2006 - Bayesian networks and probabilistic reasoning about scientific evidence when there is a lack of data.pdf:application/pdf},
}

@article{Biedermann2009,
	title = {Implementing statistical learning methods through {Bayesian} networks. {Part} 1: {A} guide to {Bayesian} parameter estimation using forensic science data},
	volume = {193},
	issn = {0379-0738},
	doi = {10.1016/J.FORSCIINT.2009.09.007},
	abstract = {As a thorough aggregation of probability and graph theory, Bayesian networks currently enjoy widespread interest as a means for studying factors that affect the coherent evaluation of scientific evidence in forensic science. Paper I of this series of papers intends to contribute to the discussion of Bayesian networks as a framework that is helpful for both illustrating and implementing statistical procedures that are commonly employed for the study of uncertainties (e.g. the estimation of unknown quantities). While the respective statistical procedures are widely described in literature, the primary aim of this paper is to offer an essentially non-technical introduction on how interested readers may use these analytical approaches - with the help of Bayesian networks - for processing their own forensic science data. Attention is mainly drawn to the structure and underlying rationale of a series of basic and context-independent network fragments that users may incorporate as building blocs while constructing larger inference models. As an example of how this may be done, the proposed concepts will be used in a second paper (Part II) for specifying graphical probability networks whose purpose is to assist forensic scientists in the evaluation of scientific evidence encountered in the context of forensic document examination (i.e. results of the analysis of black toners present on printed or copied documents). © 2009 Elsevier Ireland Ltd. All rights reserved.},
	number = {1-3},
	urldate = {2024-07-17},
	journal = {Forensic Science International},
	author = {Biedermann, A. and Taroni, F. and Bozza, S.},
	month = dec,
	year = {2009},
	pmid = {19833464},
	note = {Publisher: Elsevier},
	keywords = {Bayesian networks, Bayesian parameter estimation, Statistical learning methods},
	pages = {63--71},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\RBM7FWZ8\\Biedermann et al 2009.pdf:application/pdf},
}

@article{Biedermanni2011,
	title = {Implementing statistical learning methods through {Bayesian} networks ({Part} 2): {Bayesian} evaluations for results of black toner analyses in forensic document examination},
	volume = {204},
	issn = {0379-0738},
	doi = {10.1016/J.FORSCIINT.2010.05.001},
	abstract = {This paper presents and discusses the use of Bayesian procedures - introduced through the use of Bayesian networks in Part I of this series of papers - for 'learning' probabilities from data. The discussion will relate to a set of real data on characteristics of black toners commonly used in printing and copying devices. Particular attention is drawn to the incorporation of the proposed procedures as an integral part in probabilistic inference schemes (notably in the form of Bayesian networks) that are intended to address uncertainties related to particular propositions of interest (e.g., whether or not a sample originates from a particular source). The conceptual tenets of the proposed methodologies are presented along with aspects of their practical implementation using currently available Bayesian network software. © 2010 Elsevier Ireland Ltd.},
	number = {1-3},
	urldate = {2024-07-17},
	journal = {Forensic Science International},
	author = {Biedermann, A. and Taroni, F. and Bozza, S. and Mazzella, W. D.},
	month = jan,
	year = {2011},
	note = {Publisher: Elsevier},
	keywords = {Bayesian networks, Bayesian parameter estimation, Statistical learning methods, Black toner, Forensic document examination},
	pages = {58--66},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\6L3D2I6A\\Biedermann et al 2011.pdf:application/pdf},
}

@article{vanOorschot2019,
	title = {{DNA} transfer in forensic science: {A} review},
	volume = {38},
	issn = {1872-4973},
	doi = {10.1016/J.FSIGEN.2018.10.014},
	abstract = {Understanding the variables impacting DNA transfer, persistence, prevalence and recovery (DNA-TPPR) has become increasingly relevant in investigations of criminal activities to provide opinion on how the DNA of a person of interest became present within the sample collected. This review considers our current knowledge regarding DNA-TPPR to assist casework investigations of criminal activities. There is a growing amount of information available on DNA-TPPR to inform the relative probabilities of the evidence given alternative scenarios relating to the presence or absence of DNA from a specific person in a collected sample of interest. This information should be used where relevant. However, far more research is still required to better understand the variables impacting DNA-TPPR and to generate more accurate probability estimates of generating particular types of profiles in more casework relevant situations. This review explores means of achieving this. It also notes the need for all those interacting with an item of interest to have an awareness of DNA transfer possibilities post criminal activity, to limit the risk of contamination or loss of DNA. Appropriately trained forensic practitioners are best placed to provide opinion and guidance on the interpretation of profiles at the activity level. However, those requested to provide expert opinion on DNA-related activity level issues are often insufficiently trained to do so. We advocate recognition of DNA activity associated expertise to be distinct from expertise associated with the identification of individuals. This is to be supported by dedicated training, competency testing, authorisation, and regular fit for purpose proficiency testing. The possibilities for experts to report on activity-related issues will increase as our knowledge increases through further research, access to relevant data is enhanced, and tools to assist interpretations are better exploited. Improvement opportunities will be achieved sooner, if more laboratories and agencies accept the need to invest in these aspects as well as the training of practitioners.},
	urldate = {2024-07-17},
	journal = {Forensic Science International: Genetics},
	author = {van Oorschot, Roland A.H. and Szkuta, Bianca and Meakin, Georgina E. and Kokshoorn, Bas and Goray, Mariya},
	month = jan,
	year = {2019},
	pmid = {30399535},
	note = {Publisher: Elsevier},
	keywords = {DNA, Activity level, Transfer, Trace, Persistence, Prevalence, Recovery},
	pages = {140--166},
	file = {Accepted Version:C\:\\Users\\jstacey\\Zotero\\storage\\5DFLZLPC\\van Oorschot et al. - 2019 - DNA transfer in forensic science A review.pdf:application/pdf;PDF:C\:\\Users\\jstacey\\Zotero\\storage\\5XAY3E5B\\full-text.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\CLL25I5X\\S1872497318303958.html:text/html},
}

@article{TaylorKokshoornBiedermann2018,
	title = {Evaluation of forensic genetics findings given activity level propositions: {A} review},
	volume = {36},
	issn = {1872-4973},
	doi = {10.1016/J.FSIGEN.2018.06.001},
	abstract = {The evaluation of results of forensic genetic analyses given activity level propositions is an emerging discipline in forensic genetics. Although it is a topic with a long history, it has never been considered to be such a critically important topic for the field, as today. With the increasing sensitivity of analysis techniques, and advances in data interpretation using probabilistic models (‘probabilistic genotyping’), there is an increasing demand on forensic biologists to share specialised knowledge to help recipients of expert information address mode and timing of transfer and persistence of traces in court. Scientists thereby have a critical role in the assessment of their findings in the context of the case. This helps the judiciary with activity level inferences in a balanced, robust and transparent way, when based on (1) proper case assessment and interpretation respecting the hierarchy of propositions (supported by, for example, the use of Bayesian networks as graphical models), (2) use of appropriate data to inform probabilities, and (3) reporting guidelines by international bodies. This critical review of current literature shows that with certain prerequisites for training and quality assurance, there is a solid foundation for evidence interpretation when propositions of interest are at the ‘activity level’.},
	urldate = {2024-07-19},
	journal = {Forensic Science International: Genetics},
	author = {Taylor, Duncan and Kokshoorn, Bas and Biedermann, Alex},
	month = sep,
	year = {2018},
	pmid = {29929059},
	note = {Publisher: Elsevier},
	keywords = {DNA, Bayesian networks, Likelihood ratio, Activity level propositions, Evidence evaluation},
	pages = {34--49},
	file = {Taylor et al 2018:C\:\\Users\\jstacey\\Zotero\\storage\\VQ2C32KQ\\Taylor et al 2018.pdf:application/pdf},
}

@article{Derks2020,
	title = {A {Taxonomy} of {Explainable} {Bayesian} {Networks}},
	volume = {1342},
	issn = {1865-0937},
	url = {https://link.springer.com/chapter/10.1007/978-3-030-66151-9_14},
	doi = {10.1007/978-3-030-66151-9_14},
	abstract = {Artificial Intelligence (AI), and in particular, the explainability thereof, has gained phenomenal attention over the last few years. Whilst we usually do not question the decision-making process of these systems in situations where only the outcome is of interest,...},
	urldate = {2024-07-19},
	journal = {Communications in Computer and Information Science},
	author = {Derks, Iena Petronella and de Waal, Alta},
	year = {2020},
	note = {arXiv: 2101.11844
Publisher: Springer, Cham
ISBN: 978-3-030-66151-9},
	keywords = {Bayesian network, Explainability, Reasoning, Computer Science - Artificial Intelligence},
	pages = {220--235},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\KQ6SX543\\Derks and de Waal 2021.pdf:application/pdf},
}

@article{Evett2002,
	title = {Interpreting {Small} {Quantities} of {DNA}: the {Hierarchy} of {Propositions} and the {Use} of {Bayesian} {Networks}},
	volume = {47},
	issn = {0022-1198},
	url = {https://dx.doi.org/10.1520/JFS15291J},
	doi = {10.1520/JFS15291J},
	abstract = {The dramatic increase in the sensitivity of DNA profiling systems that has occurred over recent years has led to the need to address a wider range of interpretational problems in forensic science. The issues surrounding questions of the kind “whose DNA is this?” have been the subject of considerable controversy but now it is clear that the emphasis is shifting to questions of the kind “how did this DNA get here?” Such issues are discussed in this paper and new insights are provided by two particular recent developments. First, the notion of the “hierarchy of propositions” that has arisen from a project called Case Assessment and Interpretation (CAI) that has been running in the British Forensic Science Service (FSS). Second, a technique for drawing inferences in the face of many interacting considerations, known as “Bayesian networks”— or “Bayes' nets“ for short—that has been the subject of an earlier paper in this journal (1). The discussion is carried out by means of case studies, based on actual cases. It is clear that, whereas the inference in relation to the source of the DNA in a crime sample might be overwhelmingly strong, the inference in relation to the propositions that a jury must consider relating to the identity of the actual offender may be much more tentative.},
	number = {3},
	urldate = {2024-07-26},
	journal = {Journal of Forensic Sciences},
	author = {Evett, IW and Gill, PD and Jackson, G and Whitaker, J and Champod, C},
	month = may,
	year = {2002},
	pmid = {12051330},
	note = {Publisher: American Society of Mechanical Engineers Digital Collection},
	pages = {520--530},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\VX7B2S7Q\\Evett et al 2002.pdf:application/pdf},
}

@article{Cook1998,
	title = {A model for case assessment and interpretation},
	volume = {38},
	issn = {13550306},
	doi = {10.1016/S1355-0306(98)72099-4},
	abstract = {The authors describe a new approach to decision-making in an operational forensic science organization based on a model, embodying the principles of Bayesian inference, which has been developed through workshops run within the Forensic Science Service for forensic science practitioners. Issues which arise from the idea of pre-assessment of cases are explored by means of a case example.},
	number = {3},
	urldate = {2024-07-26},
	journal = {Science and Justice - Journal of the Forensic Science Society},
	author = {Cook, R. and Evett, I. W. and Jackson, G. and Jones, P. J. and Lambert, J. A.},
	year = {1998},
	pmid = {9800430},
	note = {Publisher: Forensic Science Society},
	pages = {151--156},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\JXL7V58Y\\Cook and Evett 1998.pdf:application/pdf},
}

@article{vuille_measuring_2021,
	title = {Measuring {Uncertainty} in {Forensic} {Science}},
	volume = {24},
	issn = {19410123},
	doi = {10.1109/MIM.2021.9345602},
	number = {1},
	urldate = {2024-07-31},
	journal = {IEEE Instrumentation and Measurement Magazine},
	author = {Vuille, Joelle and Taroni, Franco},
	month = feb,
	year = {2021},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	pages = {5--9},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\33S6KMNV\\full-text.pdf:application/pdf},
}

@article{Jackson2019,
	title = {“{Source}” or “activity” {What} is the level of issue in a criminal trial?},
	volume = {16},
	issn = {17409713},
	doi = {10.1111/J.1740-9713.2019.01253.X},
	number = {2},
	urldate = {2024-08-07},
	journal = {Significance},
	author = {Jackson, Graham and Biedermann, Alex},
	month = apr,
	year = {2019},
	note = {Publisher: Blackwell Publishing Ltd},
	pages = {36--39},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\4PDFR5WL\\full-text.pdf:application/pdf},
}

@article{Yangetal2022,
	title = {American forensic {DNA} practitioners' opinion on activity level evaluative reporting},
	volume = {67},
	issn = {1556-4029},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1111/1556-4029.15063},
	doi = {10.1111/1556-4029.15063},
	abstract = {The technical advancements made in DNA profiling now allow for very low DNA amounts to be analyzed. Accordingly, the argument often made in criminal courts is not who the DNA belongs to but rather how it was deposited. Despite the complexity of the relevant DNA transfer, persistence, prevalence, and recovery issues, forensic laboratories in some European countries have used evaluative reports with activity level propositions, while this is not current practice in the United States. The purpose of this study was to gain an overview of the opinions about activity level reporting (ALR) held by forensic biologists in the United States. A seventeen-question survey was distributed to members of the American Society of Crime Laboratory Directors and U.S. members of the International Society for Forensic Genetics. The survey included multiple-choice and open-response questions and received 54 responses. The majority of responses expressed moderate support of ALR. Participants mentioned six major concerns to be addressed prior to implementing ALR in the United States: (1) effect of number of variables involved; (2) need of education for practitioners/legal system; (3) inadequate number of activity studies with realistic scenarios; (4) difficulty of achieving admissibility in court; (5) need for standardized approaches/guidelines; and (6) requisite shift in perspective as to the validity of ALR. Overall, this small segment of U.S. forensic DNA practitioners appear to be willing to implement ALR once these concerns are fully addressed and resolved. As a follow-up, it would be worthwhile exploring these and other questions with a larger group and also other disciplines.},
	number = {4},
	urldate = {2024-08-07},
	journal = {Journal of Forensic Sciences},
	author = {Yang, Yoon Jung and Prinz, Mechthild and McKiernan, Heather and Oldoni, Fabio},
	month = jul,
	year = {2022},
	pmid = {35568965},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {DNA transfer, activity level propositions, American forensic DNA practitioners, evaluative reporting, forensic science, survey},
	pages = {1357--1369},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\SJCRA2YQ\\full-text.pdf:application/pdf},
}

@article{Cahilletal2024,
	title = {Where did it go? {A} study of {DNA} transfer in a social setting},
	volume = {73},
	issn = {1872-4973},
	url = {http://www.fsigenetics.com/article/S1872497324000978/fulltext},
	doi = {10.1016/J.FSIGEN.2024.103101},
	abstract = {{\textless}h2{\textgreater}Abstract{\textless}/h2{\textgreater}{\textless}p{\textgreater}The sensitivity of DNA analysis has progressed to the point that trace levels of DNA, originating from only a few cells, can generate informative profiles. This means that virtually any item or surface can be sampled with a reasonable chance of obtaining a DNA profile. As the presence of DNA does not suggest how it was deposited, questions are often raised as to how the DNA came to be at a particular location and the activity that led to its deposition. Therefore, understanding different modes of DNA deposition, reflective of realistic forensic casework situations, is critical for proper evaluation of DNA results in court. This study aimed to follow the movements of DNA to and from individuals and common household surfaces in a residential premises, while socially interacting. This took place over an hour and involved four participants, with known shedder status, designated as visitors (a male and a female) and hosts (a male and a female), who engaged in the activity of playing a board game while being served food. During the study, the participants were instructed to use the toilet on a single occasion to assess the transfer of DNA to new and unused underwear that was provided. All contacts made by the participants in the dining room and kitchen were video recorded to follow the movements of DNA. Samples were collected based on the history of contact, which included hands, fingernails and penile swabs. Direct contacts resulted in detectable transfer (LR {\textgreater} 1) in 87 \% (87/100) of the non-intimate samples and clothing. For surfaces touched by multiple participants, DNA from the person who made the last contact was not always detectable. The duration and number of contacts did not significantly affect the detection of the person contacting the item. On the other hand, presence of background DNA and participant's shedder status appear to play an important role. Further, unknown contributors were detected in the majority of samples. Finally, indirect transfer was observed on a number of occasions including co-habiting partners of guests who were not present at the study location. The results of this study may assist with decision making for exhibit selection or targeting areas for sampling within the home environment. Our findings can also be used in conjunction with previous literature to develop activity-level evaluations in such situations where the source of the DNA is conceded, but the mode of deposition is disputed.{\textless}/p{\textgreater}},
	urldate = {2024-08-07},
	journal = {Forensic Science International: Genetics},
	author = {Cahill, Amy and Volgin, Luke and Oorschot, Roland A.H. van and Taylor, Duncan and Goray, Mariya},
	month = nov,
	year = {2024},
	pmid = {39096604},
	note = {Publisher: Elsevier},
	keywords = {DNA transfer, Persistence, Prevalence, Activity level evaluations, Shedder, Social interactions, Video analysis},
	pages = {103101},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\73ASD7KE\\full-text.pdf:application/pdf},
}

@article{Prinzetal2024,
	title = {Global survey on evaluative reporting on {DNA} evidence with regard to activity-level propositions},
	volume = {69},
	issn = {1556-4029},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1111/1556-4029.15488},
	doi = {10.1111/1556-4029.15488},
	abstract = {For many criminal cases, the source of who deposited the DNA is not what the prosecutor and the defense are trying to dispute. In court, the question may be how the DNA was deposited at the crime scene rather than who the DNA came from. Although laboratories in many countries have begun to evaluate DNA evidence given formal activity-level propositions (ALPs), it is unknown how much other forensic practitioners know and what they think about activity-level evaluative reporting (ALR). To collect this information, a survey with 21 questions was submitted to international forensic science organizations across Europe, Australia, South America, Canada, Asia, and Africa. The survey combined open-ended and multiple-choice questions and received 162 responses. Responses revealed a wide range of knowledge on the topic. Overall, most respondents were somewhat knowledgeable about ALR, ALP, and current practices in court and expressed their support of the concept. A majority of participants identified gaps and obstacles regarding ALR they would like to see addressed. Examples include (1) need for more education/training at all stakeholder levels, (2) need for more DNA evidence-related data under realistic case scenarios, (3) need to internally implement and validate a formalized and objective approach for reporting, and (4) in some countries the need to achieve court admissibility. This global survey gathered the current concerns of forensic DNA practitioners and outlined several operational concerns. The information can be used to advance the implementation of ALR in laboratories and court testimony worldwide.},
	number = {3},
	urldate = {2024-08-07},
	journal = {Journal of Forensic Sciences},
	author = {Prinz, Mechthild and Pirtle, Devyn and Oldoni, Fabio},
	month = may,
	year = {2024},
	pmid = {38351537},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {DNA evidence, activity, expert testimony, forensic practitioners, global survey, level evaluative reporting, level propositions},
	pages = {798--813},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\TWIBT4R3\\full-text.pdf:application/pdf},
}

@article{ENFSI2016,
	title = {{ENFSI} {Guideline} for {Evaluative} {Reporting} in {Forensic} {Science}, {A} {Primer} for {Legal} {Practitioners}},
	volume = {180},
	abstract = {The ENFSI (European Network of Forensic Science Institutes, http://www.enfsi.eu/) is a key organisation in Europe bringing together more than 60 laboratories with a vision to share common quality standards and exchange knowledge and expertise. Twenty years after its foundation, ENFSI is now a pre-eminent voice on forensic science in Europe with privileged relationships with institutions such as the European Commission (with the privileged status of an EU-monopolist), Europol, CEPOL, Eurojust and Interpol. A diversity of approaches in the assessment of the scientific findings Compare these two statements taken from practice: (1) The matching glass fragments recovered on the garment of Mr S. are consistent with coming from the broken window. (2) The matching glass fragments recovered on the garment of Mr S. are more likely to be found if Mr S. broke the window rather than if he had nothing to do with the incident. The first statement only considers the results given one version of the events and thus is unbalanced and potentially misleading. The second statement offers a relative assessment of the findings given two stated scenarios. Such diversity is easy to observe among the members of the European Network of Forensic Science Institutes (ENFSI). The ENFSI working groups deal with areas of practice such as firearms, drugs, biological traces (DNA), marks (fingerprints, footwear), documents, microtraces (glass, fibres, hair, soils, plants), explosives, audio (voice), video (face, gait) and digital information, crime scene investigation, road accidents and fire investigation. These are the areas targeted by the ENFSI Guideline. This diversity has adverse consequences. First, the lack of declared common criteria-and adherence thereto-for assessing and reporting scientific findings offers no guarantee that the scientific findings are assessed consistently and within a logical framework. For example, the same findings could lead practitioners to express conclusions that convey drastically different messages: from an undefined and prone to misinterpretation "match and consistent with" to a transparent disclosure of the weight to be attached to the findings. Ambiguous wording of scientific conclusions has been recognised as a factor leading to miscarriages of justice.},
	urldate = {2024-08-07},
	journal = {Criminal Law {\textbackslash}\& Justice Weekly},
	author = {Champod, C and Biedermann, A and Vuille, J and Willis, S and Kinder, De},
	year = {2016},
	pages = {189--193},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\BR63QUXY\\m-api-1111606d-312e-c8ed-03bc-2e606f8aa541.pdf:application/pdf},
}

@article{Crispinoetal2022,
	title = {Towards another paradigm for forensic science?},
	volume = {4},
	issn = {2573-9468},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1002/wfs2.1441},
	doi = {10.1002/WFS2.1441},
	abstract = {Daubert skews the contribution of forensic science because it only took into account its Galilean dimension (construction of general predictive models). However, forensic science should better be classified in the historical sciences (clinical approach to reconstruct a past event of presence or activity). We therefore need a complementary approach that integrates the necessarily “clinical” part in the resolution of forensic issues. Such an evolution involves semiotics. While recognizing that the Bayesian way of thinking is the only prescriptive available model for interpretation fitting well in the Galilean paradigm, the complexity of the reconstruction of a past‐uncontrolled singular case and the robustness of available relevant data to it, invites consideration of its implementation in a semiotic line of arguments. Indeed, Bayes makes it possible to remain in a single harmonized model integrating both the clinical and Galilean dimensions, but rapidly the complexity of the modeling and its mathematization come up against more qualitative natural and legal reasoning. Two different systems of reasoning at stake are inevitably creating a “bug” that could explain the current forensic crisis and miscarriages of justice. This anomaly is reflected in the issue of transparency (misunderstandings by and between interlocutors on the nature of the expertise, if not science). Peirce offers a path to address the tension between complementary reasoning systems. This article is categorized under:   Crime Scene Investigation {\textgreater} Epistemology and Method   Crime Scene Investigation {\textgreater} From Traces to Intelligence and Evidence   Crime Scene Investigation {\textgreater} Education and Formation},
	number = {3},
	urldate = {2024-08-07},
	journal = {Wiley Interdisciplinary Reviews: Forensic Science},
	author = {Crispino, Frank and Weyermann, Céline and Delémont, Olivier and Roux, Claude and Ribaux, Olivier},
	month = may,
	year = {2022},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {inference, interpretation, semiotics, signification, trace},
	pages = {e1441},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\E74I2QVR\\Crispino et al 2021.pdf:application/pdf},
}

@article{Shengelia2021,
	title = {Are {Jurors} {Intuitive} {Statisticians}? {Bayesian} {Causal} {Reasoning} in {Legal} {Contexts}},
	volume = {11},
	issn = {16641078},
	url = {www.frontiersin.org},
	doi = {10.3389/FPSYG.2020.519262/BIBTEX},
	abstract = {In criminal trials, evidence often involves a degree of uncertainty and decision-making includes moving from the initial presumption of innocence to inference about guilt based on that evidence. The jurors’ ability to combine evidence and make accurate intuitive probabilistic judgments underpins this process. Previous research has shown that errors in probabilistic reasoning can be explained by a misalignment of the evidence presented with the intuitive causal models that people construct. This has been explored in abstract and context-free situations. However, less is known about how people interpret evidence in context-rich situations such as legal cases. The present study examined participants’ intuitive probabilistic reasoning in legal contexts and assessed how people’s causal models underlie the process of belief updating in the light of new evidence. The study assessed whether participants update beliefs in line with Bayesian norms and if errors in belief updating can be explained by the causal structures underpinning the evidence integration process. The study was based on a recent case in England where a couple was accused of intentionally harming their baby but was eventually exonerated because the child’s symptoms were found to be caused by a rare blood disorder. Participants were presented with a range of evidence, one piece at a time, including physical evidence and reports from experts. Participants made probability judgments about the abuse and disorder as causes of the child’s symptoms. Subjective probability judgments were compared against Bayesian norms. The causal models constructed by participants were also elicited. Results showed that overall participants revised their beliefs appropriately in the right direction based on evidence. However, this revision was done without exact Bayesian computation and errors were observed in estimating the weight of evidence. Errors in probabilistic judgments were partly accounted for, by differences in the causal models representing the evidence. Our findings suggest that understanding causal models that guide people’s judgments may help shed light on errors made in evidence integration and potentially identify ways to address accuracy in judgment.},
	urldate = {2024-08-23},
	journal = {Frontiers in Psychology},
	author = {Shengelia, Tamara and Lagnado, David},
	month = feb,
	year = {2021},
	note = {Publisher: Frontiers Media S.A.},
	keywords = {Bayesian reasoning, causal Bayes nets, causal inferences, explaining away, intuitive judgment, jury decision making, probabilistic reasoning, zero-sum},
	pages = {519262},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\M2WUXUV5\\full-text.pdf:application/pdf},
}

@article{zacher_transfer_2024,
	title = {Transfer and persistence of intruder {DNA} within an office after reuse by owner},
	volume = {73},
	issn = {1872-4973},
	doi = {10.1016/J.FSIGEN.2024.103130},
	abstract = {The heightened sensitivity of DNA typing techniques, paired with the extensive use of trace DNA in forensic investigations, has resulted in an increased need to understand how and when DNA is deposited on surfaces of interest. This study focussed on the transfer, persistence, and prevalence of trace DNA in a single occupation of an office space by an intruder, when all contacts made during occupation and for the two hours prior and post occupation were known. The extent to which DNA could be recovered from contacted/not contacted surfaces was investigated. This study investigates the impacts of these movements and use of an office space when the duration of occupancy, surface contact histories and shedder status of participants are known. Contacts were documented and surfaces in the office space were targeted for sampling. Categories were set for target sampling that included different types of contact. Direct and indirect DNA transfer was detected in 55 \% and 6 \% of samples, respectively. Contactless DNA transfer was detected in 0.5 \% of samples. The owner was observed as the sole/major/majority contributor in 77 \% of the samples and as minor contributor in 10 \% of samples. The intruder was observed as the sole/major/majority contributor in 14 \% of samples and as the minor contributor in 16 \%. An increased number of contacts increased the relative DNA contribution of the individual making the contact, however, not all observed direct contacts resulted in detectable DNA transfer. The outcome of this study will aid in better sample targeting strategies and contribute to the pool of data assisting in the development of activity level assessments.},
	urldate = {2024-09-24},
	journal = {Forensic Science International: Genetics},
	author = {Zacher, Monique and van Oorschot, Roland A.H. and Handt, Oliva and Goray, Mariya},
	month = nov,
	year = {2024},
	note = {Publisher: Elsevier},
	keywords = {DNA persistence, DNA transfer, Trace DNA, Forensic investigation},
	pages = {103130},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\PV8FTBMW\\full-text.pdf:application/pdf},
}

@article{taylor_importance_2021,
	title = {The importance of considering common sources of unknown {DNA} when evaluating findings given activity level propositions},
	volume = {53},
	issn = {1872-4973},
	doi = {10.1016/J.FSIGEN.2021.102518},
	abstract = {Evaluating forensic biological evidence considering activity level propositions is becoming more prominent around the world. In such evaluations it is common to combine results from multiple items associated with the alleged activities. The results from these items may not be conditionally independent, depending on the mechanism of cell/DNA transfer being considered and it is important that the evaluation takes these dependencies into account. Part of this consideration is to incorporate our understanding of prevalent DNA and of background DNA on objects and people, and how activities can lead to common sources of unknown DNA being deposited on items. We demonstrate a framework for evaluation of DNA evidence in such a scenario using Object-Oriented Bayesian Networks and apply it to a motivating case from South Australia.},
	urldate = {2024-10-04},
	journal = {Forensic Science International: Genetics},
	author = {Taylor, Duncan and Volgin, Luke and Kokshoorn, Bas and Champod, Christophe},
	month = jul,
	year = {2021},
	pmid = {33865097},
	note = {Publisher: Elsevier},
	keywords = {Bayesian networks, Activity level, Evidence evaluation, Background DNA},
	pages = {102518},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\CPT67L2H\\full-text.pdf:application/pdf},
}

@article{meakin_deposition_2015,
	title = {The deposition and persistence of indirectly-transferred {DNA} on regularly-used knives},
	volume = {5},
	issn = {1875-1768},
	doi = {10.1016/J.FSIGSS.2015.09.197},
	abstract = {Considerations concerning how DNA recovered from a crime scene was deposited are of increasing significance to forensic casework. While the possibility of indirect DNA transfer is well established, research into such transfer is limited and focused mainly on the handling of DNA-free items. This study investigated whether secondarily-transferred DNA can be detected on regularly-used items, and if so, for how long might it persist. Volunteers each used a set of knives regularly over a period of two days, after which, each of these ‘regular users’ shook hands with another person (‘handshaker’) and then immediately, without touching anything else, repeatedly stabbed one of their own regularly-used knives into foam for 60 s. DNA was recovered from the knife handles using mini-tapes approximately one hour, one day, and one week after the stabbings. In three of the four pairings of volunteers, complete and partial DNA profiles matching those of the regular user and handshaker respectively, at ratios of ∼10:1, were recovered from the knives within one hour. Alleles attributed to the handshaker were still detected after one week, but were significantly reduced in number and peak height for two of the three pairings. Unknown alleles were also recovered from the knives, suggesting other indirect DNA transfer events. These included repeated detection of alleles attributed to the DNA profile of a volunteer's partner. For the fourth pairing, only complete single-source DNA profiles matching the regular user's profile were recovered. This study demonstrates that, on regularly-used items, secondarily-transferred DNA can be detected and can persist for at least a week; this has implications for forensic reconstructions.},
	urldate = {2024-10-04},
	journal = {Forensic Science International: Genetics Supplement Series},
	author = {Meakin, Georgina E. and Butcher, Emma V. and van Oorschot, Roland A.H. and Morgan, Ruth M.},
	month = dec,
	year = {2015},
	note = {Publisher: Elsevier},
	keywords = {DNA persistence, DNA transfer, Trace DNA},
	pages = {e498--e500},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\F6PIMJ82\\full-text.pdf:application/pdf},
}

@article{monkman_presence_2023,
	title = {Presence of {Human} {DNA} on {Household} {Dogs} and {Its} {Bi}-{Directional} {Transfer}},
	volume = {14},
	issn = {2073-4425},
	doi = {10.3390/genes14071486},
	abstract = {Awareness of the factors surrounding the transfer of DNA from a person, item, or surface to another person, item, or surface is highly relevant during investigations of alleged criminal activity. Animals in domestic environments could be a victim, offender, or innocent party associated with a crime. There is, however, very limited knowledge of human DNA transfer, persistence, prevalence, and recovery (DNA TPPR) associated with domestic animals. This pilot study aimed to improve our understanding of DNA TPPR associated with domestic dogs by collecting and analysing samples from various external areas of dogs of various breeds, interactions with humans, and living arrangements, and conducting a series of tests to investigate the possibility of dogs being vectors for the indirect transfer of human DNA. Reference DNA profiles from the dog owners and others living in the same residence were acquired to assist interpretation of the findings. The findings show that human DNA is prevalent on dogs, and in the majority of samples, two-person mixtures are present. Dogs were also found to be vectors for the transfer of human DNA, with DNA transferred from the dog to a gloved hand during patting and a sheet while walking.},
	language = {eng},
	number = {7},
	journal = {Genes},
	author = {Monkman, Heidi and Szkuta, Bianca and van Oorschot, Roland A. H.},
	month = jul,
	year = {2023},
	pmid = {37510390},
	pmcid = {PMC10379355},
	keywords = {DNA, DNA transfer, DNA prevalence, DNA recovery, forensic, Animals, Animals, Domestic, DNA Fingerprinting, dogs, Dogs, Family Characteristics, human DNA, Humans, Pilot Projects},
	pages = {1486},
	file = {Full Text:C\:\\Users\\jstacey\\Zotero\\storage\\IRJH93CP\\Monkman et al. - 2023 - Presence of Human DNA on Household Dogs and Its Bi-Directional Transfer.pdf:application/pdf},
}

@article{hughes_how_2023,
	title = {How changes to the substrate's physical characteristics can influence the deposition of touch and salivary deposits},
	volume = {343},
	issn = {1872-6283},
	doi = {10.1016/j.forsciint.2022.111546},
	abstract = {An in-depth study into the physical substrate characteristics such as substrate surface roughness, topography, and physicochemical characteristics like wettability and surface free energy (SFE) was conducted to investigate the impact on the deposition and adherence of touch and salivary deposits on aluminium and polypropylene. A robust protocol was established to generate a set of substrates with a controlled linear surface roughness range (0.5-3.5 µm) in order to identify the impact of surface roughness on DNA transfer, persistence, prevalence, and recovery (DNA-TPPR). The polypropylene substrate was shown to produce fibres when artificially roughened, becoming more prominent at a higher surface roughness range, and has shown to have a direct impact on the distribution of salivary and touch deposits. At the low to moderate surface roughness range 0.5-2.0 µm, salivary and touch deposits have generally shown to follow the topographical features of the substrate they were deposited on, before a plateau of the surface roughness measure on the deposit was observed, indicating that a saturation point was reached and the grooves in the substrate were beginning to fill. Touch deposits have shown to maintain a consistent deposition height pre-surface roughness threshold, irrespective of substrate surface roughness while the deposition height of salivary deposits was heavily influenced by substrate surface roughness and topography. The substrate SFE, wettability, hydrophobicity, and the surface tension of the deposit was shown to drive the adhesion properties of the saliva and touch deposits on the respective substrates, and it was observed that this may be of importance for the improvement of the current DNA-TPPR understanding, DNA sampling protocols, and DNA transfer considerations within casework.},
	language = {eng},
	journal = {Forensic Science International},
	author = {Hughes, Deborah A. and Szkuta, Bianca and Oorschot, Roland A. H. van and Conlan, Xavier A.},
	month = feb,
	year = {2023},
	pmid = {36621057},
	keywords = {DNA, Trace DNA, DNA-TPPR, Non-porous substrates, Physicochemical variables, Polypropylenes, Salivary deposit, Surface Properties, Surface Tension, Touch, Touch deposit, Wettability},
	pages = {111546},
}

@article{sessa_indirect_2023,
	title = {Indirect {DNA} {Transfer} and {Forensic} {Implications}: {A} {Literature} {Review}},
	volume = {14},
	issn = {2073-4425},
	shorttitle = {Indirect {DNA} {Transfer} and {Forensic} {Implications}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10742555/},
	doi = {10.3390/genes14122153},
	abstract = {Progress in DNA profiling techniques has made it possible to detect even the minimum amount of DNA at a crime scene (i.e., a complete DNA profile can be produced using as little as 100 pg of DNA, equivalent to only 15–20 human cells), leading to new defense strategies. While the evidence of a DNA trace is seldom challenged in court by a defendant’s legal team, concerns are often raised about how the DNA was transferred to the location of the crime. This review aims to provide an up-to-date overview of the experimental work carried out focusing on indirect DNA transfer, analyzing each selected paper, the experimental method, the sampling technique, the extraction protocol, and the main results. Scopus and Web of Science databases were used as the search engines, including 49 papers. Based on the results of this review, one of the factors that influence secondary transfer is the amount of DNA shed by different individuals. Another factor is the type and duration of contact between individuals or objects (generally, more intimate or prolonged contact results in more DNA transfer). A third factor is the nature and quality of the DNA source. However, there are exceptions and variations depending on individual characteristics and environmental conditions. Considering that secondary transfer depends on multiple factors that interact with each other in unpredictable ways, it should be considered a complex and dynamic phenomenon that can affect forensic investigation in various ways, for example, placing a subject at a crime scene who has never been there. Correct methods and protocols are required to detect and prevent secondary transfer from compromising forensic evidence, as well as the correct interpretation through Bayesian networks. In this context, the definition of well-designed experimental studies combined with the use of new forensic techniques could improve our knowledge in this challenging field, reinforcing the value of DNA evidence in criminal trials.},
	number = {12},
	urldate = {2024-10-07},
	journal = {Genes},
	author = {Sessa, Francesco and Pomara, Cristoforo and Esposito, Massimiliano and Grassi, Patrizia and Cocimano, Giuseppe and Salerno, Monica},
	month = nov,
	year = {2023},
	pmid = {38136975},
	pmcid = {PMC10742555},
	pages = {2153},
	file = {PubMed Central Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\Q652TXKQ\\Sessa et al. - 2023 - Indirect DNA Transfer and Forensic Implications A Literature Review.pdf:application/pdf},
}

@book{taroni_bayesian_2006,
	address = {West Sussex, England},
	series = {statistics in {Practice}},
	title = {Bayesian {Networks} and {Probabilistic} {Inference} in {Forensic} {Science}},
	isbn = {978-0-470-09173-9},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Taroni, Franco and Aitken, Colin and Garbolino, Paolo and Biedermann, Alex},
	year = {2006},
}

@misc{thomas_regina_2009,
	title = {Regina vs  {Reed} and {Anor}},
	url = {https://www.bailii.org/ew/cases/EWCA/Crim/2009/2698.html},
	urldate = {2024-11-13},
	author = {Thomas, Lord Justice and Kitchin, Mr Justice and Holroyde, Mr Justice},
	month = dec,
	year = {2009},
}

@misc{hughes_regina_2009,
	title = {Regina vs {Atkins} and {Anor}},
	url = {https://www.bailii.org/ew/cases/EWCA/Crim/2009/1876.html},
	urldate = {2024-11-13},
	author = {Hughes, Lord Justice and Rafferty, Mrs Justice and Slade, Mrs Justice},
	month = oct,
	year = {2009},
}

@incollection{kotz_chapter_2019,
	title = {Chapter 49: {Dirichlet} and {Inverted} {Dirichlet} {Distributions}},
	volume = {334},
	booktitle = {Continuous multivariate distributions, {Volume} 1: {Models} and applications},
	publisher = {John Wiley \& Sons},
	author = {Kotz, Samuel and Balakrishnan, Narayanaswamy and Johnson, Norman L},
	year = {2019},
}

@article{vink_collection_2023,
	title = {A collection of idioms for modeling activity level evaluations in forensic science},
	volume = {6},
	issn = {2589-871X},
	url = {https://www.sciencedirect.com/science/article/pii/S2589871X23000189},
	doi = {10.1016/j.fsisyn.2023.100331},
	abstract = {This paper presents a collection of idioms that is useful for modeling activity level evaluations in forensic science using Bayesian networks. The idioms are categorized into five groups: cause-consequence idioms, narrative idioms, synthesis idioms, hypothesis-conditioning idioms, and evidence-conditioning idioms. Each category represents a specific modeling objective. Furthermore, we support the use of an idiom-based approach and emphasize the relevance of our collection by combining several of the presented idioms to create a more comprehensive template model. This model can be used in cases involving transfer evidence and disputes over the actor and/or activity. Additionally, we cite literature that employs idioms in template models or case-specific models, providing the reader with examples of their use in forensic casework.},
	urldate = {2024-11-25},
	journal = {Forensic Science International: Synergy},
	author = {Vink, M. and Sjerps, M. J.},
	month = jan,
	year = {2023},
	keywords = {Evidence, Bayesian networks, Activity level, Forensic science, Probability, Idioms},
	pages = {100331},
	file = {PubMed Central Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\KT556JGT\\Vink and Sjerps - 2023 - A collection of idioms for modeling activity level evaluations in forensic science.pdf:application/pdf},
}

@article{voigt_re-imagen_2024,
	series = {{DFRWS} {APAC} 2024 - {Selected} {Papers} from the 4th {Annual} {Digital} {Forensics} {Research} {Conference} {APAC}},
	title = {Re-imagen: {Generating} coherent background activity in synthetic scenario-based forensic datasets using large language models},
	volume = {50},
	issn = {2666-2817},
	shorttitle = {Re-imagen},
	url = {https://www.sciencedirect.com/science/article/pii/S266628172400129X},
	doi = {10.1016/j.fsidi.2024.301805},
	abstract = {Due to legal and privacy-related restrictions, the generation of synthetic data is recommended for creating datasets for digital forensic education and training. One challenge when synthesizing scenario-based forensic data is the creation of coherent background activity besides evidential actions. This work leverages the creative writing abilities of large language models (LLMs) to generate personas and actions that describe the background usage of a device consistent with the created persona. These actions are subsequently converted into a machine-readable format and executed on a virtualized device using VM control automation. We introduce Re-imagen, a framework that combines state-of-the-art LLMs and a recent unintrusive GUI automation tool to produce synthetic disk images that contain arguably coherent “wear-and-tear” artifacts that current synthesis platforms lack. While, for now, the focus is on the coherence of the generated background activity, we believe that the proposed approach is a step toward more realistic synthetic disk image generation.},
	urldate = {2024-11-25},
	journal = {Forensic Science International: Digital Investigation},
	author = {Voigt, Lena L. and Freiling, Felix and Hargreaves, Christopher J.},
	month = oct,
	year = {2024},
	keywords = {ChatGPT, Data synthesis, Digital forensic education, Forensic datasets, Forensic disk image generation, Large language models (LLM), User simulation},
	pages = {301805},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\PWJF8HYG\\Voigt et al. - 2024 - Re-imagen Generating coherent background activity in synthetic scenario-based forensic datasets usi.pdf:application/pdf},
}

@article{van_damme_study_2024,
	title = {A study into the natural occurrence of inorganic ions relevant to forensic explosives investigations on human hands},
	volume = {361},
	issn = {0379-0738},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073824002007},
	doi = {10.1016/j.forsciint.2024.112119},
	abstract = {The natural occurrence of 16 inorganic ions relevant to forensic explosives investigations on human hands was studied to support the evaluation of activity-level propositions when such traces are found on the hands or in the fingerprints of a suspect. A total of 594 hand swab extracts from 297 participants throughout Europe and the United States of America were analyzed using Ion Chromatography – Mass Spectrometry. The data provides a reference framework for future covert investigations and forensic casework. The results indicate that thiocyanate, chlorate, nitrite, lithium, strontium, and barium are rarely detected on the hands of individuals who have had no direct contact with explosives (P{\textless}0.03) and in quantities below 6 µg. Perchlorate contamination sporadically occurs without deliberately handling perchlorates (P=0.03), albeit at low levels ({\textless}12 µg). It also seems that the presence of perchlorate on hands is generally related to professions that involve explosives. Detecting substantial amounts of any of these rare ions on a suspect’s hands would require a specific explanation. Because legitimate activities exist that can also result in elevated levels of ions of interest on hands, the context surrounding their presence has to be carefully assessed for each individual case.},
	urldate = {2024-11-25},
	journal = {Forensic Science International},
	author = {van Damme, I. M. and Hulsbergen, A. W. C. and Allers, S. and Bezemer, K. D. B. and Miller, J. V. and van Asten, A. C.},
	month = aug,
	year = {2024},
	keywords = {Contamination, Forensic investigation, Background levels, Explosives trace detection, Hand swab, Ion chromatography-mass spectrometry},
	pages = {112119},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\N7I4HMSU\\van Damme et al. - 2024 - A study into the natural occurrence of inorganic ions relevant to forensic explosives investigations.pdf:application/pdf},
}

@article{roersma_prevalence_2024,
	title = {Prevalence of male {DNA} on female worn undergarments. {How} small datasets may support robust opinions in activity level evaluative reporting},
	volume = {361},
	issn = {0379-0738},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073824001786},
	doi = {10.1016/j.forsciint.2024.112097},
	abstract = {In cases of sexual assault, the interpretation of biological traces on clothing, and particularly undergarments, may be complex. This is especially so when the complainant and defendant interact socially, for instance as (ex-)partners or by co-habitation. Here we present the results from a study where latent male DNA on female worn undergarments is recovered in four groups with different levels of male-female social interaction. The results conform to prior expectation, in that less interaction tend to result in less male DNA on undergarments. We explore the use of these experimental data for evaluative reporting given activity level propositions in a mock case scenario. We show how the selection of different populations to represent the social interaction between complainant and defendant may affect the strength of the evidence. We further show how datasets of limited size can be used for robust activity level evaluative reporting.},
	urldate = {2024-11-25},
	journal = {Forensic Science International},
	author = {Roersma, R. and Storm, J. and Joling, J. and Kokshoorn, B.},
	month = aug,
	year = {2024},
	keywords = {Sensitivity analysis, Activity level evaluative reporting, Brassieres, Latent DNA recovery, Pairs of underpants, Prevalence of body fluids, Sexual assault, Y STR},
	pages = {112097},
}

@article{taylor_practical_2024,
	title = {A practical treatment of sensitivity analyses in activity level evaluations},
	volume = {355},
	issn = {0379-0738},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073824000252},
	doi = {10.1016/j.forsciint.2024.111944},
	abstract = {Evaluations of forensic observations considering activity level propositions are becoming more common place in forensic institutions. A measure that can be taken to interrogate the evaluation for robustness is called sensitivity analysis. A sensitivity analysis explores the sensitivity of the evaluation to the data used when assigning probabilities, or to the level of uncertainty surrounding a probability assignment, or to the choice of various assumptions within the model. There have been a number of publications that describe sensitivity analysis in technical terms, and demonstrate their use, but limited literature on how that theory can be applied in practice. In this work we provide some simplified examples of how sensitivity analyses can be carried out, when they are likely to show that the evaluation is sensitive to underlying data, knowledge or assumptions, how to interpret the results of sensitivity analysis, and how the outcome can be reported. We also provide access to an application to conduct sensitivity analysis.},
	urldate = {2024-11-25},
	journal = {Forensic Science International},
	author = {Taylor, Duncan and Kokshoorn, Bas and Champod, Christophe},
	month = feb,
	year = {2024},
	keywords = {Activity level, Sensitivity analysis, Bayesian Network, Court reporting, DNA profile evaluation},
	pages = {111944},
}

@article{taylor_accounting_2024,
	title = {Accounting for site-to-site {DNA} transfer on a packaged exhibit in an evaluation given activity level propositions},
	volume = {73},
	issn = {1872-4973},
	url = {https://www.sciencedirect.com/science/article/pii/S1872497324001182},
	doi = {10.1016/j.fsigen.2024.103122},
	abstract = {Considering activity level propositions in the evaluation of forensic biology findings is becoming more common place. There are increasing numbers of publications demonstrating different transfer mechanisms that can occur under a variety of circumstances. Some of these publications have shown the possibility of DNA transfer from site to site on an exhibit, for instance as a result of packaging and transport. If such a possibility exists, and the case circumstances are such that the area on an exhibit where DNA is present or absent is an observation that is an important diagnostic characteristic given the propositions, then site to site transfer should be taken into account during the evaluation of observations. In this work we demonstrate the ways in which site to site transfer can be built into Bayesian networks when carrying out activity level evaluations of forensic biology findings. We explore the effects of considering qualitative vs quantitative categorisation of DNA results. We also show the importance of taking into account multiple individual’s DNA being transferred (such as unknown or wearer DNA), even if the main focus of the evaluation is the activity of one individual.},
	urldate = {2024-11-25},
	journal = {Forensic Science International: Genetics},
	author = {Taylor, Duncan and Volgin, Luke and Kokshoorn, Bas},
	month = nov,
	year = {2024},
	keywords = {DNA transfer, Bayesian Network, Activity level evaluation, Exhibit packaging and transport, contamination, Site to site transfer},
	pages = {103122},
}

@article{kokshoorn_reporting_2023,
	title = {Reporting on forensic biology findings given activity level issues in the {Netherlands}},
	volume = {343},
	issn = {0379-0738},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073822003759},
	doi = {10.1016/j.forsciint.2022.111545},
	abstract = {There appears to be some hesitation within the forensic biology community to formally evaluate and report on findings given activity level propositions. This hesitance in part stems from concerns about the lack of relevant data on the dynamics of biological traces and doubt about the relevance of such expert opinions to the trier of fact. At the Netherlands Forensic Institute formal evaluative opinions on the probability of case findings given propositions at the activity level are provided since 2013, if requested by a mandating authority. In this study we share the results from a retrospective analysis of 74 of such requests. We explore which party initiates requests, the types of cases that are submitted, the sources of data being used to assign probabilities to DNA transfer, persistence, prevalence and recovery (TPPR) events, the conclusions that were drawn by the scientists, and how the conclusions were used by the courts. This retrospective analysis of cases demonstrates that published sources of data are generally available and can be used to address DNA TPPR events in most cases, although significant gaps still remain. The study furthermore shows that reporting on forensic biology findings given activity level propositions has been generally accepted by the district and appeal courts, as well as the other parties in the criminal justice system in the Netherlands.},
	urldate = {2024-11-25},
	journal = {Forensic Science International},
	author = {Kokshoorn, Bas and Luijsterburg, Maartje},
	month = feb,
	year = {2023},
	keywords = {Activity level, DNA transfer, Reporting, Retrospective analysis of casework},
	pages = {111545},
	file = {Full Text:C\:\\Users\\jstacey\\Zotero\\storage\\GUIHQFXS\\Kokshoorn and Luijsterburg - 2023 - Reporting on forensic biology findings given activity level issues in the Netherlands.pdf:application/pdf;PDF:C\:\\Users\\jstacey\\Zotero\\storage\\2R39P3ID\\full-text.pdf:application/pdf},
}

@article{taroni_generalised_2021,
	title = {A generalised {Bayes}' factor formula for evidence evaluation under activity level propositions: {Variations} around a fibres scenario},
	volume = {322},
	issn = {0379-0738},
	shorttitle = {A generalised {Bayes}' factor formula for evidence evaluation under activity level propositions},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073821000700},
	doi = {10.1016/j.forsciint.2021.110750},
	abstract = {Generalised Bayes’ factors and associated Bayesian networks are developed for the transfer of extrinsic evidence at the activity level, developments that extend previous work on activity level evaluation. A strategy for the assessment of extrinsic evidence is developed in stages with progressive increases in complexity. The final development is illustrated with an example involving fibres from clothing. This provides a list of factors involved in the consideration of a transfer case with activity level propositions and their roles in the determination of evidential value.},
	urldate = {2024-11-25},
	journal = {Forensic Science International},
	author = {Taroni, Franco and Garbolino, Paolo and Aitken, Colin},
	month = may,
	year = {2021},
	keywords = {Activity level proposition, Bayes’ factor, Scientific evidence evaluation},
	pages = {110750},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\TJIM5NGS\\Taroni et al. - 2021 - A generalised Bayes' factor formula for evidence evaluation under activity level propositions Varia.pdf:application/pdf},
}

@article{de_ronde_using_2021,
	title = {Using case specific experiments to evaluate fingermarks on knives given activity level propositions},
	volume = {320},
	issn = {0379-0738},
	url = {https://www.sciencedirect.com/science/article/pii/S037907382100030X},
	doi = {10.1016/j.forsciint.2021.110710},
	abstract = {Bayesian networks have shown to be a useful tool for the evaluation of forensic findings given activity level propositions. In this paper, we demonstrate how case specific experiments can be used to assign probabilities to the states of the nodes of a Bayesian network for the evaluation of fingermarks given activity level propositions. The transfer, persistence and recovery of fingermarks on knives is studied in experiments where a knife is either used to stab a victim or to cut food, representing the activities that were disputed in the case of the murder of Meredith Kercher. Two Bayesian networks are constructed, exploring the effect of different uses of the experimental data by assigning the probabilities based on the results of the experiments. The evaluation of the findings using the Bayesian networks demonstrates the potential for fingermarks in addressing activity level propositions.},
	urldate = {2024-11-25},
	journal = {Forensic Science International},
	author = {de Ronde, Anouk and Kokshoorn, Bas and de Puit, Marcel and de Poot, Christianne J.},
	month = mar,
	year = {2021},
	keywords = {Bayesian networks, Activity level, Evidence interpretation, Fingermarks},
	pages = {110710},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\83GIIDHM\\de Ronde et al. - 2021 - Using case specific experiments to evaluate fingermarks on knives given activity level propositions.pdf:application/pdf},
}

@article{de_ronde_evaluation_2019,
	title = {The evaluation of fingermarks given activity level propositions},
	volume = {302},
	issn = {0379-0738},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073819303172},
	doi = {10.1016/j.forsciint.2019.109904},
	abstract = {Fingermarks are highly relevant in criminal investigations for individualization purposes. In some cases, the question in court changes from ‘Who is the source of the fingermarks?’ to ‘How did the fingermark end up on the surface?’. In this paper, we explore the evaluation of fingermarks given activity level propositions by using Bayesian networks. The variables that provide information on activity level questions for fingermarks are identified and their current state of knowledge with regards to fingermarks is discussed. We identified the variables transfer, persistency, recovery, background fingermarks, location of the fingermarks, direction of the fingermarks, the area of friction ridge skin that left the mark and pressure distortions as variables that may provide information on how a fingermark ended up on a surface. Using three case examples, we show how Bayesian networks can be used for the evaluation of fingermarks given activity level propositions.},
	urldate = {2024-11-25},
	journal = {Forensic Science International},
	author = {de Ronde, Anouk and Kokshoorn, Bas and de Poot, Christianne J. and de Puit, Marcel},
	month = sep,
	year = {2019},
	keywords = {Bayesian network, Evidence interpretation, Activity, Touch traces},
	pages = {109904},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\YYXZHGS7\\de Ronde et al. - 2019 - The evaluation of fingermarks given activity level propositions.pdf:application/pdf},
}

@article{lim-hitchings_frequented_2024,
	title = {From frequented environments to the crime scene: {Evaluating} findings of fibre comparisons in complex transfer scenarios.},
	volume = {361},
	issn = {0379-0738},
	shorttitle = {From frequented environments to the crime scene},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073824001671},
	doi = {10.1016/j.forsciint.2024.112086},
	abstract = {The evaluation of the results from a fibre comparison given activity level propositions is well established when considering only a single group of potential primary transfers. However, secondary transfers are less prevalent in the literature despite their potential value, especially in cases where the primary transfers are not sufficiently informative. In particular, one can consider the fibres from frequented environments of the person of interest (POI) identified in a struggle. If the POI did struggle with the complainant, these fibres can potentially be recovered in small quantities on the surface of the complainant as a result of secondary or higher order transfers. Therefore, these fibres may provide useful information that can resolve competing propositions involving struggles, as well as forensic intelligence in the form of linkages or investigative leads. If a non-differentiation is indeed found between recovered fibres and fibres from the frequented environments of the POI, these results need to be properly interpreted. In this paper, a model, based on an object oriented Bayesian network (OOBN), for evaluating such findings along with its implementation is proposed. Using available data from the literature and other sources, the model was then used to assess a few hypothetical scenarios involving secondary transfers. The results provided useful insights into secondary transfer that help to validate the model and demonstrate the potential utility that can be gained by considering transfers beyond the primary order. Moreover, these results can be used to help guide future research by identifying gaps in the literature. Finally, the direct application to a case study was conducted to demonstrate the practical aspects of such a model.},
	urldate = {2024-11-25},
	journal = {Forensic Science International},
	author = {Lim-Hitchings, Yu Chen and Taroni, Franco and Massonnet, Geneviève},
	month = aug,
	year = {2024},
	keywords = {Likelihood ratio, Secondary transfer, Trace evidence, Background, Evaluative reporting, Foreign fibre groups, Interpretation},
	pages = {112086},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\AD82G8EI\\Lim-Hitchings et al. - 2024 - From frequented environments to the crime scene Evaluating findings of fibre comparisons in complex.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\4I6BCA5S\\S0379073824001671.html:text/html},
}

@article{russell_establishing_2021,
	title = {Establishing likelihood ratios for evaluating opposing propositions concerning the activity causing methamphetamine contamination: {Smoking} or manufacture?},
	volume = {326},
	issn = {0379-0738},
	shorttitle = {Establishing likelihood ratios for evaluating opposing propositions concerning the activity causing methamphetamine contamination},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073821002590},
	doi = {10.1016/j.forsciint.2021.110939},
	abstract = {In New Zealand, concerns have been raised over the presence of methamphetamine contamination in households, especially when the activity causing the contamination is unknown. The cause of contamination is also a contentious issue in clandestine laboratory cases concerning charges in relation to “Use of Premises” (Section 12: Misuse of Drugs Act 1975, New Zealand). Regardless of the cause, other than scientific opinion, there is currently no analytical technique that can satisfactorily address the provenance of methamphetamine residues. For several years, approximate methamphetamine contamination levels have been collected from suspected clandestine laboratories in New Zealand, where methamphetamine is believed to have been manufactured. This study used this data and compared it to similar data from properties where the drug is suspected to have been used (smoked) to model likelihood ratios (LR). It is well documented that the LR forms the backbone to a Bayesian method of interpreting forensic evidence. As such, this data has the potential to underpin a novel Bayesian approach in the evaluation of clandestine laboratory evidence.},
	urldate = {2024-11-25},
	journal = {Forensic Science International},
	author = {Russell, M and Wevers, Gerhard and Bogun, Ben and Mayo, Erina and McKinnel, Megan and Watson, Janine},
	month = sep,
	year = {2021},
	keywords = {Likelihood ratio, Evaluative reporting, Clandestine laboratories, Methamphetamine contamination, Methamphetamine manufacture},
	pages = {110939},
}

@article{davidson_transfer_2024,
	title = {The transfer of spermatozoa onto children’s underwear during normal domestic laundering activities},
	volume = {364},
	issn = {0379-0738},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073824003323},
	doi = {10.1016/j.forsciint.2024.112250},
	abstract = {In cases of sexual assaults involving children, biological evidence such as semen is often found on clothes rather than on intimate swabs. Two cases involving young girls were submitted to one of the AFSP BFF forensic science laboratories instigating further research into the background levels of semen on children’s underwear after being laundered, without sexual acts occurring. This study considered the potential for background levels of semen in the washing machine and the levels of semen transferred with varying laundry storage and washing arrangements. The results concur with previous studies that low levels of spermatozoa can transfer during washing; however, this study demonstrated that background levels of semen can be present on children’s clothing during domestic laundering activities which include male underwear when there are sexually active males within the household. Further, that semen can persist in the washing machine even after three washes. This study has produced data which will assist forensic scientists to evaluate cases of child abuse in a domestic setting.},
	urldate = {2024-11-25},
	journal = {Forensic Science International},
	author = {Davidson, G. and Lee-Gorman, M. and Davidson, A.},
	month = nov,
	year = {2024},
	keywords = {Body Fluid Forum, laundry, enhanced sperm recovery, evaluation, spermatozoa, transfer \& persistence},
	pages = {112250},
}

@article{vergeer_evaluation_2020,
	title = {Evaluation of glass evidence at activity level: {A} new distribution for the background population},
	volume = {316},
	issn = {0379-0738},
	shorttitle = {Evaluation of glass evidence at activity level},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073820302930},
	doi = {10.1016/j.forsciint.2020.110431},
	abstract = {For evidence evaluation of the physicochemical properties of glass at activity level a well-known formula introduced by Evett \& Buckleton [1,2] is commonly used. Parameters in this formula are, amongst others, the probability in a background population to find on somebody’s clothing the observed number of glass sources and the probability in a background population to find on somebody’s clothing a group of fragments with the same size as the observed matching group. Recently, for efficiency reasons, the Netherlands Forensic Institute changed its methodology to measure not all the glass fragments but a subset of glass fragments found on clothing. Due to the measurement of subsets, it is difficult to get accurate estimates for these parameters in this formula. We offer a solution to this problem. The heart of the solution consists of relaxing the assumption of conditional independence of group sizes of background fragments, and modelling the probability of an allocation of background fragments into groups given a total number of background fragments by a two-parameter Chinese restaurant process (CRP) [3]. Under the assumption of random sampling of fragments to be measured from recovered fragments in the laboratory, parameter values for the Chinese restaurant process may be estimated from a relatively small dataset of glass in other relevant cases. We demonstrate this for a dataset of glass fragments collected from upper garments in casework, show model fit and provide a prototypical calculation of an LR at activity level accompanied with a parameter sensitivity analysis for reasonable ranges of the CRP parameter values. Considering that other laboratories may want to measure subsets as well, we believe this is an important alternative approach to the evaluation of numerical LRs for glass analyses at activity level.},
	urldate = {2024-11-25},
	journal = {Forensic Science International},
	author = {Vergeer, Peter and Leegwater, Anna Jeannette and Slooten, Klaas},
	month = nov,
	year = {2020},
	keywords = {Forensic, Activity level, Likelihood ratio, Chinese restaurant process, Glass},
	pages = {110431},
	file = {Full Text:C\:\\Users\\jstacey\\Zotero\\storage\\WFZNN6E3\\Vergeer et al. - 2020 - Evaluation of glass evidence at activity level A new distribution for the background population.pdf:application/pdf},
}

@article{kokshoorn_activity_2017,
	title = {Activity level {DNA} evidence evaluation: {On} propositions addressing the actor or the activity},
	volume = {278},
	issn = {0379-0738},
	shorttitle = {Activity level {DNA} evidence evaluation},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073817302360},
	doi = {10.1016/j.forsciint.2017.06.029},
	abstract = {More often than not, the source of DNA traces found at a crime scene is not disputed, but the activity or timing of events that resulted in their transfer is. As a consequence, practitioners are increasingly asked to assign a value to DNA evidence given propositions about activities provided by prosecution and defense counsel. Given that the dispute concerns the nature of the activity that took place or the identity of the actor that carried out the activity, several factors will determine how to formulate the propositions. Determining factors are (1) whether defense claims the crime never took place, (2) whether defense claims someone other than the accused (either an unknown individual or a known person) performed the criminal activity, and (3) whether it is claimed and disputed that the suspect performed an alternative, legitimate activity or has a relation to the victim, the object, or the scene of crime that implies a legitimate interaction. Addressing such propositions using Bayesian networks, we demonstrate the effects of the various proposition sets on the evaluation of the evidence.},
	urldate = {2024-11-25},
	journal = {Forensic Science International},
	author = {Kokshoorn, Bas and Blankers, Bart J. and de Zoete, Jacob and Berger, Charles E. H.},
	month = sep,
	year = {2017},
	keywords = {DNA, Bayesian network, Criminalistics, Hypotheses, Weight of evidence},
	pages = {115--124},
	file = {Submitted Version:C\:\\Users\\jstacey\\Zotero\\storage\\YMPWX4PA\\Kokshoorn et al. - 2017 - Activity level DNA evidence evaluation On propositions addressing the actor or the activity.pdf:application/pdf},
}

@article{falardeau_distribution_2020,
	title = {Distribution of aerosol paint droplets in open- and closed-space environments: {Towards} activity level evaluation},
	volume = {306},
	issn = {0379-0738},
	shorttitle = {Distribution of aerosol paint droplets in open- and closed-space environments},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073819304773},
	doi = {10.1016/j.forsciint.2019.110065},
	abstract = {One way to determine the presence of a suspect on the scene of graffiti sprayed public territory, is by seeking the presence of aerosol paint droplets transferred to his clothing and accessories. Previous studies have evaluated the distribution of spray paint droplets in closed space, on clothing, and its persistence following washing. Yet, due to the difficulty in observing and counting them under a stereomicroscope, the previous studies had to limit the tests conducted and limit the amount of data collected. In this paper, we perform simulations of spraying using a fluorescent paint and a computer-automated extraction of features and counting of the droplets. With these tools we were able to perform many different tests in a relatively small amount of time and gather data about the density, distribution, and size of the droplets transferred. We discuss the obtained results and hypothesize that such evaluations of the transferred droplets would permit to distinguish between different scenarios at an activity level, such as the presence on the scene as a simple witness or bystander, versus the illicit action of spraying a graffiti or a tag.},
	urldate = {2024-11-25},
	journal = {Forensic Science International},
	author = {Falardeau, Mylène and Muehlethaler, Cyril},
	month = jan,
	year = {2020},
	keywords = {Activity level evaluation, Aerosol paint, Computer-automated extraction, Density, Distribution, Droplets, Environment, Graffiti, Size, Stereomicroscope, Total area},
	pages = {110065},
}

@article{maitre_application_2022,
	title = {An application example of the likelihood ratio approach to the evaluation of organic gunshot residues using a fictional scenario and recently published data},
	volume = {335},
	issn = {0379-0738},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073822000974},
	doi = {10.1016/j.forsciint.2022.111267},
	abstract = {The analysis of gunshot residues (GSR) can provide important information with regard to the involvement of a person of interest (POI) in a firearm-related incident. Organic gunshot residues (OGSR) have been investigated in order to provide additional and complementary information to the traditional inorganic gunshot residue (IGSR) particles detected by scanning electron microscopy (SEM). Currently, many procedures and analytical methods have been developed to detect OGSR-related compounds collected from the shooter’s hands. However, such studies provide no information regarding the inclusion of such results in an activity level evaluation for discharging a firearm. The aim of this article is to assess the feasibility of using the likelihood ratio (LR) approach as a tool to evaluate OGSR results for activity level propositions. The developed model focuses on the assignment of an LR for several compounds detected in OGSR. A simple worst-case simulation was investigated in order to assess the applicability of the LR approach to evaluate OGSR traces. This simulation highlighted the importance of addressing an appropriate pair of activity level propositions when evaluating the results.},
	urldate = {2024-11-25},
	journal = {Forensic Science International},
	author = {Maitre, Matthieu and Horder, Mark and Kirkbride, K. Paul and Gassner, Anne-Laure and Weyermann, Céline and Gupta, Anjali and Beavis, Alison and Roux, Claude},
	month = jun,
	year = {2022},
	keywords = {Activity level, Interpretation, Weight of evidence, Firearm, GSR, OGSR},
	pages = {111267},
}

@article{osnat_likelihood-ratio_2022,
	title = {A likelihood-ratio framework for evaluating results of forensic gunshot-residue analysis},
	volume = {336},
	issn = {0379-0738},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073822001694},
	doi = {10.1016/j.forsciint.2022.111339},
	abstract = {When reporting results of Gunshot Residue (GSR) analysis from a person suspected to be involved in a recent shooting, most forensic experts only provide the court with the raw results (i.e. the number of GSR particles found) and a disclaimer that a positive finding does not prove that the suspect was involved in a firearm shooting incident whilst a negative finding does not prove that he was not. Probabilistic analysis of the GSR results provides more value to the court, so the present study calculated likelihood ratio (LR) values for finding 0–8 characteristic GSR particles (containing Lead, Barium and Antimony) on a suspect's hands, based on the available GSR data from the published literature as well as studies by the authors. Defense propositions, i.e. modes for GSR acquisition other than involvement in a shooting event, were divided into three broad categories: low, medium and heavy background. For each background level and number of GSR particles found, minimal and maximal LR values were calculated. Thus, for each proposition the defense provides for the presence of GSR on the defendant's hands, the forensic expert can provide a possible set of minimal and maximal LR values, leaving the court to examine the defendant's contention and decide which of the three background modes is more plausible according to the circumstances of the specific case.},
	urldate = {2024-11-25},
	journal = {Forensic Science International},
	author = {Osnat, Israelsohn Azulay and Hila, Rosengarten and Amit, Cohen and Yigal, Zidon and Zohar, Pasternak},
	month = jul,
	year = {2022},
	keywords = {GSR, Gunshot residue, LR, Shooting incident, Statistics},
	pages = {111339},
}

@article{curran_estimating_2024,
	title = {Estimating probability terms for the background presence of glass when considering activity in forensic casework},
	volume = {364},
	issn = {0379-0738},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073824003025},
	doi = {10.1016/j.forsciint.2024.112221},
	abstract = {Coulson et al. [1] proposed methodology for the estimation of the P and S terms used in glass interpretation when assessing the value of the findings given activity level propositions. These terms arise in a model proposed by Evett [2], Evett and Buckleton [3], and are based on survey data. Specifically they proposed a model for estimating Pk, k = 0, 1, 2, … and Sn, n = 1, 2, where Pk is the probability of finding k distinct sources (or groups) of glass on a person of interest (POI), and Sn is the probability that the kth source consists of n fragments. In this article we make a number of extensions to the work of Coulson et al. [1]. Firstly we derive an estimate of the uncertainty in the parameter of the Coulson et al. model, and show how this may be used—for example, to compute an estimate of how the probabilities may vary or how to compare estimates resulting from different surveys. Secondly, we extend the model by allowing a more sensible modelling of the “excess” zeros (in the case of the P terms) and excess ones (in the case of the S terms). The methodology used to make these extensions relies on purely frequentist theory of estimation in keeping with the original work. A Bayesian approach to estimation will be the subject of future work. Additionally, we demonstrate the use of an R (R Core Team, [4]) package, called fitPS (Curran, [5]) which makes the methodology described in this article easy to implement in practice.},
	urldate = {2024-11-25},
	journal = {Forensic Science International},
	author = {Curran, James M. and Buzzini, Patrick and Trejos, Tatiana},
	month = nov,
	year = {2024},
	keywords = {Estimation, Activity, Statistics, Glass evidence},
	pages = {112221},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\KJQEJNYR\\Curran et al. - 2024 - Estimating probability terms for the background presence of glass when considering activity in foren.pdf:application/pdf},
}

@article{sloan_evaluation_2020,
	title = {An evaluation of human stabbing performance to inform the standardisation of textile damage examinations: {Do} simulation trials correlate to reported stabbings?},
	volume = {312},
	issn = {0379-0738},
	shorttitle = {An evaluation of human stabbing performance to inform the standardisation of textile damage examinations},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073820301675},
	doi = {10.1016/j.forsciint.2020.110305},
	abstract = {Forensic textile damage examinations are commonly requested in cases such as stabbings. These requests often involve the testing of knives or other weapons submitted to determine if they could have caused the damage to the evidential garment. Currently a forensic practitioner conducts this testing by manually performing the stabbing action. A biomechanics performance trial was conducted to evaluate how a range of human factors contribute to the creation of textile damage by stabbing actions. Surveys of sharp force fatalities and clinical penetrative injuries reported the chest and abdomen as the most frequent target location for stab wounds. The location of the cut-type damage recorded during the trial was found to correlate to the location of stab injuries incurred during actual stabbing cases. The type of weapon had an impact on the actions undertaken. Participants mostly utilised the smaller utility and hunting knives in underarm thrusting or overarm hacking actions, whereas an overarm hacking action, or combined hacking/slashing action was performed when using the machete. The familiarity of the knife, shape of the handle and perceived risk of injury determined how the handle was held. Participants frequently stabbed into the target immediately in front of their dominant hand, however care should be taken in interpreting this in a casework scenario. The machete was used with the highest mean velocity, and the utility knife the lowest.},
	urldate = {2024-11-25},
	journal = {Forensic Science International},
	author = {Sloan, Kate and Robertson, James and Fergusson, Macarthur and Spratford, Wayne},
	month = jul,
	year = {2020},
	keywords = {Biomechanics, Human stabbing performance, Sharp force fatality, Stab injury, Textile damage examination},
	pages = {110305},
}

@article{galais_exploring_2024,
	title = {Exploring the influence of washing activities on the transfer and persistence of fibres in forensic science},
	volume = {361},
	issn = {0379-0738},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073824001592},
	doi = {10.1016/j.forsciint.2024.112078},
	abstract = {In forensic science, a robust and sound interpretation and evaluation of transferred fibre evidence requires an understanding of the principles and mechanisms that underpin fibre transfer, yet existing research lacks consistency and repeatability. This study investigates the impact of washing activities on both the release of fibres into wastewater and the transfer of constituent fibres from donor garments to receiver swatches. Using a low-cost friction tester and automated data collection through photography and ImageJ image processing software, controlled conditions were maintained for repeated experiments. Results indicated significant fibre release during wash cycles, with load size and donor garment history playing crucial roles. The donor garments subjected to repetitive washes exhibit a progressive decrease in the number of fibres transferred, independently of the load size. This study underscores the importance of considering a garment's washing history in forensic science contexts, but also for consistency in the way that data are collected.},
	urldate = {2024-11-25},
	journal = {Forensic Science International},
	author = {Galais, Virginie and Gannicliffe, Chris and Dugard, Patricia and Wilson, Stephanie and Daéid, Niamh Nic and Ménard, Hervé},
	month = aug,
	year = {2024},
	keywords = {Forensic science, Transfer, Automated data collection, Fiber, Washing, Wastewater},
	pages = {112078},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\4R3J7RGM\\Galais et al. - 2024 - Exploring the influence of washing activities on the transfer and persistence of fibres in forensic.pdf:application/pdf},
}

@article{hughes_impact_2024,
	title = {The impact of substrate characteristics on the collection and persistence of biological materials, and their implications for forensic casework},
	volume = {356},
	issn = {0379-0738},
	url = {https://www.sciencedirect.com/science/article/pii/S037907382400032X},
	doi = {10.1016/j.forsciint.2024.111951},
	abstract = {This study assessed the level of nucleic acid persistence on the substrate pre-, and post-swabbing, in order to assess whether biological materials (touch, saliva, semen, and blood) are collected differently depending on the substrate characteristics. A total of 48 samples per deposit and substrate variety (n = 384) were assessed by tracking the persistence of nucleic acid using Diamond™ Nucleic Acid Dye (DD) staining and Polilight photography. The number of DD nucleic acid fluorescent complexes formed post-staining were counted (fluorescent count) and in conjunction with the fluorescence signal intensity (DD nucleic acid complex accumulation) used to estimate the level of nucleic acid persistence on substrates. Touch deposits have shown to be the most persistent deposit with strong adhesion capabilities on both substrate verities. Saliva displayed a higher persistence than semen and/or blood. Semen displayed a high collection efficiency as well as a high fluorescence signal intensity. Blood displayed a low persistence on both substrates with a superior collection efficiency that may also indicate a higher probability to become dislodged from surfaces given a particular activity. Our research has shown that the persistence and recovery of biological deposits is not only measurable but more importantly, may have the potential to be estimated, as such, may build an understanding that can provide valuable guidance for collection efficiency evaluations, and the assessing of the probability of particular profiles, given alternate propositions of means of transfer occurring.},
	urldate = {2024-11-25},
	journal = {Forensic Science International},
	author = {Hughes, Deborah A. and Szkuta, Bianca and van Oorschot, Roland A. H. and Conlan, Xavier A.},
	month = mar,
	year = {2024},
	keywords = {DNA persistence, DNA recovery, DNA collection efficiency, Indirect DNA transfer, Substrate effect},
	pages = {111951},
}

@article{ballantyne_transparent_2024,
	title = {A transparent approach: {Openness} in forensic science reporting},
	volume = {8},
	issn = {2589-871X},
	shorttitle = {A transparent approach},
	url = {https://www.sciencedirect.com/science/article/pii/S2589871X24000214},
	doi = {10.1016/j.fsisyn.2024.100474},
	abstract = {There have been numerous calls for increased transparency and disclosure in forensic science. However, there is a paucity of guidance on how to achieve this transparency in reports, and the impacts it may have on criminal justice proceedings. We describe one multi-disciplinary forensic laboratory's journey to fully transparent reporting, disclosing matters of scientific relevance and importance. All expert reports across 17 disciplines now contain information regarding the fundamental principles and methodology, validity and error, assumptions and limitations, competency testing and quality assurance, cognitive factors, and areas of scientific controversy. Staff support for transparent reporting increased following introduction, with most reporting largely positive impacts. A slight increase in questioning in court has been experienced, with increased legal attention paid to the indicia of scientific validity. Transparency in expert forensic science reports is possible, and can improve the use of scientific evidence in courts without compromising the timeliness of service.},
	urldate = {2024-11-25},
	journal = {Forensic Science International: Synergy},
	author = {Ballantyne, Kaye N. and Summersby, Stephanie and Pearson, James R. and Nicol, Katherine and Pirie, Erin and Quinn, Catherine and Kogios, Rebecca},
	month = jan,
	year = {2024},
	keywords = {Forensic science, Communicating opinion, Expert evidence, Expert reports, Transparency},
	pages = {100474},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\8ZWG62Z9\\Ballantyne et al. - 2024 - A transparent approach Openness in forensic science reporting.pdf:application/pdf},
}

@article{chorozidis_knowledge_2024,
	title = {Knowledge and research mapping of the data and database forensics domains: {A} bibliometric analysis},
	volume = {171},
	issn = {0950-5849},
	shorttitle = {Knowledge and research mapping of the data and database forensics domains},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584924000776},
	doi = {10.1016/j.infsof.2024.107472},
	abstract = {The field of digital forensics has undergone rapid development alongside the technological advancements of the latest century. This study focuses in two of its subdomains, namely database forensics and data forensics. Though the concept of a database is relatively old, there is an academic void when it comes to its research compared to different domains in digital forensics. Data forensics has a myriad of applications, however there appears to be a lack of standardization in regards to the field itself throughout the different disciplines of the forensic field. Our main objectives with this study were to identify the prominent trends, uncover research gaps or further research necessity and to provide a high level outline of the selected domains. To fulfill the objectives, we designed and executed a protocol with predefined phases, steps, and activities that all stem from the principles of bibliometric analysis. The findings of the methodological procedure are presented and the research questions are answered in a concise manner. The two domains have considerable growth, given how recently they emerged in literature. However, there are issues present in the current literature that might hinder the future research and might repulse not only the aspiring but also the current professionals of the forensic field. These issues must be resolved in order to make the selected domains less elusive when it comes to cross-domain applications and when new practitioners are concerned.},
	urldate = {2024-11-25},
	journal = {Information and Software Technology},
	author = {Chorozidis, Georgios and Georgiou, Konstantinos and Mittas, Nikolaos and Angelis, Lefteris},
	month = jul,
	year = {2024},
	keywords = {Bibliometric analysis, Data forensics, Database forensics, Digital forensics, Scientometrics, Systematic mapping study},
	pages = {107472},
}

@article{fantinato_investigative_2024,
	title = {Investigative use of human environmental {DNA} in forensic genetics},
	volume = {70},
	issn = {1872-4973},
	url = {https://www.sciencedirect.com/science/article/pii/S1872497324000152},
	doi = {10.1016/j.fsigen.2024.103021},
	abstract = {Individuals leave behind traces of their DNA wherever they go. DNA can be transferred to surfaces and items upon touch, can be released into the air, and may be deposited in indoor dust. The mere presence of individuals in a location is sufficient to facilitate either direct or indirect DNA transfer into the surrounding environment. In this study, we analyzed samples recovered from commonly touched surfaces such as light switches and door handles in an office environment. We evaluated two different methods to isolate DNA and co-extract DNA and RNA from the samples. DNA profiles were compared to the references of the inhabitants of the different locations and were analyzed taking into consideration the type of sampled surface, sampling location and information about the activities in a room during the sampling day. Results from DNA samples collected from surfaces were also compared to those from air and dust samples collected in parallel from the same areas. We characterized the amount and composition of DNA found on various surfaces and showed that surface DNA sampling can be used to detect occupants of a location. The results also indicate that combining information from environmental samples collected from different DNA sources can improve our understanding of DNA transfer events in an indoor setting. This study further demonstrates the potential of human environmental DNA as an investigative tool in forensic genetics.},
	urldate = {2024-11-25},
	journal = {Forensic Science International: Genetics},
	author = {Fantinato, Chiara and Gill, Peter and Fonneløp, Ane Elida},
	month = may,
	year = {2024},
	keywords = {Forensic science, Background DNA, DNA transfer, Trace DNA, eDNA, mRNA profiling},
	pages = {103021},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\EHN3MJEB\\Fantinato et al. - 2024 - Investigative use of human environmental DNA in forensic genetics.pdf:application/pdf},
}

@article{vanneste_interpretation_2024,
	title = {Interpretation of a paint cross-transfer on a burglary scene – {A} case report},
	volume = {64},
	issn = {1355-0306},
	url = {https://www.sciencedirect.com/science/article/pii/S1355030624000091},
	doi = {10.1016/j.scijus.2024.02.004},
	abstract = {In forensic paint examination, paint traces retrieved on a crime scene are regularly compared to painted objects seized from a suspect. Less often, traces are only observed on the seized objects and compared to a damaged painted object on the crime scene. In some specific cases, paint traces may be found both on the crime scene and on one or multiple seized painted objects. The latter may be the result of a cross-transfer that occurred during the illicit act. However, mere coincidence is another possible explanation for these observations and may not be neglected. Proper consideration of the relevant populations and sufficient analytical data permits the evaluation of the results given activity level propositions. This allows the forensic expert to present a transparent and well-balanced statement on the value of their findings concerning the disputed issues in court.},
	number = {3},
	urldate = {2024-11-25},
	journal = {Science \& Justice},
	author = {Vanneste, Florian and Lim-Hitchings, Yu Chen and Massonnet, Geneviève and Lunstroot, Kyra},
	month = may,
	year = {2024},
	keywords = {Evaluative reporting, Cross-transfer, Paint examination},
	pages = {258--263},
	file = {ScienceDirect Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\Z3CI6INT\\S1355030624000091.html:text/html},
}

@misc{new_zealand_ministry_of_health_annual_2024,
	title = {Annual {Data} {Explorer} 2023/24: {New} {Zealand} {Health} {Survey}},
	url = {https://minhealthnz.shinyapps.io/nz-health-survey-2023-24-annual-data-explorer},
	urldate = {2024-11-27},
	author = {New Zealand Ministry of Health},
	year = {2024},
	file = {minhealthnz.shinyapps.io/nz-health-survey-2023-24-annual-data-explorer/_w_0abf662f/#!/explore-topics:C\:\\Users\\jstacey\\Zotero\\storage\\K3D5J3V2\\_w_0abf662f.html:text/html},
}

@article{yuan_nicotine_2015,
	title = {Nicotine and the adolescent brain},
	volume = {593},
	copyright = {© 2015 The Authors. The Journal of Physiology © 2015 The Physiological Society},
	issn = {1469-7793},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1113/JP270492},
	doi = {10.1113/JP270492},
	abstract = {Adolescence encompasses a sensitive developmental period of enhanced clinical vulnerability to nicotine, tobacco, and e-cigarettes. While there are sociocultural influences, data at preclinical and clinical levels indicate that this adolescent sensitivity has strong neurobiological underpinnings. Although definitions of adolescence vary, the hallmark of this period is a profound reorganization of brain regions necessary for mature cognitive and executive function, working memory, reward processing, emotional regulation, and motivated behavior. Regulating critical facets of brain maturation are nicotinic acetylcholine receptors (nAChRs). However, perturbations of cholinergic systems during this time with nicotine, via tobacco or e-cigarettes, have unique consequences on adolescent development. In this review, we highlight recent clinical and preclinical data examining the adolescent brain's distinct neurobiology and unique sensitivity to nicotine. First, we discuss what defines adolescence before reviewing normative structural and neurochemical alterations that persist until early adulthood, with an emphasis on dopaminergic systems. We review how acute exposure to nicotine impacts brain development and how drug responses differ from those seen in adults. Finally, we discuss the persistent alterations in neuronal signaling and cognitive function that result from chronic nicotine exposure, while highlighting a low dose, semi-chronic exposure paradigm that may better model adolescent tobacco use. We argue that nicotine exposure, increasingly occurring as a result of e-cigarette use, may induce epigenetic changes that sensitize the brain to other drugs and prime it for future substance abuse.},
	language = {en},
	number = {16},
	urldate = {2024-11-27},
	journal = {The Journal of Physiology},
	author = {Yuan, Menglu and Cross, Sarah J. and Loughlin, Sandra E. and Leslie, Frances M.},
	year = {2015},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1113/JP270492},
	pages = {3397--3412},
	file = {Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\BCUZ4S6E\\Yuan et al. - 2015 - Nicotine and the adolescent brain.pdf:application/pdf;Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\EC2J27W9\\JP270492.html:text/html},
}

@article{sijen_identification_2021,
	title = {On the {Identification} of {Body} {Fluids} and {Tissues}: {A} {Crucial} {Link} in the {Investigation} and {Solution} of {Crime}},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2073-4425},
	shorttitle = {On the {Identification} of {Body} {Fluids} and {Tissues}},
	url = {https://www.mdpi.com/2073-4425/12/11/1728},
	doi = {10.3390/genes12111728},
	abstract = {Body ﬂuid and body tissue identiﬁcation are important in forensic science as they can provide key evidence in a criminal investigation and may assist the court in reaching conclusions. Establishing a link between identifying the ﬂuid or tissue and the DNA proﬁle adds further weight to this evidence. Many forensic laboratories retain techniques for the identiﬁcation of biological ﬂuids that have been widely used for some time. More recently, many different biomarkers and technologies have been proposed for identiﬁcation of body ﬂuids and tissues of forensic relevance some of which are now used in forensic casework. Here, we summarize the role of body ﬂuid/ tissue identiﬁcation in the evaluation of forensic evidence, describe how such evidence is detected at the crime scene and in the laboratory, elaborate different technologies available to do this, and reﬂect real life experiences. We explain how, by including this information, crucial links can be made to aid in the investigation and solution of crime.},
	language = {en},
	number = {11},
	urldate = {2024-11-28},
	journal = {Genes},
	author = {Sijen, Titia and Harbison, SallyAnn},
	month = oct,
	year = {2021},
	pages = {1728},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\RTTBRCM2\\Sijen and Harbison - 2021 - On the Identification of Body Fluids and Tissues A Crucial Link in the Investigation and Solution o.pdf:application/pdf},
}

@phdthesis{kim_understanding_nodate,
	type = {Master of {Science}},
	title = {Understanding {Smoking} and {Vaping} {Habits} in {Aotearoa} {New} {Zealand} {Using} {Wastewater}-{Based} {Epidemiology}},
	abstract = {In 2022, the rate of daily smokers in New Zealand (NZ) fell to 8\%, a historical low that reflects the efficacy of efforts and regulation to meet the NZ government’s Smokefree 2025 goal. Recently, however, the prevalence of e-cigarette/vaping has been shown to rapidly increase according to a recent Ministry of Health survey, e-cigarette use rapidly began increasing in 2019 and overtook cigarette use in 2021. Worryingly, e-cigarette usage is high in NZ youths. To monitor and understand these trends, it is critical to develop reliable procedures to measure cigarette smoking and vaping prevalence and use this to obtain reliable and current data in NZ. This study constitutes the most extensive study ever undertaken to understand NZ nicotine consumption through Wastewater-Based Epidemiology (WBE), with the main aims of analysing both cigarette smoking and vaping habits in NZ and using this to identify cigarette smoking \& vaping/e-cigarette use trends. To this end, this study involved the development, validation and implementation of an SPE-LC-MS/MS method to analyse nicotine metabolites cotinine (COT) and trans-3’-hydroxycotinine (COT-OH) in addition to tobacco-specific minor alkaloids anatabine (ANTA) and anabasine (ANBA) in wastewater.},
	language = {en},
	author = {Kim, Hea Jeong},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\B5EJTZV5\\Kim - Understanding Smoking and Vaping Habits in Aotearoa New Zealand Using Wastewater-Based Epidemiology.pdf:application/pdf},
}

@article{gill_r_2022,
	title = {R {F} {U} derived {L} {R} s for activity level assignments using {Bayesian} {Networks}},
	volume = {56},
	issn = {1872-4973, 1878-0326},
	url = {https://www.fsigenetics.com/article/S1872-4973(21)00144-7/fulltext},
	doi = {10.1016/j.fsigen.2021.102608},
	language = {English},
	urldate = {2024-11-28},
	journal = {Forensic Science International: Genetics},
	author = {Gill, Peter and Bleka, Øyvind and Fonneløp, Ane Elida},
	month = jan,
	year = {2022},
	pmid = {34735938},
	note = {Publisher: Elsevier},
	keywords = {Evidence evaluation, Secondary transfer, Direct transfer, Likelihood ratio (LR), Mixtures, Bayesian Network, ALTRaPht, EuroForMix},
	file = {Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\A9CTLB2B\\Gill et al. - 2022 - R F U derived L R s for activity level assignments using Bayesian Networks.pdf:application/pdf},
}

@misc{gill_react_2024,
	title = {The {ReAct} project: {Analysis} of data from 23 different laboratories to characterise {DNA} recovery given two sets of activity level propositions},
	copyright = {http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {The {ReAct} project},
	url = {http://biorxiv.org/lookup/doi/10.1101/2024.10.10.617148},
	doi = {10.1101/2024.10.10.617148},
	abstract = {Abstract
          
            The ReAct (Recovery, Activity) project is an ENFSI (European Network of Forensic Science Institutes) supported initiative comprising a large consortium of laboratories. Here, the results from more than 23 laboratories are presented. The primary purpose was to design experiments simulating typical casework circumstances; collect data and to implement Bayesian networks to assess the value (i.e., likelihood ratio) of DNA results given activity level propositions. Two different experimental designs were used to simulate a robbery, where a screwdriver was used to force a door or window. Propositions and case information were chosen following laboratory feedback listing typical casework circumstances (included in the paper). Whereas the proposition representing prosecution’s view is applicable to many cases: the defendant forced the door/window with his screwdriver; the proposition representing defence’s view are more variable depending on what is alleged to have happened. In a direct transfer experiment, the defendant owned and used the screwdriver, but he did not force the door/window in question. An unknown person used the defendant’s stolen screwdriver. In an indirect transfer experiment, the defendant neither owned, saw, nor used the screwdriver, nor did they force the door or window. Instead, an unknown individual—someone with whom the defendant had interacted (e.g., by shaking hands or touching the same objects)—used the recovered tool to force entry. In the first experiment, if the defendant forced the door/window, then s/he is the last handler of the screw driver, whereas if the tool was stolen and used by an unknown person, the defendant is the first handler. For the second experiment, given the defence view, the defendant never held the screwdriver. We envisaged the situation where an object manipulated by the defendant (or the defendant himself/herself) would be touched by the unknown offender who would then force the window. The time delay between the touching of the object and the forcing of the window was 0, 1h, and 2h. This was accomplished by handshaking experiments, where the person forcing the door would shake a person’s hand, and then use the screwdriver either immediately after hand-shake or after 1, 2 hours. It was found for the direct transfer experiment that unless a single contributor profile aligning with the known person’s of interest profile was retrieved, the results did not allow to discriminate propositions. On the other hand, for the indirect transfer experiment, both single and major contributor profiles that aligned with the person of interest (POI) supported the proposition that the person used the tool rather than an unknown person who had touched an object, when indeed the former was true. There was considerable variation in median recoveries of DNA between laboratories (between 200pg - 5ng) for a given experiment if quantities are taken into account. These differences affect the likelihood ratios given activity level propositions. More than 2,700 samples were analysed in the course of this study. Two different Bayesian Networks are made available via an open source application written in Shiny R:
            Shiny\_React()
            . For comparison, all datasets were analysed using a qualitative method categorised into absent, single, balanced or major given contributors.
          
          The need for standardisation of methods is highlighted, along with the need for new methods to assess probability of laboratory dependent DNA recovery. Open access databases that are made freely available are important adjuncts},
	language = {en},
	urldate = {2024-11-28},
	publisher = {Genetics},
	author = {Gill, Peter and Fonneløp, Ane Elida and Hicks, Tacha and Xenophontos, Stavroulla and Cariolou, Marios and Van Oorschot, Roland and Buckel, Iris and Sukser, Viktorija and Papić, Sunčica and Merkaš, Siniša and Kostic, Ana and Pereira, Angela Marques and Teutsch, Christina and Forsberg, Christina and Haas, Cordula and Petkovski, Elizabet and Hass, Fabian and Masek, Jan and Stosic, Jelena and Lee, Yong Sheng and Syn, Christopher Kiu-Choong and Groombridge, Linda and Trimborn, Marc and Hadjivassiliou, Marilena and Breathnach, Michelle and Novackova, Jana and Parson, Walther and Hatzer-Grubwieser, Petra and Pietikäinen, Sanna and Joas, Simone and Willuweit, Sascha and Grethe, Stefanie and Milićević, Tamara and Hasselqvist, Therese and Kallupurackal, Venus and Stenzl, Vlastimil and Jansson, Staffan and Glocker, Ingrun and Brunck, Sarah and Nyhagen, Karoline and Dyve Lingelem, Anne Berit and Autere, Heli and Thornbury, Devon and Pedersen, Natalie and Fox, Stephanie and Moore, David and Escott, Gemma and Petersen, Cathrine Bie and Larsen, Hans Jakob and Giles, Rebecca and Allen, Paul Stafford and Bastisch, Ingo},
	month = oct,
	year = {2024},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\L6QMV2GD\\Gill et al. - 2024 - The ReAct project Analysis of data from 23 different laboratories to characterise DNA recovery give.pdf:application/pdf},
}

@article{xu_forensic_2024,
	title = {Forensic {Science} and {How} {Statistics} {Can} {Help} {It}: {Evidence}, {Hypothesis} {Testing}, and {Graphical} {Models}},
	copyright = {© 2024. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”).  Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
	shorttitle = {Forensic {Science} and {How} {Statistics} {Can} {Help} {It}},
	url = {https://www.proquest.com/publiccontent/docview/2981988166?pq-origsite=primo&sourcetype=Working%20Papers},
	abstract = {The persistent issue of wrongful convictions in the United States emphasizes the need for scrutiny and improvement of the criminal justice system. While statistical methods for the evaluation of forensic evidence, including glass, fingerprints, and DNA, have significantly contributed to solving intricate crimes, there is a notable lack of national-level standards to ensure the appropriate application of statistics in forensic investigations. We discuss the obstacles in the application of statistics in court, and emphasize the importance of making statistical interpretation accessible to non-statisticians, especially those who make decisions about potentially innocent individuals. We investigate the use and misuse of statistical methods in crime investigations, in particular the likelihood ratio approach. We further describe the use of graphical models, where hypotheses and evidence can be represented as nodes connected by arrows signifying association or causality. We emphasize the advantages of special graph structures, such as object-oriented Bayesian networks and chain event graphs, which allow for the concurrent examination of evidence of various nature.},
	language = {English},
	urldate = {2024-11-28},
	journal = {arXiv.org},
	author = {Xu, Xiangyu and Vinci, Giuseppe},
	month = mar,
	year = {2024},
	note = {Place: Ithaca, United States
Publisher: Cornell University Library, arXiv.org
Section: Statistics
University: Cornell University Library arXiv.org},
	keywords = {Hypothesis testing, Likelihood ratio, Hypotheses, Applications, Bayesian analysis, Crime, Graphical representations, Statistical methods},
}

@article{vooijs_towards_2015,
	title = {Towards source level evaluation of the evidential value of fibre examinations},
	volume = {250},
	issn = {0379-0738},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073815000729},
	doi = {10.1016/j.forsciint.2015.02.009},
	abstract = {This paper aims to provide the first steps towards a numerical source level evaluation of fibre evidence. For that purpose, likelihood ratio equations are derived for four generic scenarios, in which the source frequency, the number of references and trace types investigated, and the number of matches vary. Previous experimental studies into the evaluation of fibre evidence are reviewed and we demonstrate how the results of these studies, as well as other data, can be used to evaluate the derived equations for the four scenarios. Evaluation is not straightforward and requires a number of assumptions. This is mainly because the relevant population under consideration in a specific case cannot be sufficiently evaluated. In addition, the subjective match-criterion in current forensic fibre examinations makes it impossible to implement a good evaluation of the within-variation of samples. As a result, the discrimination power, currently calculated for discrimination studies, is only valid for samples with negligible heterogeneity. We conclude that reporting a numerical evidential value for forensic fibre examinations is not yet feasible as the data are available for only a few types of fibres and cannot be used without several assumptions. We propose a number of developments that are required to improve the accuracy and numerical analysis.},
	urldate = {2024-11-28},
	journal = {Forensic Science International},
	author = {Vooijs, Cees and Vergeer, Peter and van der Weerd, Jaap},
	month = may,
	year = {2015},
	keywords = {Discrimination power, Discrimination study, Fiber evidence, Likelihood ratio LR, Match, Numerical evaluation, Population study},
	pages = {57--67},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\74SJE7FY\\Vooijs et al. - 2015 - Towards source level evaluation of the evidential value of fibre examinations.pdf:application/pdf},
}

@phdthesis{coates_bayesian_2023,
	type = {Master of {Science}},
	title = {Bayesian {Networks} to address activity level {Propositions} in forensic biology casework},
	abstract = {Understanding the transfer, persistence, prevalence and recovery (TPPR) of DNA is an essential component of crime scene investigations allowing for the calculation of probabilities relating to how DNA may have been deposited. The primary aim of this study was to construct Bayesian Networks to address the activity leading to DNA deposition and how TPPR factors influence six different casework scenarios.},
	language = {en},
	school = {The University of Auckland},
	author = {Coates, Olivia},
	year = {2023},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\FLVIAYKA\\Coates - Bayesian Networks to address activity level Propositions in forensic biology casework.pdf:application/pdf},
}

@misc{chen_learning_2018,
	title = {Learning {Discrete} {Bayesian} {Networks} from {Continuous} {Data}},
	url = {http://arxiv.org/abs/1512.02406},
	doi = {10.48550/arXiv.1512.02406},
	abstract = {Learning Bayesian networks from raw data can help provide insights into the relationships between variables. While real data often contains a mixture of discrete and continuous-valued variables, many Bayesian network structure learning algorithms assume all random variables are discrete. Thus, continuous variables are often discretized when learning a Bayesian network. However, the choice of discretization policy has signiﬁcant impact on the accuracy, speed, and interpretability of the resulting models. This paper introduces a principled Bayesian discretization method for continuous variables in Bayesian networks with quadratic complexity instead of the cubic complexity of other standard techniques. Empirical demonstrations show that the proposed method is superior to the established minimum description length algorithm. In addition, this paper shows how to incorporate existing methods into the structure learning process to discretize all continuous variables and simultaneously learn Bayesian network structures.},
	language = {en},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Chen, Yi-Chun and Wheeler, Tim Allan and Kochenderfer, Mykel John},
	month = sep,
	year = {2018},
	note = {arXiv:1512.02406 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\EZE2G3XW\\Chen et al. - 2018 - Learning Discrete Bayesian Networks from Continuous Data.pdf:application/pdf},
}

@article{duran_understanding_2024,
	title = {From understanding to justifying: {Computational} reliabilism for {AI}-based forensic evidence evaluation},
	volume = {9},
	issn = {2589871X},
	shorttitle = {From understanding to justifying},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2589871X24001013},
	doi = {10.1016/j.fsisyn.2024.100554},
	abstract = {Techniques from artificial intelligence (AI) can be used in forensic evidence evaluation and are currently applied in biometric fields. However, it is generally not possible to fully understand how and why these algorithms reach their conclusions. Whether and how we should include such ‘black box’ algorithms in this crucial part of the criminal law system is an open question that has not only scientific but also ethical, legal, and philosophical angles. Ideally, the question should be debated by people with diverse backgrounds.},
	language = {en},
	urldate = {2024-11-28},
	journal = {Forensic Science International: Synergy},
	author = {Durán, Juan M. and Van Der Vloed, David and Ruifrok, Arnout and Ypma, Rolf J.F.},
	year = {2024},
	pages = {100554},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\JIXX36BS\\Durán et al. - 2024 - From understanding to justifying Computational reliabilism for AI-based forensic evidence evaluatio.pdf:application/pdf},
}

@article{gill_dna_2020,
	title = {{DNA} commission of the {International} {Society} for {Forensic} {Genetics}: {Assessing} the value of forensic biological evidence - {Guidelines} highlighting the importance of propositions. {Part} {II}: {Evaluation} of biological traces considering activity level propositions},
	volume = {44},
	issn = {18724973},
	shorttitle = {{DNA} commission of the {International} society for forensic genetics},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1872497319304247},
	doi = {10.1016/j.fsigen.2019.102186},
	language = {en},
	urldate = {2024-11-28},
	journal = {Forensic Science International: Genetics},
	author = {Gill, Peter and Hicks, Tacha and Butler, John M. and Connolly, Ed and Gusmão, Leonor and Kokshoorn, Bas and Morling, Niels and Van Oorschot, Roland A.H. and Parson, Walther and Prinz, Mechthild and Schneider, Peter M. and Sijen, Titia and Taylor, Duncan},
	month = jan,
	year = {2020},
	pages = {102186},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\3RFDENDC\\Gill et al. - 2020 - DNA commission of the International society for forensic genetics Assessing the value of forensic b.pdf:application/pdf},
}

@phdthesis{stroud_evaluative_2022,
	type = {Master of {Science}},
	title = {Evaluative {Reporting} {Systems} {Using} {Bayesian} {Networks} for {Biological} {Fluid} {Identification}},
	abstract = {The identification of biological fluids in forensic samples is a key requirement in forensic science that relies on chemical and biological tests, most of which exhibit some degree of false positivity. There is a notable lack of research into cross reactions of biological fluids in stains commonly found at crime scenes. In particular, this research focused on blood, semen, and saliva as possible sources of the analysed stains. Currently, forensic scientists rely on describing results using words such as probable, possible, and likely without always being able to provide robust statistical support for these conclusions. In this research we developed multiple Bayesian Networks to assist in creating an evaluative reporting framework that can address most cases related to blood, semen, and saliva at the source level. Constructed Networks for each of these sources were applied to case scenarios to demonstrate their efficacy.},
	language = {en},
	school = {The University of Auckland},
	author = {Stroud, Angela Grace},
	year = {2022},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\258KIKD2\\Stroud - Evaluative Reporting Systems Using Bayesian Networks for Biological Fluid Identification.pdf:application/pdf},
}

@phdthesis{trnka_statistical_2024,
	type = {Master of {Science}},
	title = {Statistical and {Molecular} {Methods} for the {Attribution} of {Mixed} {Body} {Fluid} {Sources} to {Individual} {Donors}},
	abstract = {Attributing mixed biological materials in crime stains to individual donors remains a challenge in forensic science. Characterisation of the messenger RNA (mRNA) transcripts in a biological material can be used for body fluid identification (BFID) purposes. Single nucleotide polymorphisms (SNPs) in the coding regions of genomic DNA (gDNA) are copied into mRNA transcripts and can be used to distinguish between individuals. Thus, comparing cSNPs in mRNA transcripts obtained from crime stains to reference cSNP profiles from known individuals presents a way to assign biological materials to donors.},
	language = {en},
	school = {The University of Auckland},
	author = {Trnka, Anika Nina Correll},
	year = {2024},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\3J3BVZXJ\\Trnka - STATISTICAL AND MOLECULAR METHODS FOR THE ATTRIBUTION OF MIXED BODY FLUID SOURCES TO INDIVIDUAL DONO.pdf:application/pdf},
}

@article{vink_template_2024,
	title = {A template {Bayesian} network for combining forensic evidence on an item with an uncertain relation to the disputed activities},
	volume = {9},
	issn = {2589871X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2589871X24000937},
	doi = {10.1016/j.fsisyn.2024.100546},
	abstract = {Most of the forensic evidence evaluations given activity level propositions are centered around an item which is assumed to be linked to an alleged activity. However, the relation between an item of interest and an activity may be contested. This study presents a template Bayesian network (BN) for the evaluation of transfer evidence given activity level propositions considering a dispute about the relation of an item to one or more activities. The template BN includes a set of association propositions that enables the combined evaluation of evidence concerning alleged activities of the suspect and evidence concerning the use of an alleged item in those activities. Since the two types of evidence are often from different forensic disciplines, the BN is especially useful in interdisciplinary casework. Throughout the paper, we use a fictive case example that captures the essence of cases for which the template model can be used. The template BN provides a flexible starting point that can be adapted to specific case situations and supports structured probabilistic reasoning by a forensic scientist.},
	language = {en},
	urldate = {2024-11-28},
	journal = {Forensic Science International: Synergy},
	author = {Vink, M. and De Koeijer, J.A. and Sjerps, M.J.},
	year = {2024},
	keywords = {Bayesian networks, Activity level, Combining evidence, Forensic science, Association propositions, Interdisciplinary casework},
	pages = {100546},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\PVXX8YD8\\Vink et al. - 2024 - A template Bayesian network for combining forensic evidence on an item with an uncertain relation to.pdf:application/pdf},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 15 pages, 5 figures},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\7DAHV3HQ\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf},
}

@article{michelet_chatgpt_2024,
	title = {{ChatGPT}, {Llama}, can you write my report? {An} experiment on assisted digital forensics reports written using (local) large language models},
	volume = {48},
	issn = {26662817},
	shorttitle = {{ChatGPT}, {Llama}, can you write my report?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666281723002020},
	doi = {10.1016/j.fsidi.2023.301683},
	abstract = {Generative AIs, especially Large Language Models (LLMs) such as ChatGPT or Llama, have advanced significantly, positioning them as valuable tools for digital forensics. While initial studies have explored the potential of ChatGPT in the context of investigations, the question of to what extent LLMs can assist the forensic report writing process remains unresolved. To answer the question, this article first examines forensic reports with the goal of generalization (e.g., finding the ‘average structure’ of a report). We then evaluate the strengths and limitations of LLMs for generating the different parts of the forensic report using a case study. This work thus provides valuable insights into the automation of report writing, a critical facet of digital forensics investigations. We conclude that combined with thorough proofreading and corrections, LLMs may assist practitioners during the report writing process but at this point cannot replace them.},
	language = {en},
	urldate = {2024-11-28},
	journal = {Forensic Science International: Digital Investigation},
	author = {Michelet, Gaëtan and Breitinger, Frank},
	month = mar,
	year = {2024},
	pages = {301683},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\BHMDK3LU\\Michelet and Breitinger - 2024 - ChatGPT, Llama, can you write my report An experiment on assisted digital forensics reports written.pdf:application/pdf},
}

@misc{dong_hymba_2024,
	title = {Hymba: {A} {Hybrid}-head {Architecture} for {Small} {Language} {Models}},
	shorttitle = {Hymba},
	url = {http://arxiv.org/abs/2411.13676},
	doi = {10.48550/arXiv.2411.13676},
	abstract = {We propose Hymba, a family of small language models featuring a hybrid-head parallel architecture that integrates transformer attention mechanisms with state space models (SSMs) for enhanced efficiency. Attention heads provide high-resolution recall, while SSM heads enable efficient context summarization. Additionally, we introduce learnable meta tokens that are prepended to prompts, storing critical information and alleviating the “forced-to-attend” burden associated with attention mechanisms. This model is further optimized by incorporating cross-layer key-value (KV) sharing and partial sliding window attention, resulting in a compact cache size. During development, we conducted a controlled study comparing various architectures under identical settings and observed significant advantages of our proposed architecture. Notably, Hymba achieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model surpasses all sub-2B public models in performance and even outperforms Llama-3.2-3B with 1.32\% higher average accuracy, an 11.67× cache size reduction, and 3.49× throughput.},
	language = {en},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Dong, Xin and Fu, Yonggan and Diao, Shizhe and Byeon, Wonmin and Chen, Zijia and Mahabaleshwarkar, Ameya Sunil and Liu, Shih-Yang and Keirsbilck, Matthijs Van and Chen, Min-Hung and Suhara, Yoshi and Lin, Yingyan and Kautz, Jan and Molchanov, Pavlo},
	month = nov,
	year = {2024},
	note = {arXiv:2411.13676 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 20 pages, models are available on huggingface},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\PY2QDPJV\\Dong et al. - 2024 - Hymba A Hybrid-head Architecture for Small Language Models.pdf:application/pdf},
}

@misc{lu_ai_2024,
	title = {The {AI} {Scientist}: {Towards} {Fully} {Automated} {Open}-{Ended} {Scientific} {Discovery}},
	shorttitle = {The {AI} {Scientist}},
	url = {http://arxiv.org/abs/2408.06292},
	doi = {10.48550/arXiv.2408.06292},
	abstract = {One of the grand challenges of artificial general intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used as aides to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they still conduct only a small part of the scientific process. This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models to perform research independently and communicate their findings. We introduce The AI Scientist, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation. In principle, this process can be repeated to iteratively develop ideas in an open-ended fashion, acting like the human scientific community. We demonstrate its versatility by applying it to three distinct subfields of machine learning: diffusion modeling, transformer-based language modeling, and learning dynamics. Each idea is implemented and developed into a full paper at a cost of less than \$15 per paper. To evaluate the generated papers, we design and validate an automated reviewer, which we show achieves near-human performance in evaluating paper scores. The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by our automated reviewer. This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking us closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems. Our code is open-sourced at https://github.com/SakanaAI/AI-Scientist},
	language = {en},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Lu, Chris and Lu, Cong and Lange, Robert Tjarko and Foerster, Jakob and Clune, Jeff and Ha, David},
	month = sep,
	year = {2024},
	note = {arXiv:2408.06292 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\BZPLUEPP\\Lu et al. - 2024 - The AI Scientist Towards Fully Automated Open-Ended Scientific Discovery.pdf:application/pdf},
}

@misc{muennighoff_olmoe_2024,
	title = {{OLMoE}: {Open} {Mixture}-of-{Experts} {Language} {Models}},
	shorttitle = {{OLMoE}},
	url = {http://arxiv.org/abs/2409.02060},
	doi = {10.48550/arXiv.2409.02060},
	abstract = {We introduce OLMOE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMOE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMOE-1B-7B-INSTRUCT. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE training, analyze routing in our model showing high specialization, and open-source all aspects of our work: model weights, training data, code, and logs.},
	language = {en},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Muennighoff, Niklas and Soldaini, Luca and Groeneveld, Dirk and Lo, Kyle and Morrison, Jacob and Min, Sewon and Shi, Weijia and Walsh, Pete and Tafjord, Oyvind and Lambert, Nathan and Gu, Yuling and Arora, Shane and Bhagia, Akshita and Schwenk, Dustin and Wadden, David and Wettig, Alexander and Hui, Binyuan and Dettmers, Tim and Kiela, Douwe and Farhadi, Ali and Smith, Noah A. and Koh, Pang Wei and Singh, Amanpreet and Hajishirzi, Hannaneh},
	month = sep,
	year = {2024},
	note = {arXiv:2409.02060 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 61 pages (24 main), 36 figures, 14 tables},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\HSYJAMTS\\Muennighoff et al. - 2024 - OLMoE Open Mixture-of-Experts Language Models.pdf:application/pdf},
}

@misc{tay_efficient_2022,
	title = {Efficient {Transformers}: {A} {Survey}},
	shorttitle = {Efficient {Transformers}},
	url = {http://arxiv.org/abs/2009.06732},
	doi = {10.48550/arXiv.2009.06732},
	abstract = {Transformer model architectures have garnered immense interest lately due to their eﬀectiveness across a range of domains like language, vision and reinforcement learning. In the ﬁeld of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of “X-former” models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory eﬃciency. With the aim of helping the avid researcher navigate this ﬂurry, this paper characterizes a large and thoughtful selection of recent eﬃciency-ﬂavored “X-former” models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
	language = {en},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
	month = mar,
	year = {2022},
	note = {arXiv:2009.06732 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Version 2: 2022 edition},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\DI5VDYM4\\Tay et al. - 2022 - Efficient Transformers A Survey.pdf:application/pdf},
}

@misc{zhang_xlam_2024,
	title = {{xLAM}: {A} {Family} of {Large} {Action} {Models} to {Empower} {AI} {Agent} {Systems}},
	shorttitle = {{xLAM}},
	url = {http://arxiv.org/abs/2409.03215},
	doi = {10.48550/arXiv.2409.03215},
	abstract = {Autonomous agents powered by large language models (LLMs) have attracted significant research interest. However, the open-source community faces many challenges in developing specialized models for agent tasks, driven by the scarcity of high-quality agent datasets and the absence of standard protocols in this area. We introduce and publicly release xLAM, a series of large action models designed for AI agent tasks. The xLAM series includes five models with both dense and mixture-of-expert architectures, ranging from 1B to 8x22B parameters, trained using a scalable, flexible pipeline that unifies, augments, and synthesizes diverse datasets to enhance AI agents’ generalizability and performance across varied environments. Our experimental results demonstrate that xLAM consistently delivers exceptional performance across multiple agent ability benchmarks, notably securing the 1st position on the Berkeley Function-Calling Leaderboard, outperforming GPT-4, Claude-3, and many other models in terms of tool use. By releasing the xLAM series, we aim to advance the performance of open-source LLMs for autonomous AI agents, potentially accelerating progress and democratizing access to high-performance models for agent tasks.},
	language = {en},
	urldate = {2024-11-28},
	publisher = {arXiv},
	author = {Zhang, Jianguo and Lan, Tian and Zhu, Ming and Liu, Zuxin and Hoang, Thai and Kokane, Shirley and Yao, Weiran and Tan, Juntao and Prabhakar, Akshara and Chen, Haolin and Liu, Zhiwei and Feng, Yihao and Awalgaonkar, Tulika and Murthy, Rithesh and Hu, Eric and Chen, Zeyuan and Xu, Ran and Niebles, Juan Carlos and Heinecke, Shelby and Wang, Huan and Savarese, Silvio and Xiong, Caiming},
	month = sep,
	year = {2024},
	note = {arXiv:2409.03215 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Technical report for the Salesforce xLAM model series},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\6LQST57M\\Zhang et al. - 2024 - xLAM A Family of Large Action Models to Empower AI Agent Systems.pdf:application/pdf},
}

@article{bibbo_air_2024,
	title = {Air {\textless}span style="font-variant:small-caps;"{\textgreater}{DNA}{\textless}/span{\textgreater} forensics: {Novel} air collection method investigations for human {\textless}span style="font-variant:small-caps;"{\textgreater}{DNA}{\textless}/span{\textgreater} identification},
	issn = {0022-1198, 1556-4029},
	shorttitle = {Air {\textless}span style="font-variant},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/1556-4029.15662},
	doi = {10.1111/1556-4029.15662},
	abstract = {Modern techniques can generate highly discriminatory DNA profiles from minuscule biological samples, providing valuable information in criminal investigations and court proceedings. However, trace and touch DNA samples, due to their nature, often have lower success rates than other biological materials, such as blood. Further, forensically aware criminals can utilize gloves and meticulously clean the crime scene to remove DNA traces of themselves from contacted surfaces. Air sampling offers a novel approach to the collection of human DNA that has the potential to bypass some of these issues. This study reports on the results of research into the prevalence and persistence of human DNA in the air. The ability to collect human DNA from the air was investigated with the use of an AirPrep Cub Sampler ACD220 in different spaces, with and without the presence of individuals for various durations of sample collection. Results of this study demonstrate that level of occupation and sampling duration each have an influence on quantity and quality of DNA recovered from the air whereas the effects of orientation and distance of participants from the collection device as well as sequence of occupation remain unclear and require further investigation.},
	language = {en},
	urldate = {2024-11-28},
	journal = {Journal of Forensic Sciences},
	author = {Bibbo, Emily and Taylor, Duncan and Van Oorschot, Roland A. H. and Goray, Mariya},
	month = nov,
	year = {2024},
	pages = {1556--4029.15662},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\5479JDV9\\Bibbo et al. - 2024 - Air DNA forensics Novel air collection method investi.pdf:application/pdf},
}

@article{nyarko_forensic_2024,
	title = {Forensic detection of heterogeneous activity in data using deep learning methods},
	volume = {21},
	issn = {26673053},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S266730532300128X},
	doi = {10.1016/j.iswa.2023.200303},
	abstract = {The abundance of digital images has been facilitated by smartphones and inexpensive storage. Digital forensic investigation requires the processing of tons of digital images collected on devices to either identify or validate the device’s user or to ascertain whether the operator has any connections to the case that would be of interest. Examining and evaluating heterogeneous activity presents several difficulties, including variability, complex interaction across information, and volume. Digital forensics processes are said to need the inspection and analysis stages. This research presents a hybrid optimization of the Grey Wolf and artificial bee colony (GW-ABC) optimization with deep learning model Convolutional Neural Network (CNN) i.e., GW-ABC-CNN, and the developed framework is integrated as a module for Autopsy software. The main objective of this research is to detect the heterogeneous activity of humans from the Heterogeneous Human Activity Recognition (HHAR) database. The developed model is integrated into the data-source ingest module; in this module, pre-processing, feature extraction, and detection process is performed. Moreover, in the pre-processing stage, the Min-Max normalization method is used and the required frequency and time features are extracted using the GW-ABC method. In addition, CNN is used to detect heterogeneous activity; this detection process is performed by four layers. Finally, the effectiveness of the developed model is assessed, and the outcomes of using the GW-ABC-CNN paradigm were compared to those of other strategies to evaluate the model’s effectiveness.},
	language = {en},
	urldate = {2024-11-28},
	journal = {Intelligent Systems with Applications},
	author = {Nyarko, Benedicta Nana Esi and Bin, Wu and Zhou, Jinzhi and Odoom, Justice and Danso, Samuel Akwasi and Addai, Gyarteng Emmanuel Sarpong},
	month = mar,
	year = {2024},
	pages = {200303},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\D56UPA4A\\Nyarko et al. - 2024 - Forensic detection of heterogeneous activity in data using deep learning methods.pdf:application/pdf},
}

@article{monkman_role_2025,
	title = {The role of cats in human {DNA} transfer},
	volume = {74},
	issn = {18724973},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1872497324001285},
	doi = {10.1016/j.fsigen.2024.103132},
	abstract = {Domestic animals, such as cats and dogs, are present in the majority of Australian households. Recently, questions regarding the possibility that domestic animals can serve as silent witnesses, from whom evidence can be collected, or act as vectors of contamination and transfer, have started to be raised. Yet, little is known regarding the transfer and prevalence of human DNA to and from cats. This study investigated if cats are reservoirs and vectors for human DNA transfer. Twenty cats from 15 households were sampled from 4 different areas (head (fur), back (fur), left (skin) and right (fur)) to obtain information on the background DNA that may be found on an animal. Further, transfer of human DNA to and from an animal, after a short patting contact, was tested. Human DNA was found to be prevalent on all cats. Of the areas sampled, most DNA was collected from the top of the fur from the back followed by the head and right/fur. No or very low quantities of human DNA was recovered from the left (skin) area. Most of the human DNA originated from the owners, but DNA from others was also often present (47 \% of samples). Further, the transfer tests demonstrated that human DNA transferred readily to (detected in 45 \% of samples) and from (detected in 80 \% of samples) cats during patting. These results show that animals can act as reservoirs of human DNA and vectors for human DNA transfer that may need to be considered during evaluative DNA reporting. Furthermore, if an interaction between an animal and a perpetrator is suspected, consideration should be given to collecting DNA evidence from suspected contact areas on an animal.},
	language = {en},
	urldate = {2024-11-28},
	journal = {Forensic Science International: Genetics},
	author = {Monkman, Heidi and Van Oorschot, Roland A.H. and Goray, Mariya},
	month = jan,
	year = {2025},
	pages = {103132},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\IQX6MUZB\\Monkman et al. - 2025 - The role of cats in human DNA transfer.pdf:application/pdf},
}

@article{samie_driver_2025,
	title = {Driver or passenger? {Use} of a {Bayesian} {Network} for the evaluation of {DNA} results in a fatal car accident},
	volume = {74,},
	issn = {1872-4973},
	url = {https://doi.org/10.1016/j.fsigen.2024.103166.},
	number = {103166},
	journal = {Forensic Science International: Genetics},
	author = {Samie, Lydie and Champod, Christophe and Hicks, Tacha and Delemont, Séverine and Castella, Vincent},
	year = {2025},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\EJKEFU3J\\Samie et al 2024.pdf:application/pdf},
}

@misc{noauthor_introbayesstats_nodate,
	title = {{IntroBayesStats}},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\8D3J7XLE\\IntroBayesStats.pdf:application/pdf},
}

@misc{noauthor_hugin_nodate,
	title = {{HUGIN} {API} {REFERENCE} {MANUAL}},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\XLMQYB3Q\\HUGIN API REFERENCE MANUAL.pdf:application/pdf},
}

@article{aitken_probabilistic_1989,
	title = {Probabilistic reasoning in evidential assessment},
	volume = {29},
	issn = {0015-7368},
	url = {https://www.sciencedirect.com/science/article/pii/S0015736889732709},
	doi = {10.1016/S0015-7368(89)73270-9},
	abstract = {Computational procedures, based on probabilities for incorporating and assessing evidence obtained in the course of a criminal investigation, are described. These procedures depend on a network approach. Suspects and individual pieces of evidence are the nodes of the network. Causal connections are the links of the network. Various applications of the procedures are discussed in the context of one artificial example.},
	number = {5},
	urldate = {2024-11-29},
	journal = {Journal of the Forensic Science Society},
	author = {Aitken, C. G. G. and Gammerman, A. J.},
	month = sep,
	year = {1989},
	keywords = {Assessment of evidence, Evidence propagation, Expert systems, Networks, Probabilistic reasoning, Subjective beliefs},
	pages = {303--316},
	file = {ScienceDirect Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\U8HJSL4K\\S0015736889732709.html:text/html},
}

@article{philip_dawid_using_1997,
	title = {Using a {Graphical} {Method} to {Assist} the {Evaluation} of {Complicated} {Patterns} of {Evidence}},
	volume = {42},
	issn = {0022-1198},
	url = {https://doi.org/10.1520/JFS14102J},
	doi = {10.1520/JFS14102J},
	abstract = {The forensic scientist often faces the task of interpreting patterns of evidence which involve many variables. Combining different items of evidence within a complex framework of circumstances requires logical powers of reasoning and this can be assisted by formal methods. We discuss one such method which, as has already been pointed out by Aitken and Gammerman (1), offers considerable potential for creating probabilistic expert systems to assist in evidence interpretation. In particular, we show how the method, which is based on a directed acyclic graph, enables dependencies between different aspects of the evidence to be considered. The discussion is based on an imaginary case example.},
	number = {2},
	urldate = {2024-11-29},
	journal = {Journal of Forensic Sciences},
	author = {Philip Dawid, A and Evett, IW},
	month = mar,
	year = {1997},
	pages = {226--231},
	file = {Full Text:C\:\\Users\\jstacey\\Zotero\\storage\\YRVSPAVK\\Philip Dawid and Evett - 1997 - Using a Graphical Method to Assist the Evaluation of Complicated Patterns of Evidence.pdf:application/pdf;Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\HJD7RJC8\\Using-a-Graphical-Method-to-Assist-the-Evaluation.html:text/html},
}

@techreport{nist_expert_working_group_on_human_factors_in_forensic_dna_interpretation_forensic_2024,
	address = {Gaithersburg, MD},
	title = {Forensic {DNA} {Interpretation} and {Human} {Factors}: {Improving} the {Practice} {Through} a {Systems} {Approach}},
	shorttitle = {Forensic {DNA} {Interpretation} and {Human} {Factors}},
	url = {https://nvlpubs.nist.gov/nistpubs/ir/2024/NIST.IR.8503.pdf},
	language = {en},
	number = {NIST IR 8503},
	urldate = {2024-11-29},
	institution = {National Institute of Standards and Technology},
	author = {NIST Expert Working Group on Human Factors in Forensic DNA Interpretation},
	year = {2024},
	doi = {10.6028/NIST.IR.8503},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\RDW3BPKX\\Taylor - 2024 - Forensic DNA Interpretation and Human Factors Improving the Practice Through a Systems Approach.pdf:application/pdf},
}

@techreport{european_network_of_forensic_science_institutes_enfsi_2016,
	title = {{ENFSI} {Guideline} for {EvaluatIve} {Reporting} in {Forensic} {Science}},
	url = {https://enfsi.eu/wp-content/uploads/2016/09/m1_guideline.pdf},
	author = {{European Network of Forensic Science Institutes}},
	year = {2016},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\9S7PA8CP\\m1_guideline.pdf:application/pdf},
}

@misc{NRGDStandard-2024,
	type = {{DNA} analysis and interpretation},
	title = {{DNA} analysis and interpretation - {DNA} {Assessment} {Framework}},
	url = {https://www.nrgd.nl/deskundigheidsgebieden/dna-analyse-en-interpretatie},
	author = {Netherlands Register for Court Experts (NRGD)},
	month = mar,
	year = {2022},
}

@article{hicks_logical_2022,
	title = {A {Logical} {Framework} for {Forensic} {DNA} {Interpretation}},
	volume = {13},
	issn = {2073-4425},
	url = {https://www.mdpi.com/2073-4425/13/6/957},
	doi = {10.3390/genes13060957},
	abstract = {The forensic community has devoted much effort over the last decades to the development of a logical framework for forensic interpretation, which is essential for the safe administration of justice. We review the research and guidelines that have been published and provide examples of how to implement them in casework. After a discussion on uncertainty in the criminal trial and the roles that the DNA scientist may take, we present the principles of interpretation for evaluative reporting. We show how their application helps to avoid a common fallacy and present strategies that DNA scientists can apply so that they do not transpose the conditional. We then discuss the hierarchy of propositions and explain why it is considered a fundamental concept for the evaluation of biological results and the differences between assessing results given propositions that are at the source level or the activity level. We show the importance of pre-assessment, especially when the questions relate to the alleged activities, and when transfer and persistence need to be considered by the scientists to guide the court. We conclude with a discussion on statement writing and testimony. This provides guidance on how DNA scientists can report in a balanced, transparent, and logical way.},
	number = {6},
	journal = {Genes},
	author = {Hicks, Tacha and Buckleton, John and Castella, Vincent and Evett, Ian and Jackson, Graham},
	year = {2022},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\I8H6WTP9\\Hicks et al. - 2022 - A Logical Framework for Forensic DNA Interpretation.pdf:application/pdf},
}

@article{ali_how_2024,
	title = {How to best assess shedder status: a comparison of popular shedder tests},
	issn = {1437-1596},
	shorttitle = {How to best assess shedder status},
	url = {https://doi.org/10.1007/s00414-024-03351-8},
	doi = {10.1007/s00414-024-03351-8},
	abstract = {“Shedder status” describes the inherent variation between individuals to leave touch DNA on a surface through direct contact. Depending on the amount and quality of DNA or cellular deposition, individuals are typically deemed high, intermediate, or low shedders. Although many shedder tests have been described, variability in study design and categorisation criteria has limited the ability of researchers to accurately compare results, as well as accrue the necessary population data. As activity level reporting becomes more common, the need for reliable and standardised testing increases. To assess reproducibility, this study compared shedder status data generated by six participants using three different shedder tests, as modified from the literature. This involved DNA quantification and profiling of a handprint made on a glass plate, DNA quantification and profiling of a grip mark made on a plastic conical tube, and cell scoring of a Diamond™ Dye-stained fingermark. All participants washed and dried their hands fifteen minutes before each deposit. To assess the impact of behaviour on shedder designation, participants either refrained from activity or went about their daily tasks during this wait. The shedder status of participants changed between tests, as DNA-based testing often generated lower shedder statuses than cell scores. Further, when different categorisation methods were applied to a single test, intra-person variability increased as the number of shedder designations increased from two (low/high) to five (low/low-intermediate/intermediate/intermediate-high/high). Moving forward, the utilisation of a single shedder test and standardised categorisation criteria is needed to employ shedder testing in forensic casework.},
	language = {en},
	urldate = {2024-12-11},
	journal = {International Journal of Legal Medicine},
	author = {Ali, Darya and van Oorschot, Roland A. H. and Linacre, Adrian and Goray, Mariya},
	month = nov,
	year = {2024},
	keywords = {Touch DNA, Diamond dye, Handheld tube, Handprint, Shedder status, Shedder test},
	file = {Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\AYF4Y4D8\\Ali et al. - 2024 - How to best assess shedder status a comparison of popular shedder tests.pdf:application/pdf},
}

@article{taylor_using_2025,
	title = {Using an interaction timeline to investigate factors related to shedder status},
	volume = {76},
	issn = {1872-4973},
	url = {https://www.sciencedirect.com/science/article/pii/S1872497324002011},
	doi = {10.1016/j.fsigen.2024.103205},
	abstract = {A major factor that influences DNA transfer is the propensity of individuals to ‘shed’ DNA, commonly referred to as their ‘shedder status’. In this work we provide a novel method to analyse and interrogate DNA transfer data from a largely uncontrolled study that tracks the movements and actions of a group of individuals over the course of an hour. By setting up a model that provides a simplistic description of the world, parameters within the model that represent properties of interest can be iteratively refined until the model can sufficiently describe a set of final DNA observations. Because the model describing reality can be constructed and parametrised in any desired configuration, aspects that may be difficult to traditionally test together can be investigated. To that end, we use a 60-min timeline of activity between four individuals and use DNA profiling results from objects taken at the conclusion of the hour to investigate factors that may affect shedder status. We simultaneously consider factors of: the amount of DNA transferred per contact, the rate of self-DNA regeneration, the capacity of hands to hold DNA, and the rate of non-self-DNA removal, all of which may ultimately contribute to someone’s shedder status.},
	urldate = {2024-12-11},
	journal = {Forensic Science International: Genetics},
	author = {Taylor, Duncan and Cahill, Amy and van Oorschot, Roland A. H. and Volgin, Luke and Goray, Mariya},
	month = mar,
	year = {2025},
	keywords = {MCMC, DNA transfer, Trace DNA, Persistence, Shedder status},
	pages = {103205},
	file = {ScienceDirect Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\SSPAK3JE\\S1872497324002011.html:text/html},
}

@techreport{johnson_tracey_l_national_2024,
	title = {National {Institute} of {Justice} {Research} {Report}: {Introducing} the {NIJ} {Forensic} {Intelligence} {Framework}: {Pillars} and {Guiding} {Principles} for {Successful} {Implementation}},
	url = {https://www.ojp.gov/pdffiles1/nij/309128.pdf},
	language = {en},
	number = {NCJ 309128},
	institution = {United States National Institute of Justice},
	author = {{Johnson, Tracey L.} and Lopez, Basia E. and {McGrath, Jonathan} and Hudgins, Caleb D. and {Pimsler, Meaghan L.} and White, Veronica},
	month = nov,
	year = {2024},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\B635KIA6\\Introducing the NIJ Forensic Intelligence Framework Pillars and Guiding Principles for Successful I.pdf:application/pdf},
}

@article{taylor_forensic_2024,
	title = {Forensic intelligence in {Australia} and {New} {Zealand}: {Status} and future directions},
	volume = {364},
	issn = {0379-0738},
	shorttitle = {Forensic intelligence in {Australia} and {New} {Zealand}},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073824002883},
	doi = {10.1016/j.forsciint.2024.112207},
	abstract = {Forensic science is underutilised. Operating models restricted to the support of court outcomes do not address core requirements of contemporary policing and public security, which are to disrupt criminal activity and prevent crime. Forensic intelligence (FORINT) is a principal means of enhancing the role of forensic science, emphasising proactivity and cross-case, cross-crime domain insights. To catalyse implementation, a FORINT Specialist Advisory Group (SAG) has been established under the Australia \& New Zealand Policing Advisory Agency (ANZPAA) National Institute of Forensic Science (NIFS). The SAG has established a concept of operations with four lines of effort – namely, to (i) promote awareness and consistency, (ii) shape the workforce, (iii) develop information management frameworks and (iv) guide operational implementation. This aims to shift Australia \& New Zealand from its present state (of substantial interagency variability) to a state of widespread, consistent and effective FORINT delivery in terms of: (a) culture, (b) information management, (c) education \& training, and (d) organisation \& operating environment. There are risks to implementing FORINT, in terms of privacy/confidentiality, bias/misinterpretation, and resource impost. However, these are not necessarily FORINT-specific, and solutions or mitigations exist. Moreover, these issues are outweighed by the risks of not implementing FORINT – such as a failure to reveal threats, missed opportunities, and poor resource efficiency. This paper is a call to arms. For policing and laboratories – now is the time to implement and entrench FORINT. For academia – now is the time to build foundations for this future. For supporting industries – now is the time to develop partnerships and facilitate delivery.},
	urldate = {2024-12-11},
	journal = {Forensic Science International},
	author = {Taylor, M. L. and Turbett, G. R. and Lee, J. and Sears, A.},
	month = nov,
	year = {2024},
	keywords = {Forensic science, Forensic intelligence, FORINT, Intelligence-led policing},
	pages = {112207},
	file = {ScienceDirect Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\5Y7I5ABV\\S0379073824002883.html:text/html},
}

@article{oatley_forensic_2020,
	title = {Forensic intelligence and the analytical process},
	volume = {10},
	copyright = {© 2020 Wiley Periodicals, Inc.},
	issn = {1942-4795},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1354},
	doi = {10.1002/widm.1354},
	abstract = {A review was undertaken of the developments made with integrating forensic evidence into the analytical process to support police investigations. Evidence such as DNA, fingerprints, fibers, accelerants, tyre marks, and so forth, can support to differing degrees the various working theories or hypotheses about the nature of the alleged crime, the persons of interest and the modus operandi. Investigators however, either forensic or detective, bring various biases to evidence capture and analysis, biases which are better understood in the intelligence community. Structured analytical techniques have a long history in intelligence analysis, for example analysis of competing hypotheses, which serves several purposes: information sharing, clarity of communication, and to highlight the common forms of bias brought to bear in an investigation. We illustrate the representation of links based on traces and intelligence, and how these can be stored in databases permitting better “reasoning” with evidence. We also present some recommendations for integration of forensic intelligence into the investigative analytic process and review information systems in this area. This article is categorized under: Fundamental Concepts of Data and Knowledge {\textgreater} Human Centricity and User Interaction Application Areas {\textgreater} Society and Culture Fundamental Concepts of Data and Knowledge {\textgreater} Knowledge Representation},
	language = {en},
	number = {3},
	urldate = {2024-12-11},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Oatley, Giles and Chapman, Brendan and Speers, James},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1354},
	keywords = {forensic, intelligence, links, reasoning},
	pages = {e1354},
	file = {Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\J9I6ULI9\\widm.html:text/html},
}

@article{miranda_is_2024,
	title = {Is it time for a unified forensic science taxonomy?},
	volume = {365},
	issn = {03790738},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0379073824003347},
	doi = {10.1016/j.forsciint.2024.112252},
	abstract = {Prompted by disparate data highlighted in responses to a Sydney Declaration survey, this paper examines how forensic science ‘disciplines’ are defined and described by various professional organizations. A considerable degree of disagreement in taxonomic organization of forensic science ‘disciplines’ was uncovered. This paper suggests that the global forensic science community come together to develop and implement a standard taxonomic system for defining, describing and classifying the disciplines/subdisciplines within forensic science. Such uniform taxonomic structure could serve to align education and awareness while at the same time contributing to the goal of professionalization. This taxonomy should be built on a foundation based on underlying forensic science philosophies and principles, such as those delineated in the Sydney Declaration.},
	language = {en},
	urldate = {2024-12-11},
	journal = {Forensic Science International},
	author = {Miranda, Michelle D. and Niceberg, Ciaran H. and Roux, Claude},
	month = dec,
	year = {2024},
	pages = {112252},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\UYG632DG\\Miranda et al. - 2024 - Is it time for a unified forensic science taxonomy.pdf:application/pdf},
}

@misc{new_zealand_ministry_of_health_health_2024,
	type = {web application},
	title = {Health {Advisory} and {Regulatory} {Platform} ({HARP})},
	url = {https://vaping.harp.health.nz/},
	urldate = {2024-12-16},
	author = {New Zealand Ministry of Health},
	year = {2024},
}

@misc{new_zelanad_government_smokefree_2020,
	title = {Smokefree {Environments} and {Regulated} {Products} ({Vaping}) {Amendment} {Act}},
	author = {New Zelanad Government},
	year = {2020},
}

@misc{new_zealand_government_smokefree_2023,
	title = {Smokefree {Environments} and {Regulated} {Products} {Amendment} {Regulations} 2023},
	author = {New Zealand Government},
	year = {2023},
}

@article{robertson_chain_2024,
	title = {Chain event graphs for assessing activity-level propositions in forensic science in relation to drug traces on banknotes},
	volume = {23},
	issn = {1470-8396},
	url = {https://doi.org/10.1093/lpr/mgae013},
	doi = {10.1093/lpr/mgae013},
	abstract = {Graphical models can be used to compare support given by evidence to propositions put forward by competing parties during court proceedings. Such models can also be used to evaluate support for activity-level propositions, that is, propositions referring to the nature of activities associated with evidence and how this evidence came to be at a crime scene. Graphical methods can be used to show different scenarios that might explain the evidence in a case and to distinguish between evidence requiring evaluation by a jury and quantifiable evidence from the crime scene. Such visual representations can be helpful for forensic practitioners, police, and lawyers who may need to assess the value that different pieces of evidence make to their arguments. In this article, we demonstrate how chain event graphs (CEGs) can be applied to a drug trafficking case. We show how different evidence (i.e. expert judgement and data from a crime scene) can be combined using a CEG and show how the hierarchical model deriving from the graph can be used to evaluate the degree of support for different activity-level propositions in the case. We also develop a modification of the standard CEG to simplify its use in forensic applications.},
	number = {1},
	urldate = {2024-12-17},
	journal = {Law, Probability and Risk},
	author = {Robertson, Gail and Wilson, Amy L and Smith, Jim Q},
	month = jan,
	year = {2024},
	pages = {mgae013},
	file = {Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\8TLRU3ZQ\\Robertson et al. - 2024 - Chain event graphs for assessing activity-level propositions in forensic science in relation to drug.pdf:application/pdf;Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\Y2MJKFCF\\7845965.html:text/html},
}

@article{williams_embracing_2021,
	title = {Embracing likelihood ratios and highlighting the principles of forensic interpretation},
	volume = {3},
	issn = {2665-9107},
	url = {https://www.sciencedirect.com/science/article/pii/S2665910721000402},
	doi = {10.1016/j.fsir.2021.100209},
	abstract = {Due to the police commissioning of expert forensic services worldwide, the police may specify the level of forensic service they need; thus, forensic scientists can be put under pressure to present scientific findings as quickly as possible and in simple and definitive terms. Frequently, this means reporting forensics results out of the context of the case, as forensic practitioners may not be given relevant case information. However, to do so can be misleading, inaccurate and potentially lead to miscarriages of justice. Thus, whilst well-intentioned, efforts to accelerate and simplify forensic science evidence can undermine the justice system. However, providing that all forensic scientists and practitioners follow three basic forensic interpretation principles based on the formulation of the likelihood ratio component of Bayes Theorem approach, and are given case context, then the chances of miscarriages of justice arising from forensic science should be minimised. Principle \#1: Always consider at least one alternative hypothesis. Principle \#2: Always consider the probability of the evidence given the proposition and not the probability of the proposition given the evidence. Principle \#3: Always consider the framework of circumstance. The expression of a likelihood ratio lies at the core of a pre-assessment that, if used correctly, should minimise bias in the forensic investigation. Several disciplines use likelihood ratios to interpret their findings and place them in the context of the case. There are still several forensic science areas where this is not undertaken, and therefore they may be a source of concern.},
	urldate = {2024-12-17},
	journal = {Forensic Science International: Reports},
	author = {Williams, Graham A. and Maskell, Peter D.},
	month = jul,
	year = {2021},
	keywords = {Likelihood ratio, Bayes’ theorem, Bias, Forensic interpretation},
	pages = {100209},
}

@article{cadola_occurrence_2021,
	title = {The occurrence and genesis of transfer traces in forensic science: a structured knowledge database},
	volume = {54},
	issn = {0008-5030},
	shorttitle = {The occurrence and genesis of transfer traces in forensic science},
	url = {https://doi.org/10.1080/00085030.2021.1890941},
	doi = {10.1080/00085030.2021.1890941},
	abstract = {While forensic science is generally focused on associating a trace to its source, trace’s relevance is best addressed at the activity responsible for its genesis. Recurring studies show the potential of the Bayesian approach in order to address activity level’s propositions in a rational and transparent manner. The objective of this research is to identify and review literature and models for transfer traces to create a relevant database for activity level interpretation. As of December 17th, 2020, a thorough review of 2042 existing peer-reviewed publications and studies concerning transfer traces has been conducted. The data have been classified by different criteria such as, the type of trace, year of publication, and type of study (i.e. population). Every publication has been critically analyzed according to its relevance, among others, with regards to a Canadian environment. This process identified research that needed to be completed. A database collecting publication and data on activity level assessment has been created. This database is available for consultation to laboratories, police agencies, lawyers and universities, thus contributing to the transparency of the expert opinion.},
	number = {2},
	urldate = {2024-12-17},
	journal = {Canadian Society of Forensic Science Journal},
	author = {Cadola, Liv and Charest, Marina and Lavallée, Catherine and Crispino, Frank},
	month = apr,
	year = {2021},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00085030.2021.1890941},
	keywords = {forensic science, Interpretation, activité, activity level, criminalistique, evidential value, force probante, interprétation, traces de transfert, transfer traces},
	pages = {86--100},
	file = {Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\LK9B5E6Y\\Cadola et al. - 2021 - The occurrence and genesis of transfer traces in forensic science a structured knowledge database.pdf:application/pdf},
}

@article{champod_dna_2013,
	title = {{DNA} transfer: informed judgment or mere guesswork?},
	volume = {4},
	issn = {1664-8021},
	shorttitle = {{DNA} transfer},
	url = {https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2013.00300/full},
	doi = {10.3389/fgene.2013.00300},
	abstract = {With the advances of analytical sensitivity, it is now possible to detect a DNA profile from minute quantity of DNA. It opens new investigative avenues (in cold cases for example), but also new interpretative challenges. Here, forensic scientists deal with items bearing DNA cellular material from areas showing no visible stain and have limited means to identify the nature of the body fluid involved. Such DNA cells can be considered as trace evidence that can be exchanged for reasons connected to the alleged facts under investigation (generally a direct transfer) but also following alternative and versatile ways (through secondary or tertiary transfer) that have no connection to the facts under investigation. The trace becomes an ubiquitous material that can be found for unconnected reasons. In addition, and especially with trace quantities of DNA, the debate in court is less focused on the issue of the source of the DNA (often the parties will not dispute it), but on the mechanisms whereby the biological material has been transferred (Taroni et al., 2013). In other words, the well-known territory of source level DNA statistics (see Buckleton et al. 2005, for example) does not help with the interpretation process, but the forensic scientist is invited to assess how likely it would be to observe this amount of DNA given various transfer mechanisms. The review by Meakin and Jamieson (2013) led them to conclude that the quantity of DNA or the quality of the profile cannot be used “to reliably infer the mode of transfer by which the DNA came to be on the surface of interest”.This rather complicated new landscape leads to two questions:(1) Is it the role of the scientist to offer guidance as to the probability of the DNA findings given various transfer mechanisms put forward by the parties depending on the case circumstances?(2) Can a forensic scientist robustly assess the probability of the DNA findings given alleged transfer scenarios in the current state of knowledge?Regarding the first, my view is that it is definitely the role of the forensic scientist to provide as much guidance to the trier of facts if the knowledge he/she may bring is outside the general knowledge of the court and relevant to the task at hand. Shying away from this duty on the ground that considerations regarding transfer of trace DNA is less known than source level DNA statistics is not acceptable. There is a risk with leaving the presence of DNA to be assessed by others, left to advocacy, when the scientist can bring decisive knowledge (let alone the papers reviewed by Meakin and Jamieson), including highlighting how complex the task may be. We want to avoid the simplistic line of argument that I have heard at times: “We have found DNA corresponding to the defendant on the trigger of firearm, hence he manipulated the gun”. It is crucial for a fair administration of justice that forensic scientists weigh their expectations of the amount of DNA recovered given both views. Hence scientists’ guidance is required when the consideration of transfer mechanisms, persistence and background levels of the material has a significant impact on the understanding of the alleged activities and requires expert knowledge. But to provide guidance, the scientist will need information regarding the alleged case circumstances from both prosecution and defence’s perspective. The duty may also require the scientist to highlight how little is known on transfer mechanisms and urge for a very careful assessment of the evidential contribution of the forensic findings, regardless of their strength with regards to the issue of the source itself. The absence of knowledge should not be an excuse for a guilty silence and for delegating the task to the fact finder without making explicit the complexity surrounding such an assessment.In relation to the second question, Risinger (2013) warns against the “abuse of the notion of subjective probability”, … “by simply making their best guess from experience when more should be required”. In contrast, courts (I will concentrate here on the jurisdiction of England and Wales) have recently given a lot of freedom or authority to DNA scientists to exercise their professional judgement even when limited or no published data were available. In R v Reed and Reed, the court ruled that in the context of the analysis of minute quantity of DNA, a reporting scientist is fully entitled to assess and weigh the relative merits of the possible mechanisms whereby cellular material can be exchanged. In that case the forensic scientist testified that, in her experience, it was highly unlikely that the appellants had innocently touched the knives and it was unrealistic that each appellant had passed their DNA to someone else who then transferred it to the pieces of plastic which were found at the victim’s address. The court while recognizing that the scientific knowledge on transferability was incomplete, ruled that enough reliability had been demonstrated when the scientist is asked to consider cases where more than 200 picograms of DNA had been recovered. The court however stressed that “care must be taken to guard against the dangers of that evaluation being tainted with the verisimilitude of scientific certainty”. The scientist is then authorised to comment on the probability of the forensic results given various transfer mechanisms as long as he/she makes it clear that we are dealing here with large uncertainty. This judgement led to a few commentaries. Jamieson (2011) highlighted the limited body of evidence represented by the few papers quoted by the court to support their opinion and warned against the view that the personal experience might override scientific research. A worry echoed in an editorial (Nic Da\&\#233;id, 2010) in Science and Justice following the next case against Weller.In R v Weller (a case involving the transfer of a reasonable quantity of DNA under the fingernails of the defendant), the defence appealed on the ground that knowledge regarding transfer and persistence mechanisms of DNA was not sufficient for experts to have been able to express an evaluation of the relative merit of the alleged activities. The Court of Appeal confirmed the positions taken in Reed and Reed. Given the difficulties of conducting systematically experiments replicating the circumstances in a particular case, the court recognized that a scientist is fully entitled to express a professional opinion on his/her expectation of DNA quantities given each mechanism envisaged by the court if the scientist has sufficient casework day-to-day experience. Jamieson and Meakin (2010) expressed their concerns after Weller seeing courts in England and Wales putting more trust in claimed experience than in published, peer-reviewed, publications. Rudin and Inman (2010a) also insisted on the fact that bald experience is not an acceptable substitute for experimental data.Following these two cases, the Court of Appeal confirmed that view in subsequent rulings, not only in relation to consideration of DNA transfer but also of the sources of complex DNA mixtures. In R v Thomas, a DNA scientist invoked her 12 years experience (and some unpublished and undisclosed data) to suggest that in a three-person DNA mixture, there was a low expectation of finding components matching all those of the appellant adventitiously. In R v Dlugosz, Pickering and MDS, the court, recognizing their extensive professional experience, allowed two DNA scientists to qualify the occurrence of alleles in a complex mixture corresponding to the defendant as a “rare” for one and “somewhat unusual” for the other. The qualitative opinion expressed by the scientists was offered as an acceptable substitute in cases where the mixture is too complex for a quantitative assessment. However, as pointed out by Evett and Pope (2013), “there is no scientific basis for this belief – no scientific literature provides a reliable methodology, scientists are not trained to make such assessments and there is no body of standards to support them. Casework experience is not a substitute.” One needs to assess the robustness of such qualitative opinion through a structured program of proficiency tests: it should not be based on casework data, but on DNA mixtures obtained under controlled conditions. Expressing qualitative judgments on the basis (or assumptions) of casework samples, without any calibration mechanism, is dangerous in my view. The expressed opinion could be the expression of nothing more than the ipse dixit of the expert.The Court of Appeal endorsed such a laissez faire approach drawing from a much larger jurisprudence applicable to expertise in general, with some decisions relating to other areas of forensic disciplines. For example, in R v Otway, involving gait analysis, and two other cases, namely R v Atkins and Atkins (face recognition) and R v T (footwear mark), the court recognized that an expert may express a qualitative opinion in the absence of quantitative (or statistical) supporting data as long as the subjective nature of the opinion and its foundation are transparently presented without giving more scientific weight to the judgment than it disserves. Edmond (2010) remains rightly sceptical with the approach of dressing an opinion with all the concessions of limitations, but still allowing it, when the real significance of the forensic findings remains simply unknown. In my view, what is critical, when it comes to offer expert opinions (in the present discussion regarding DNA transfer), is striking the appropriate balance between structured documented data (published or not) and unfettered personal opinion. Should these opinions be based in extenso on experience? My answer is clearly negative. I believe that experience constitutes a poor substitute to a systematic and structured acquisition of data. Any scientist offering views as to his/her expectations for the forensic findings under given case-related circumstances should be able to put forward documented sets of controlled experiments whose relevancy to the case under dispute can be argued. A further question is how many controlled experiments should be conducted and how close should they be to the alleged circumstances. In my view that question should be approached on a case-by-case basis using the adversarial mechanisms available to the parties. The major improvement here is that all parties can access and challenge the body of knowledge available to the expert proffering an opinion. As Rudin and Inman (2010a) indicated, the problem with experience only based opinions is that it cannot be challenged beyond the sterile opposition between mere opinions. Requiring the disclosure of structured data opens the route to a new type of debate regarding the relative merits of the assessments provided.We could legitimately ask, as did Rudin and Inman (2010b), whether or not forensic science has gone too far in terms of sensitivity, meaning that the risks associated with the analysis of irrelevant (meaning not associated with the criminal activities under investigation) items are too high. I believe that the problem lies more in the usage made by law enforcement authorities of such sensitive technologies. There are only gains in terms of investigative leads if we take advantage of sensitive techniques, but maybe these methods should be used only in the investigative phase, not as a basis for evidence relied on at trial. Highly sensitive DNA analysis offers extraordinary ways to enhance an investigation through the suggestion of potential named sources (through DNA databases) for the inquiry to consider. I am not calling for limiting such opportunities. However, moving from such investigative information towards elements of evidentiary purposes to be used in court requires very careful attention. It may well be the case that a decisive investigative information will not be brought to court because of the issues discussed above. This is not a failure of forensic science, but simply an appropriate and fair (re-)positioning of the scientific techniques within the criminal justice process.},
	language = {English},
	urldate = {2024-12-17},
	journal = {Frontiers in Genetics},
	author = {Champod, Christophe},
	month = dec,
	year = {2013},
	note = {Publisher: Frontiers},
	keywords = {Contact DNA, Weight of evidence, Evaluation, Expert Testimony, Forensic Science},
	file = {Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\VWF67FDG\\Champod - 2013 - DNA transfer informed judgment or mere guesswork.pdf:application/pdf},
}

@article{buckleton_when_2021,
	title = {When evaluating {DNA} evidence within a likelihood ratio framework, should the propositions be exhaustive?},
	volume = {50},
	issn = {1872-4973},
	url = {https://www.sciencedirect.com/science/article/pii/S1872497320301782},
	doi = {10.1016/j.fsigen.2020.102406},
	abstract = {We seek to develop a rational approach to forming propositions when little information is available from the outset, as this often happens in casework. If propositions used when evaluating evidence are not exhaustive (in the context of the case), then there is a theoretical risk that an LR greater than one may be associated with a proposition in the numerator that - if all meaningful propositions had been considered - would in fact have a lower posterior probability after consideration of the evidence. Ideally, all propositions should be considered. However, with multiple propositions, some terms will be larger than others and for simplification very small terms can be neglected without changing the order of magnitude of the value of the evidence (i.e. LR). Our analysis shows that mathematically a contributor’s DNA can be assumed to be present under both prosecution and alternative propositions (Hp and Ha) if there is a reasonable prior probability of their DNA being present and their inclusion is supported by the profile. This is because the terms associated to these sub-propositions will dominate our LR. For example, in the absence of specific information, when considering two persons of interest (POI) as potential contributors to a mixed DNA profile we suggest the assumption of one when examining the presence of the other, after checking that both collectively explain the profile well. This represents more meaningful propositions and allows better discrimination. Slooten and Caliebe have shown that the overall LR is the weighted average of LRs with the same number of contributors (NoC) under both propositions. The weights involve both an assessment of the probability of the crime scene DNA profile and the probability of this NoC given the background information.},
	urldate = {2024-12-18},
	journal = {Forensic Science International: Genetics},
	author = {Buckleton, John and Taylor, Duncan and Bright, Jo-Anne and Hicks, Tacha and Curran, James},
	month = jan,
	year = {2021},
	keywords = {exhaustive, Forensic DNA, likelihood ratio, propositions},
	pages = {102406},
	file = {Full Text:C\:\\Users\\jstacey\\Zotero\\storage\\IKSIQM26\\Buckleton et al. - 2021 - When evaluating DNA evidence within a likelihood ratio framework, should the propositions be exhaust.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\BIEAQWM7\\S1872497320301782.html:text/html},
}

@article{letendre_first_2021,
	title = {First lessons regarding the data analysis of physicochemical traces at activity level in {TTADB}},
	volume = {54},
	issn = {0008-5030},
	url = {https://doi.org/10.1080/00085030.2021.1899655},
	doi = {10.1080/00085030.2021.1899655},
	abstract = {If transfer traces inform about the source from which they originate, they also have an informative potential on their generating activity. To help practitioners interpret such traces and assess their evidential value at the activity level, this research aims at producing a structured knowledge base on physicochemical transfer traces here defined as ignitable liquids, cosmetics, explosives, narcotics and paints. Studies focusing on background, persistence, transfer and contamination assessment (e.g. washing machines, laboratory), mimicking operative situations (e.g. cross transfers of paint from crowbars onto door frames from a burglary), and covering different supports (e.g. textiles, plastics, doors) were privileged with a special concern for the Canadian relevancy. This research also helps highlighting gaps of knowledge in the overall literature, hence, new research to launch.},
	number = {3},
	urldate = {2024-12-18},
	journal = {Canadian Society of Forensic Science Journal},
	author = {Letendre, Heidi and Séguin, Karelle and Grenier, Annick and Mousseau, Vincent and Cadola, Liv and Crispino, Frank},
	month = jul,
	year = {2021},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00085030.2021.1899655},
	keywords = {interpretation, cosmetic, Cosmétique, Explosif, explosive, Ignitable liquids, Interprétation, Liquides inflammables, narcotic, Narcotique, paint, Peinture},
	pages = {139--156},
}

@article{ross_forensic_2025,
	title = {Forensic science: {Where} to from {Here}?},
	volume = {366},
	issn = {0379-0738},
	shorttitle = {Forensic science},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073824003670},
	doi = {10.1016/j.forsciint.2024.112285},
	abstract = {‘Where to from Here’ (WTFH) was the theme chosen for the 2023 meeting of the International Association of Forensic Sciences (IAFS). This theme reflects the fact that forensic science is evolving rapidly, not only within individual sub-disciplines but, critically, across the whole forensic science landscape. Identifying and predicting evolutionary change will enable a more focused and constructive future for forensic science. The IAFS meeting originally scheduled for 2020 was cancelled due to the global impact of the COVID-19 pandemic. However, on 18 May 2021, an IAFS Virtual Event launched the Sydney Declaration as an integral part of the WTFH initiative. The Sydney Declaration articulates a definition and seven principles for forensic science and provides a much-needed platform for forensic science into the future. It is aspirational, not focused on organisations, techniques or protocols, and provides a shared understanding of forensic science and its principles. The 2023 IAFS meeting built on the Sydney Declaration, with five themes developed as the basis for eliciting information from delegates related to the WTFH concept. The themes were: (i) integration and harmonisation; (ii) digital transformation; (iii) research; (iv) education and training; and (v) technology. Information across these themes was gathered via short, sharply focused panel discussions in the final session of each of the 22 disciplines represented at the meeting. In addition, there was a panel-based seminar on the Sydney Declaration and a panel-based plenary session on the conference theme. Meeting delegates were also able to provide their thoughts during the meeting, and for a two-week post-conference window, via a dedicated meeting app. Information from all of these sources has been collated to provide a consolidated WTFH landscape for forensic science.},
	urldate = {2024-12-18},
	journal = {Forensic Science International},
	author = {Ross, Alastair and Lennard, Chris and Roux, Claude},
	month = jan,
	year = {2025},
	keywords = {Forensic science, Collective voice, Cultural shift, Sydney Declaration},
	pages = {112285},
}

@article{roux_sydney_2022,
	title = {The {Sydney} declaration – {Revisiting} the essence of forensic science through its fundamental principles},
	volume = {332},
	issn = {0379-0738},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073822000123},
	doi = {10.1016/j.forsciint.2022.111182},
	abstract = {Unlike other more established disciplines, a shared understanding and broad acceptance of the essence of forensic science, its purpose, and fundamental principles are still missing or mis-represented. This foundation has been overlooked, although recognised by many forensic science forefathers and seen as critical to this discipline's advancement. The Sydney Declaration attempts to revisit the essence of forensic science through its foundational basis, beyond organisations, technicalities or protocols. It comprises a definition of forensic science and seven fundamental principles that emphasise the pivotal role of the trace as a vestige, or remnant, of an investigated activity. The Sydney Declaration also discusses critical features framing the forensic scientist’s work, such as context, time asymmetry, the continuum of uncertainties, broad scientific knowledge, ethics, critical thinking, and logical reasoning. It is argued that the proposed principles should underpin the practice of forensic science and guide education and research directions. Ultimately, they will benefit forensic science as a whole to be more relevant, effective and reliable.},
	urldate = {2024-12-18},
	journal = {Forensic Science International},
	author = {Roux, Claude and Bucht, Rebecca and Crispino, Frank and De Forest, Peter and Lennard, Chris and Margot, Pierre and Miranda, Michelle D. and NicDaeid, Niamh and Ribaux, Olivier and Ross, Alastair and Willis, Sheila},
	month = mar,
	year = {2022},
	keywords = {Trace, Clues, Context, Critical thinking, Ethics, Logical reasoning, Principles, Signs, Time asymmetry, Uncertainties},
	pages = {111182},
	file = {Full Text:C\:\\Users\\jstacey\\Zotero\\storage\\8NWEYLVD\\Roux et al. - 2022 - The Sydney declaration – Revisiting the essence of forensic science through its fundamental principl.pdf:application/pdf},
}

@article{martire_understanding_2024,
	title = {Understanding ‘error’ in the forensic sciences: {A} primer},
	volume = {8},
	issn = {2589871X},
	shorttitle = {Understanding ‘error’ in the forensic sciences},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2589871X24000172},
	doi = {10.1016/j.fsisyn.2024.100470},
	abstract = {This paper distils seven key lessons about ‘error’ from a collaborative webinar series between practitioners at Victoria Police Forensic Services Department and academics. It aims to provide the common understanding of error necessary to foster interdisciplinary dialogue, collaboration and research. The lessons underscore the inevitability, complexity and subjectivity of error, as well as opportunities for learning and growth. Ultimately, we argue that error can be a potent tool for continuous improvement and accountability, enhancing the reliability of forensic sciences and public trust. It is hoped the shared understanding provided by this paper will support future initiatives and funding for collaborative developments in this vital domain.},
	language = {en},
	urldate = {2025-01-13},
	journal = {Forensic Science International: Synergy},
	author = {Martire, Kristy A. and Chin, Jason M. and Davis, Carolyn and Edmond, Gary and Growns, Bethany and Gorski, Stacey and Kemp, Richard I. and Lee, Zara and Verdon, Christopher M. and Jansen, Gabrielle and Lang, Tanya and Neal, Tess M.S. and Searston, Rachel A. and Slocum, Joshua and Summersby, Stephanie and Tangen, Jason M. and Thompson, Matthew B. and Towler, Alice and Watson, Darren and Werrett, Melissa V. and Younan, Mariam and Ballantyne, Kaye N.},
	year = {2024},
	pages = {100470},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\GF7SVGDA\\Martire et al. - 2024 - Understanding ‘error’ in the forensic sciences A primer.pdf:application/pdf},
}

@misc{blackmun_daubert_1993,
	title = {Daubert v. {Merrell} {Dow} {Pharmaceuticals} {Inc}.},
	author = {Blackmun, Harry A},
	year = {1993},
}

@article{leonetti_ensuring_2024,
	title = {Ensuring the reliability of evidence in the {New} {Zealand} criminal courts: {The} admissibility of forensic science},
	volume = {53},
	issn = {1473-7795},
	shorttitle = {Ensuring the reliability of evidence in the {New} {Zealand} criminal courts},
	url = {https://doi.org/10.1177/14737795241237799},
	doi = {10.1177/14737795241237799},
	abstract = {This article presents a systematic and critical assessment of the reliability of forensic science in New Zealand. It documents the types of forensic-science being offered in criminal cases, the party presenting the evidence, the experts’ affiliations, how often there are challenges to the admissibility of the expert evidence and their timing in the proceedings, how often experts rely upon the uniqueness assumption, and how often experts testify to an individualised identification or ‘match’ of a source of forensic evidence. It finds that several of the common forensic disciplines in the criminal justice system in New Zealand have been the subject of critique and criticism internationally, the most common source of expert evidence was presented by the prosecution and provided by institutional police laboratories, and in most cases the forensic expert testified either to the uniqueness assumption or to an individualised match determination. It concludes that the New Zealand Parliament should amend the Evidence Act 2006 to require a demonstration of foundational validity and as-applied reliability as a precondition to the admissibility of any purported scientific evidence.},
	language = {en},
	number = {4},
	urldate = {2025-01-13},
	journal = {Common Law World Review},
	author = {Leonetti, Carrie},
	month = dec,
	year = {2024},
	note = {Publisher: SAGE Publications Ltd},
	pages = {197--222},
	file = {SAGE PDF Full Text:C\:\\Users\\jstacey\\Zotero\\storage\\7WNJ2QS3\\Leonetti - 2024 - Ensuring the reliability of evidence in the New Zealand criminal courts The admissibility of forens.pdf:application/pdf},
}

@article{taylor_how_2020,
	title = {How can courts take into account the uncertainty in a likelihood ratio?},
	volume = {48},
	issn = {1872-4973},
	url = {https://www.sciencedirect.com/science/article/pii/S1872497320301344},
	doi = {10.1016/j.fsigen.2020.102361},
	abstract = {As legal practitioners and courts become more aware of scientific methods and evidence evaluation, they are demanding measures of the reliability of expert opinion. In particular, there are calls for error rates to accompany opinion evidence in comparative forensic sciences. While error rates or confidence intervals can be useful for those disciplines that claim to identify the source of a trace, the call for these statistical tools has extended to sciences that present opinions in the form of a likelihood ratio. In this article we argue against presenting both a likelihood ratio and numerical measures of its uncertainty. We explain how the LR already encapsulates uncertainty. Instead we consider how sensitivity analyses can be used to guide the presentation of LRs that are informative to the court and not unfair to defendants.},
	urldate = {2025-01-13},
	journal = {Forensic Science International: Genetics},
	author = {Taylor, Duncan and Balding, David},
	month = sep,
	year = {2020},
	keywords = {Likelihood ratio, Evidence evaluation, Error rates, Uncertainty},
	pages = {102361},
	file = {ScienceDirect Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\2ZZ2SDSB\\S1872497320301344.html:text/html},
}

@article{hicks_importance_2015,
	title = {The importance of distinguishing information from evidence/observations when formulating propositions},
	volume = {55},
	issn = {13550306},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S135503061500091X},
	doi = {10.1016/j.scijus.2015.06.008},
	language = {en},
	number = {6},
	urldate = {2025-01-14},
	journal = {Science \& Justice},
	author = {Hicks, T. and Biedermann, A. and De Koeijer, J.A. and Taroni, F. and Champod, C. and Evett, I.W.},
	month = dec,
	year = {2015},
	pages = {520--525},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\XF75AAGG\\Hicks et al. - 2015 - The importance of distinguishing information from evidenceobservations when formulating proposition.pdf:application/pdf},
}

@article{taroni_dismissal_2016,
	title = {Dismissal of the illusion of uncertainty in the assessment of a likelihood ratio},
	volume = {15},
	issn = {1470-8396, 1470-840X},
	url = {https://academic.oup.com/lpr/article-lookup/doi/10.1093/lpr/mgv008},
	doi = {10.1093/lpr/mgv008},
	language = {en},
	number = {1},
	urldate = {2025-01-14},
	journal = {Law, Probability and Risk},
	author = {Taroni, Franco and Bozza, Silvia and Biedermann, Alex and Aitken, Colin},
	month = mar,
	year = {2016},
	pages = {1--16},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\LCYFFZ48\\Taroni et al. - 2016 - Dismissal of the illusion of uncertainty in the assessment of a likelihood ratio.pdf:application/pdf},
}

@techreport{rss_statistics_and_the_law_section_healthcare_2022,
	title = {Healthcare serial killer or coincidence? {Statistical} {Issues} in investigation of suspected medical misconduct},
	url = {https://rss.org.uk/RSS/media/File-library/News/2022/Report_Healthcare_serial_killer_or_coincidence_statistical_issues_in_investigation_of_suspected_medical_misconduct_Sept_2022_FINAL.pdf},
	urldate = {2025-01-15},
	institution = {Royal Statistical Society},
	author = {RSS Statistics {and} the Law Section},
	month = sep,
	year = {2022},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\U6U7ZKWI\\Healthcare serial killer or coincidence Statistical Issues in investigation of suspected medical mi.pdf:application/pdf},
}

@article{dror_contextual_2006,
	title = {Contextual information renders experts vulnerable to making erroneous identifications},
	volume = {156},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {03790738},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0379073805005876},
	doi = {10.1016/j.forsciint.2005.10.017},
	language = {en},
	number = {1},
	urldate = {2025-01-15},
	journal = {Forensic Science International},
	author = {Dror, Itiel E. and Charlton, David and Péron, Ailsa E.},
	month = jan,
	year = {2006},
	pages = {74--78},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\BCVXVSY4\\Dror et al. - 2006 - Contextual information renders experts vulnerable to making erroneous identifications.pdf:application/pdf},
}

@article{buckleton_extending_2024,
	title = {Extending the discussion on inconsistency in forensic decisions and results},
	volume = {69},
	copyright = {© 2024 American Academy of Forensic Sciences.},
	issn = {1556-4029},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1556-4029.15558},
	doi = {10.1111/1556-4029.15558},
	abstract = {The subject of inter- and intra-laboratory inconsistency was recently raised in a commentary by Itiel Dror. We re-visit an inter-laboratory trial, with which some of the authors of this current discussion were associated, to diagnose the causes of any differences in the likelihood ratios (LRs) assigned using probabilistic genotyping software. Some of the variation was due to different decisions that would be made on a case-by-case basis, some due to laboratory policy and would hence differ between laboratories, and the final and smallest part was the run-to-run difference caused by the Monte Carlo aspect of the software used. However, the net variation in LRs was considerable. We believe that most laboratories will self-diagnose the cause of their difference from the majority answer and in some, but not all instances will take corrective action. An inter-laboratory exercise consisting of raw data files for relatively straightforward mixtures, such as two mixtures of three or four persons, would allow laboratories to calibrate their procedures and findings.},
	language = {en},
	number = {4},
	urldate = {2025-01-15},
	journal = {Journal of Forensic Sciences},
	author = {Buckleton, John and Bright, Jo-Anne and Taylor, Duncan and Curran, James and Kalafut, Tim},
	year = {2024},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1556-4029.15558},
	keywords = {forensic DNA analysis, inter-laboratory consistency, quality assurance},
	pages = {1125--1137},
	file = {Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\53BWW3IQ\\Buckleton et al. - 2024 - Extending the discussion on inconsistency in forensic decisions and results.pdf:application/pdf},
}

@article{swofford_probabilistic_2022,
	title = {Probabilistic reporting and algorithms in forensic science: {Stakeholder} perspectives within the {American} criminal justice system},
	volume = {4},
	issn = {2589-871X},
	shorttitle = {Probabilistic reporting and algorithms in forensic science},
	url = {https://www.sciencedirect.com/science/article/pii/S2589871X22000055},
	doi = {10.1016/j.fsisyn.2022.100220},
	abstract = {In recent years, there have been efforts to promote probabilistic reporting and the use of computational algorithms across several forensic science disciplines. Reactions to these efforts have been mixed—some stakeholders argue they promote greater scientific rigor whereas others argue that the opacity of algorithmic tools makes it challenging to meaningfully scrutinize the evidence presented against a defendant resulting from these systems. Consequently, the forensic community has been left with no clear path to navigate these concerns as each proposed approach has countervailing benefits and risks. To explore these issues further and provide a foundation for a path forward, this study draws on semi-structured interviews with fifteen participants to elicit the perspectives of key criminal justice stakeholders, including laboratory managers, prosecutors, defense attorneys, judges, and other academic scholars, on issues related to interpretation and reporting practices and the use of computational algorithms in forensic science within the American legal system.},
	urldate = {2025-01-15},
	journal = {Forensic Science International: Synergy},
	author = {Swofford, H. and Champod, C.},
	month = jan,
	year = {2022},
	keywords = {Forensic science, Statistics, Algorithms, Pattern evidence, Probabilities},
	pages = {100220},
	file = {Full Text:C\:\\Users\\jstacey\\Zotero\\storage\\B5MRMJ5K\\Swofford and Champod - 2022 - Probabilistic reporting and algorithms in forensic science Stakeholder perspectives within the Amer.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\8YZGKARI\\S2589871X22000055.html:text/html},
}

@article{dror_subjectivity_2011,
	title = {Subjectivity and bias in forensic {DNA} mixture interpretation},
	volume = {51},
	issn = {1355-0306},
	doi = {10.1016/j.scijus.2011.08.004},
	abstract = {The objectivity of forensic science decision making has received increased attention and scrutiny. However, there are only a few published studies experimentally addressing the potential for contextual bias. Because of the esteem of DNA evidence, it is important to study and assess the impact of subjectivity and bias on DNA mixture interpretation. The study reported here presents empirical data suggesting that DNA mixture interpretation is subjective. When 17 North American expert DNA examiners were asked for their interpretation of data from an adjudicated criminal case in that jurisdiction, they produced inconsistent interpretations. Furthermore, the majority of 'context free' experts disagreed with the laboratory's pre-trial conclusions, suggesting that the extraneous context of the criminal case may have influenced the interpretation of the DNA evidence, thereby showing a biasing effect of contextual information in DNA mixture interpretation.},
	language = {eng},
	number = {4},
	journal = {Science \& Justice: Journal of the Forensic Science Society},
	author = {Dror, Itiel E. and Hampikian, Greg},
	month = dec,
	year = {2011},
	pmid = {22137054},
	keywords = {DNA Fingerprinting, Humans, Bias, Adult, Decision Making, Female, Forensic Genetics, Judgment, Male, Observer Variation, Professional Competence, Contextual influences, DNA interpretation, Forensic decision making, Human cognition},
	pages = {204--208},
	file = {ScienceDirect Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\XQMIMGNP\\S1355030611000967.html:text/html},
}

@article{winburn_objectivity_2021,
	title = {Objectivity is a myth that harms the practice and diversity of forensic science},
	volume = {3},
	issn = {2589-871X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8484737/},
	doi = {10.1016/j.fsisyn.2021.100196},
	abstract = {•
              Forensic science data are theory laden; pure scientific objectivity is a myth.
            
            
              •
              Upholding this myth marginalizes forensic scientists with subjective positionalities
            
            
              •
              Objectivity rhetoric is exclusive; ethical forensic science needs diverse perspectives.
            
            
              •
              Espousing objectivity prevents us from supporting the communities we serve.
            
            
              •
              Mitigated objectivity acknowledges implicit bias, constraining it via quality control.},
	urldate = {2025-01-16},
	journal = {Forensic Science International: Synergy},
	author = {Winburn, Allysha Powanda and Clemmons, Chaunesey M.J.},
	month = sep,
	year = {2021},
	pmid = {34622187},
	pmcid = {PMC8484737},
	keywords = {Diversity, Cognitive bias, Equity, Implicit bias, Inclusion, Mitigated objectivity, Quality control, Social studies of science, Subjectivity},
	pages = {100196},
	file = {PubMed Central Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\LLXZN8AL\\Winburn and Clemmons - 2021 - Objectivity is a myth that harms the practice and diversity of forensic science.pdf:application/pdf},
}

@article{freeman_apictorial_1964,
	title = {Apictorial {Jigsaw} {Puzzles}: {The} {Computer} {Solution} of a {Problem} in {Pattern} {Recognition}},
	volume = {EC-13},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0367-7508},
	shorttitle = {Apictorial {Jigsaw} {Puzzles}},
	url = {http://ieeexplore.ieee.org/document/4038109/},
	doi = {10.1109/PGEC.1964.263781},
	abstract = {This paper describes the development of a procedure is of value only if the time and storage requirements inthat enables a digital computer to solve "apictorial" jigsaw puzzles, crease only inodestly with increases in the complexity of i.e., puzzles in which all pieces are uniformly gray and the only available information is the shape of the pieces. The problem was selected the puzzle.A"re-r rapproah, which ma wor because it provided an excellent vehicle to develop computer tech- for small puzzles, very rapidly becomes futile as the niques for manipulation of arbitrary geometric patterns, for pattern number of pieces is increased. identification, and for game solving. The kinds of puzzles and their If the pieces of a jigsaw puzzle are turned over so that properties are discussed in detail. Methods are described for char- no picture information is available, i.e., so that all pieces acterizing and classifying piece contours, for selecting and ordering are uniformly gray, assembly of the puzzle becomes conpieces that are "most likely" to mate with a given piece, for deter- . . . T mining likelihood of fit, for overcoming ambiguities, and for evalua- siderably more difficult. Techniques for solution must tion of the progressive puzzle assembly. An illustration of an actual now rely entirely on shape. Puzzles of this type will be computer solution of a puzzle is given. referred to as apictorial puzzles. A procedure for solving such puzzles by means of a digital computer is the subject of this paper.},
	language = {en},
	number = {2},
	urldate = {2025-01-16},
	journal = {IEEE Transactions on Electronic Computers},
	author = {Freeman, H. and Garder, L.},
	month = apr,
	year = {1964},
	pages = {118--127},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\WX4GJL4V\\Freeman and Garder - 1964 - Apictorial Jigsaw Puzzles The Computer Solution of a Problem in Pattern Recognition.pdf:application/pdf},
}

@article{taroni_reconciliation_2018,
	title = {Reconciliation of subjective probabilities and frequencies in forensic science},
	volume = {17},
	issn = {1470-8396},
	url = {https://doi.org/10.1093/lpr/mgy014},
	doi = {10.1093/lpr/mgy014},
	abstract = {There is a continuous flow of articles published in legal and scientific journals that recite outworn direct or subtle attacks on Bayesian reasoning and/or the use of the subjective or personalistic interpretation of probability. An example is the recent paper written by Kaplan et al. (2016), who, by referring to Kafadar’s review paper (2015), opined, but did not justify, that there is a ‘… need to reduce subjectivity in the evaluation of forensic science’ and argued that ‘… the view presented here supports the use of objective probabilities’ (Kaplan et al., 2016). To understand why the objection on the use of subjective probability is not persuasive and why the widely claimed objective probabilities do not exist, one must first scrutinize the historically competing interpretations of probability and their associated definitions. The basis of the defence of the use of the subjectivist interpretation of probability is the understanding of the simple points, misunderstood by critics, that subjectivity is not a synonym for arbitrariness and that the implementation of subjectivism does not neglect the use of the acquired knowledge that is often available in terms of relative frequencies. We will illustrate these points by reference to practical applications in forensic science where probabilities are often represented by relative frequencies. In this regard, our discussion clarifies the connection and the distinction between probabilities and frequencies. Specifically, we emphasize that probability is an expression of our personal belief, an interpretation not to be equated with relative frequency as a mere summary of data. Our argument reveals the inappropriateness of attempts to interpret relative frequencies as probabilities, and naturally solves common problems that derive from such attempts. Further we emphasize that, despite the fact that they can be given an explicit role in probability assignments, neither are relative frequencies a necessary condition for such assignments nor, in forensic applications that consider events for which probabilities need to be specified, need they be meaningfully conceptualized in a frequentist perspective.},
	number = {3},
	urldate = {2025-01-16},
	journal = {Law, Probability and Risk},
	author = {Taroni, F and Garbolino, P and Biedermann, A and Aitken, C and Bozza, S},
	month = sep,
	year = {2018},
	pages = {243--262},
	file = {Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\9EDBQLTB\\Taroni et al. - 2018 - Reconciliation of subjective probabilities and frequencies in forensic science.pdf:application/pdf},
}

@article{dror_cognitive_2020,
	title = {Cognitive and {Human} {Factors} in {Expert} {Decision} {Making}: {Six} {Fallacies} and the {Eight} {Sources} of {Bias}},
	volume = {92},
	issn = {0003-2700},
	shorttitle = {Cognitive and {Human} {Factors} in {Expert} {Decision} {Making}},
	url = {https://doi.org/10.1021/acs.analchem.0c00704},
	doi = {10.1021/acs.analchem.0c00704},
	abstract = {Fallacies about the nature of biases have shadowed a proper cognitive understanding of biases and their sources, which in turn lead to ways that minimize their impact. Six such fallacies are presented: it is an ethical issue, only applies to “bad apples”, experts are impartial and immune, technology eliminates bias, blind spot, and the illusion of control. Then, eight sources of bias are discussed and conceptualized within three categories: (A) factors that relate to the specific case and analysis, which include the data, reference materials, and contextual information, (B) factors that relate to the specific person doing the analysis, which include past experience base rates, organizational factors, education and training, and personal factors, and lastly, (C) cognitive architecture and human nature that impacts all of us. These factors can impact what the data are (e.g., how data are sampled and collected, or what is considered as noise and therefore disregarded), the actual results (e.g., decisions on testing strategies, how analysis is conducted, and when to stop testing), and the conclusions (e.g., interpretation of the results). The paper concludes with specific measures that can minimize these biases.},
	number = {12},
	urldate = {2025-01-17},
	journal = {Analytical Chemistry},
	author = {Dror, Itiel E.},
	month = jun,
	year = {2020},
	note = {Publisher: American Chemical Society},
	pages = {7998--8004},
	file = {Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\RTFRMUKB\\Dror - 2020 - Cognitive and Human Factors in Expert Decision Making Six Fallacies and the Eight Sources of Bias.pdf:application/pdf},
}

@article{marques-mateu_quantifying_2018,
	title = {Quantifying the uncertainty of soil colour measurements with {Munsell} charts using a modified attribute agreement analysis},
	volume = {171},
	issn = {0341-8162},
	url = {https://www.sciencedirect.com/science/article/pii/S0341816218302595},
	doi = {10.1016/j.catena.2018.06.027},
	abstract = {The use of Munsell colour charts is the classical way of determining colour information in soil science. The procedure is well-known and consists of visually comparing soil samples with colour chips contained in the charts. This visual approach has several drawbacks and although the chart-based procedure is routinely used, it is not easy to find systematic studies on the accuracy of this methodology. In this paper, we seek to gain insight into the strengths and weaknesses of using soil colour charts as a colour measurement device. The tool used to conduct our study is a modification to the attribute agreement analysis (AAA) method which consists of finding matches between colour standards and colour designations obtained by several appraisers. In the experiment, standards were obtained using a trichromatic colorimeter coupled to a computer program that implements the k nearest neighbour (k-NN) classification algorithm. In order to do the experiment, 276 soil samples were observed twice by four trained appraisers (2208 data records). The naïve count of matches across all the records in the dataset gave {\textless}5\% of agreement for all three colour components Hue, Value and Chroma. The modified AAA criterion implemented in the study gave a clear increase in all indicators with values ranging from 82.2\% to 100\% in the agreement within appraisers, 39.5\% in the agreement between appraisers, and 42.8\% in the agreement of appraisers vs. standards. Results also show that users of the Munsell charts tend to mostly report correct Hues but higher Values and Chromas than true soil colours.},
	urldate = {2025-01-20},
	journal = {CATENA},
	author = {Marqués-Mateu, Ángel and Moreno-Ramón, Héctor and Balasch, Sebastià and Ibáñez-Asensio, Sara},
	month = dec,
	year = {2018},
	keywords = {Attribute agreement analysis, Colorimetry, k-NN algorithm, Munsell charts, Soil colour},
	pages = {44--53},
}

@article{wolfe_rare_2005,
	title = {Rare items often missed in visual searches},
	volume = {435},
	copyright = {2005 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/435439a},
	doi = {10.1038/435439a},
	abstract = {Errors in spotting key targets soar alarmingly if they appear only infrequently during screening.},
	language = {en},
	number = {7041},
	urldate = {2025-01-20},
	journal = {Nature},
	author = {Wolfe, Jeremy M. and Horowitz, Todd S. and Kenner, Naomi M.},
	month = may,
	year = {2005},
	note = {Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, multidisciplinary, Science},
	pages = {439--440},
	file = {PubMed Central Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\4DYZAVFA\\Wolfe et al. - 2005 - Rare items often missed in visual searches.pdf:application/pdf},
}

@article{wolfe_low_2007,
	title = {Low target prevalence is a stubborn source of errors in visual search tasks},
	volume = {136},
	issn = {1939-2222},
	doi = {10.1037/0096-3445.136.4.623},
	abstract = {In visual search tasks, observers look for targets in displays containing distractors. Likelihood that targets will be missed varies with target prevalence, the frequency with which targets are presented across trials. Miss error rates are much higher at low target prevalence (1\%-2\%) than at high prevalence (50\%). Unfortunately, low prevalence is characteristic of important search tasks such as airport security and medical screening where miss errors are dangerous. A series of experiments show this prevalence effect is very robust. In signal detection terms, the prevalence effect can be explained as a criterion shift and not a change in sensitivity. Several efforts to induce observers to adopt a better criterion fail. However, a regime of brief retraining periods with high prevalence and full feedback allows observers to hold a good criterion during periods of low prevalence with no feedback. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {4},
	journal = {Journal of Experimental Psychology: General},
	author = {Wolfe, Jeremy M. and Horowitz, Todd S. and Van Wert, Michael J. and Kenner, Naomi M. and Place, Skyler S. and Kibbi, Nour},
	year = {2007},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Attention, National Security, Screening, Signal Detection (Perception), Visual Search},
	pages = {623--638},
	file = {Accepted Version:C\:\\Users\\jstacey\\Zotero\\storage\\I2J48HTB\\Wolfe et al. - 2007 - Low target prevalence is a stubborn source of errors in visual search tasks.pdf:application/pdf;Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\A43L6SKX\\2007-16657-007.html:text/html},
}

@article{evett_finding_2017,
	title = {Finding the way forward for forensic science in the {US}—{A} commentary on the {PCAST} report},
	volume = {278},
	issn = {0379-0738},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073817302256},
	doi = {10.1016/j.forsciint.2017.06.018},
	abstract = {A recent report by the US President’s Council of Advisors on Science and Technology (PCAST), (2016) has made a number of recommendations for the future development of forensic science. Whereas we all agree that there is much need for change, we find that the PCAST report recommendations are founded on serious misunderstandings. We explain the traditional forensic paradigms of match and identification and the more recent foundation of the logical approach to evidence evaluation. This forms the groundwork for exposing many sources of confusion in the PCAST report. We explain how the notion of treating the scientist as a black box and the assignment of evidential weight through error rates is overly restrictive and misconceived. Our own view sees inferential logic, the development of calibrated knowledge and understanding of scientists as the core of the advance of the profession.},
	urldate = {2025-01-20},
	journal = {Forensic Science International},
	author = {Evett, I. W. and Berger, C. E. H. and Buckleton, J. S. and Champod, C. and Jackson, G.},
	month = sep,
	year = {2017},
	keywords = {Evidence, Likelihood ratio, Probability, Comparison methods, Forensic inference},
	pages = {16--23},
	file = {Accepted Version:C\:\\Users\\jstacey\\Zotero\\storage\\3PHVL723\\Evett et al. - 2017 - Finding the way forward for forensic science in the US—A commentary on the PCAST report.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\EYAUIBFU\\S0379073817302256.html:text/html},
}

@techreport{presidents_council_of_advisors_on_science_and_technology_pcast_report_2016,
	address = {Washington DC},
	title = {Report to the president {Forensic} {Science} in {Criminal} {Courts}: {Ensuring} {Scientific} {Validity} of {Feature}-{Comparison} {Methods}},
	url = {https://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/PCAST/pcast_forensic_scienc e_report_final.pdf.},
	author = {President’s Council of Advisors on Science {and} Technology (PCAST)},
	year = {2016},
}

@article{dror_most_2023,
	title = {The most consistent finding in forensic science is inconsistency},
	volume = {68},
	copyright = {© 2023 American Academy of Forensic Sciences.},
	issn = {1556-4029},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1556-4029.15369},
	doi = {10.1111/1556-4029.15369},
	abstract = {The most consistent finding in many forensic science domains is inconsistency (i.e., lack of reliability, reproducibility, repeatability, and replicability). The lack of consistency is a major problem, both from a scientific and a criminal justice point of view. Examining forensic conclusion data, from across many forensic domains, highlights the underlying cognitive issues and offers a better understanding of the issues and challenges. Such insights enable the development of ways to minimize these inconsistencies and move forward. The aim is to highlight the problem, so that it can be minimized and the reliability of forensic science evidence can be improved.},
	language = {en},
	number = {6},
	urldate = {2025-01-20},
	journal = {Journal of Forensic Sciences},
	author = {Dror, Itiel E.},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1556-4029.15369},
	keywords = {cognitive bias, conclusions, decision making, expertise, forensic conclusions, human factors, linear sequential unmasking, reliability, variability},
	pages = {1851--1855},
	file = {Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\7H38ES83\\Dror - 2023 - The most consistent finding in forensic science is inconsistency.pdf:application/pdf;Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\AG8V2R9X\\1556-4029.html:text/html},
}

@techreport{butler_dna_2024,
	address = {Gaithersburg, MD},
	title = {{DNA} {Mixture} {Interpretation}: {A} {NIST} {Scientific} {Foundation} {Review}},
	shorttitle = {{DNA} {Mixture} {Interpretation}},
	url = {https://nvlpubs.nist.gov/nistpubs/ir/2024/NIST.IR.8351.pdf},
	language = {en},
	number = {NIST IR 8351},
	urldate = {2025-01-29},
	institution = {National Institute of Standards and Technology},
	author = {Butler, John M},
	year = {2024},
	doi = {10.6028/NIST.IR.8351},
	pages = {NIST IR 8351},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\E85EMGAI\\Butler - 2024 - DNA Mixture Interpretation A NIST Scientific Foundation Review.pdf:application/pdf},
}

@book{taylor_forensic_2023,
	title = {Forensic {DNA} {Trace} {Evidence} {Interpretation}: {Activity} {Level} {Propositions} and {Likelihood} {Ratios}},
	isbn = {978-1-000-80141-5},
	shorttitle = {Forensic {DNA} {Trace} {Evidence} {Interpretation}},
	abstract = {Forensic DNA Trace Evidence Interpretation: Activity Level Propositions and Likelihood Ratios provides all foundational information required for a reader to understand the practice of evaluating forensic biology evidence given activity level propositions and to implement the practice into active casework within a forensic institution. The book begins by explaining basic concepts and foundational theory, pulling together research and studies that have accumulated in forensic journal literature over the last 20 years. The book explains the laws of probability - showing how they can be used to derive, from first principles, the likelihood ratio - used throughout the book to express the strength of evidence for any evaluation. Concepts such as the hierarchy of propositions, the difference between experts working in an investigative or evaluative mode and the practice of case assessment and interpretation are explained to provide the reader with a broad grounding in the topics that are important to understanding evaluation of evidence. Activity level evaluations are discussed in relation to biological material transferred from one object to another, the ability for biological material to persist on an item for a period of time or through an event, the ability to recover the biological material from the object when sampled for forensic testing and the expectations of the prevalence of biological material on objects in our environment. These concepts of transfer, persistence, prevalence and recovery are discussed in detail in addition to the factors that affect each of them.The authors go on to explain the evaluation process: how to structure case information and formulate propositions. This includes how a likelihood ratio formula can be derived to evaluate the forensic findings, introducing Bayesian networks and explaining what they represent and how they can be used in evaluations and showing how evaluation can be tested for robustness. Using these tools, the authors also demonstrate the ways that the methods used in activity level evaluations are applied to questions about body fluids. There are also chapters dedicated to reporting of results and implementation of activity level evaluation in a working forensic laboratory. Throughout the book, four cases are used as examples to demonstrate how to relate the theory to practice and detail how laboratories can integrate and implement activity level evaluation into their active casework.},
	language = {en},
	publisher = {CRC Press},
	author = {Taylor, Duncan and Kokshoorn, Bas},
	month = may,
	year = {2023},
	note = {Google-Books-ID: Z9u4EAAAQBAJ},
	keywords = {Law / Forensic Science, Science / Life Sciences / Genetics \& Genomics, Social Science / Criminology, Social Science / Sociology / General},
	file = {Ebook:C\:\\Users\\jstacey\\Zotero\\storage\\95L9LSG3\\Taylor and Kokshoorn - 2023 - Forensic DNA Trace Evidence Interpretation Activity Level Propositions and Likelihood Ratios.epub:application/epub+zip},
}

@techreport{texas_forensic_science_comission_final_2024,
	title = {Final {Report} on {Complaint} {No}. 23.67; {Tiffany} {Roy}; ({Timothy} {Kalafut}, {Ph}.{D}.; {Evaluation} of {Biological}/{DNA} {Results} {Given} {Activity} {Level} {Propositions})},
	url = {https://www.txcourts.gov/media/1458950/final-report-complaint-2367-roytiffany-073024_redacted.pdf},
	author = {Texas Forensic Science Comission},
	month = jul,
	year = {2024},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\U9IZU6I6\\_.pdf:application/pdf},
}

@article{herhaus_kinetics_2023,
	title = {Kinetics of {Plasma} {Cell}-{Free} {DNA} under a {Highly} {Standardized} and {Controlled} {Stress} {Induction}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-4409},
	url = {https://www.mdpi.com/2073-4409/12/4/564},
	doi = {10.3390/cells12040564},
	abstract = {Psychological stress affects the immune system and activates peripheral inflammatory pathways. Circulating cell-free DNA (cfDNA) is associated with systemic inflammation, and recent research indicates that cfDNA is an inflammatory marker that is sensitive to psychological stress in humans. The present study investigated the effects of acute stress on the kinetics of cfDNA in a within-subjects design. Twenty-nine males (mean age: 24.34 ± 4.08 years) underwent both the Trier Social Stress Test (TSST) and a resting condition. Blood samples were collected at two time points before and at 9 time points up to 105 min after both conditions. The cfDNA immediately increased 2-fold after the TSST and returned to baseline levels after 30 min after the test, showing that a brief psychological stressor was sufficient to evoke a robust and rapid increase in cfDNA levels. No associations were detected between perceived stress, whereas subjects with higher basal cfDNA levels showed higher increases. The rapid cfDNA regulation might be attributed to the transient activation of immune cells caused by neuroendocrine-immune activation. Further research is required to evaluate the reliability of cfDNA as a marker of neuroendocrine-immune activation, which could be used for diagnostics purposes or monitoring of treatment progression.},
	language = {en},
	number = {4},
	urldate = {2025-02-17},
	journal = {Cells},
	author = {Herhaus, Benedict and Neuberger, Elmo and Juškevičiūtė, Ema and Simon, Perikles and Petrowski, Katja},
	month = jan,
	year = {2023},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {circulating cell-free DNA (cfDNA), psychological stress, Trier Social Stress Test (TSST)},
	pages = {564},
	file = {Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\7IUIELIL\\Herhaus et al. - 2023 - Kinetics of Plasma Cell-Free DNA under a Highly Standardized and Controlled Stress Induction.pdf:application/pdf},
}

@article{elkins_survey_2025,
	title = {Survey on forensic {DNA} biology training in forensic science service laboratories in the {United} {States}},
	volume = {70},
	issn = {1556-4029},
	doi = {10.1111/1556-4029.15671},
	abstract = {Training is an essential component of onboarding new hires in forensic science service provider (FSSP) laboratories. There are several DNA training standards published by the American Academy of Forensic Sciences (AAFS) Academy Standards Board (ASB) American National Standards Institute (ANSI) accredited framework. In this study, we conducted a survey of forensic DNA laboratory training programs to better understand training activities and materials. The survey was approved by the IRB and emailed to forensic laboratory directors, assistant directors, and/or DNA technical leaders and responses were submitted by them or their designees. Over thirty leaders and stakeholders responded. In this article, we report on the results of the survey. Respondents indicated that training activities included readings, writing assignments and quizzes, shadowing analysts, and mock casework and that training is documented and is a collaborative effort of the technical leader, unit supervisor, advanced forensic scientists, and other analysts and technicians. Laboratories assess competency using multiple methods including performance on mock casework, report writing, laboratory practical and competency tests, and a mock trial and testimony. The top three training activities reported are hands-on practice, shadowing, and readings. The top three focuses of the training are laboratory techniques (extraction, quantitation, amplification, and capillary electrophoresis), STR typing, and mixture analysis. Ethics violations and failure to pass the competency tests and mock trial, even after remediation, are the top reasons new hires fail training. Finally, the top items respondents would like to spend more time offering training on are troubleshooting, validation, and root cause analysis.},
	language = {eng},
	number = {1},
	journal = {Journal of Forensic Sciences},
	author = {Elkins, Kelly M. and Joseph, Shervonne and Skrant, Cassandra},
	month = jan,
	year = {2025},
	pmid = {39601447},
	keywords = {DNA, survey, DNA Fingerprinting, Humans, Professional Competence, competency testing, forensic DNA, Forensic Sciences, in‐house training, Inservice Training, Laboratories, Laboratory Personnel, Microsatellite Repeats, standards, Surveys and Questionnaires, training, training standards, United States},
	pages = {61--72},
}

@techreport{uk_forensic_science_regulator_codes_2021,
	title = {Codes of {Practice} and  {Conduct}: {Development} of evaluative opinions},
	url = {https://www.gov.uk/government/publications/development-of-evaluative-opinions},
	abstract = {Sets out a standardised approach to formulating evaluative opinions used by forensic scientists in the criminal justice system.},
	language = {en},
	number = {FSR-C-118},
	urldate = {2025-02-18},
	author = {UK Forensic Science Regulator},
	year = {2021},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\BNP9GZJ8\\Development of evaluative opinions.pdf:application/pdf;Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\3CFQTR6Q\\development-of-evaluative-opinions.html:text/html},
}

@techreport{ballantyne_introductory_2017,
	title = {An introductory guide to evaluative reporting},
	institution = {Australia New Zealand Policing Advisory Agency - National Institute of Forensic Science (ANZPAA-NIFS)},
	author = {Ballantyne, K. N. and Bunford, J and Found, B and Neville, D and Taylor, Duncan and Wevers, Gerhard and Catoggio, D},
	month = jun,
	year = {2017},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\7QUFHCLK\\_.PDF:application/pdf},
}

@misc{scientific_working_group_on_dna_analysis_methods_swgdam_2020,
	title = {{SWGDAM} {Training} {Guidelines}},
	url = {https://www.swgdam.org/publications},
	author = {Scientific Working Group on DNA Analysis Methods},
	year = {2020},
}

@misc{noauthor_statistics_2023,
	title = {Statistics and the {Evaluation} of {Forensic} {Evidence}},
	url = {https://www.formation-continue-unil-epfl.ch/formation/statistics-evaluation-forensic-evidence-cas/},
	urldate = {2025-02-19},
	journal = {Unil EPFL Continuing Education},
	year = {2023},
	file = {Statistics and the Evaluation of Forensic Evidence - Unil EPFL Continuing Education:C\:\\Users\\jstacey\\Zotero\\storage\\GKX8DTQ7\\statistics-evaluation-forensic-evidence-cas.html:text/html},
}

@misc{noauthor_advanced_2023,
	title = {Advanced {DNA} interpretation given activity level propositions},
	url = {https://www.formation-continue-unil-epfl.ch/formation/advanced-dna-interpretation/},
	urldate = {2025-02-19},
	journal = {Unil EPFL Continuing Education},
	year = {2023},
	file = {Advanced DNA interpretation - Formation Continue Unil EPFL:C\:\\Users\\jstacey\\Zotero\\storage\\HZB23SSY\\advanced-dna-interpretation.html:text/html},
}

@misc{kalafut_longest_2025,
	address = {Baltimore, Maryland},
	type = {Workshop},
	title = {The {Longest} {Journey} {Starts} {With} a {Single} {Step}: {Evaluating} {Biological} {Results} {Given} {Activity}-{Level} {Propositions} — {The} {Problem}, the {Theory}, the {Solution}, and {Strategies} for {Implementation}},
	url = {https://www.aafs.org/program/tuesday-workshops},
	language = {en},
	urldate = {2025-02-19},
	author = {Kalafut, Tim and Gittelson, Simone and Willis, Sheila},
	month = feb,
	year = {2025},
	file = {Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\TJRS4WW3\\tuesday-workshops.html:text/html},
}

@misc{kalafut_beyond_2024,
	address = {San Antonio, Texas},
	type = {Workshop},
	title = {Beyond the ‘{Who} {Done} {It}…’ – {DNA} {Interpretation} {Given} {Activity} {Level} {Propositions}},
	url = {https://www.ishinews.com/workshops-announced-for-ishi-35/},
	author = {Kalafut, Tim and Gittelson, Simone},
	month = sep,
	year = {2024},
}

@article{van_oorschot_need_2017,
	title = {Need for dedicated training, competency assessment, authorisations and ongoing proficiency testing for those addressing {DNA} transfer issues},
	volume = {6},
	issn = {1875-1768},
	url = {https://www.sciencedirect.com/science/article/pii/S1875176817302330},
	doi = {10.1016/j.fsigss.2017.09.013},
	abstract = {Those authorized to report on the weight of DNA evidence are frequently called upon to address questions on DNA transfer, persistence, prevalence and recovery (DNA-TPPR). Due to a lack of standardization, knowledge and training, these questions are often addressed poorly or inadequately, thus, potentially adversely impacting justice outcomes. As the forensic community moves towards contemplating activity-level assessments, it becomes incumbent on the expert to be well equipped to do so. The likelihood of alternative scenarios must be provided using data and methodologies that are valid both foundationally and as applied. The expert must be aware of the current knowledge of variables potentially impacting any assessment, including those involving DNA-TPPR, and the associated limitations. We have conducted preliminary assessments of DNA reporting scientists on their general understanding of DNA-TPPR and ability to identify key factors that could impact transfer probabilities. Differences were observed between individuals, from between and within laboratories, in their level of comprehension of DNA-TPPR. Apart from the need for further studies to validate the variables impacting DNA-TPPR and the extent of their impact in different situations, we advocate that individuals utilizing this information should be specifically authorized to conduct such analyses and provide expert opinion in criminal investigations and legal proceedings relating to DNA-TPPR. Furthermore, improvements are required in the foundational and ongoing training of these individuals. The setting of standards to be met by such training, how to test competency, and the availability of regular purpose-fit proficiency tests, requires urgent attention.},
	urldate = {2025-02-19},
	journal = {Forensic Science International: Genetics Supplement Series},
	author = {van Oorschot, Roland A. H. and Szkuta, Bianca and Ballantyne, Kaye N. and Goray, Mariya},
	month = dec,
	year = {2017},
	keywords = {Activity level, DNA transfer, Competency, Training},
	pages = {e32--e34},
	file = {Full Text:C\:\\Users\\jstacey\\Zotero\\storage\\T6LZ3F3W\\van Oorschot et al. - 2017 - Need for dedicated training, competency assessment, authorisations and ongoing proficiency testing f.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\EBPKWKV2\\S1875176817302330.html:text/html},
}

@article{woollacott_transfer_2025,
	title = {The {Transfer}, {Prevalence}, {Persistence}, and {Recovery} of {DNA} from {Body} {Areas} in {Forensic} {Science}: {A} {Review}},
	volume = {5},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2673-6756},
	shorttitle = {The {Transfer}, {Prevalence}, {Persistence}, and {Recovery} of {DNA} from {Body} {Areas} in {Forensic} {Science}},
	url = {https://www.mdpi.com/2673-6756/5/1/9},
	doi = {10.3390/forensicsci5010009},
	abstract = {Forensic and medical examiners are often required to sample the body of a victim (either living or deceased), or a suspect of a criminal offence, for foreign DNA. This can provide useful information when the alleged activity involves the presence of various bodily fluids such as blood, semen, and/or saliva, as well as skin contact made between a perpetrator and a victim. Optimal recovery techniques for the collection of DNA evidence, following crime-relevant skin contact, can be dependent on the surface being sampled. Additional factors to consider include the body areas typically contacted during various activities and the likelihood of non-self-DNA being present in those areas prior to contacts of interest. Therefore, an understanding of DNA transfer, prevalence, persistence, and recovery on a body can aid in the interpretation of DNA results given activity-level questions and increase the value of the findings from this type of evidence. This review aims to summarise research on DNA-TPPR concerning various human body surfaces following different types of activities. This review examines the prevalence of background DNA on different skin surfaces, the reported DNA transfer associated with different forms of contact, and how different cofounding factors can affect the persistence of DNA.},
	language = {en},
	number = {1},
	urldate = {2025-02-19},
	journal = {Forensic Sciences},
	author = {Woollacott, Cara and Goray, Mariya and van Oorschot, Roland A. H. and Taylor, Duncan},
	month = mar,
	year = {2025},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {transfer, background DNA, body sampling, persistence, prevalence, recovery, skin surfaces, trace DNA},
	pages = {9},
	file = {Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\CAXPILS2\\Woollacott et al. - 2025 - The Transfer, Prevalence, Persistence, and Recovery of DNA from Body Areas in Forensic Science A Re.pdf:application/pdf},
}

@misc{organization_of_scientific_area_committees_osac_for_forensic_science_best_2022,
	title = {Best {Practice} {Recommendations} for {Evaluative} {Forensic} {DNA} {Testimony}},
	url = {https://www.nist.gov/system/files/documents/2022/01/04/OSAC%202022-S-0024%20BPR%20for%20Evaluative%20Forensic%20DNA%20Testimony.OPEN%20COMMENT%20VERSION.pdf},
	urldate = {2025-02-03},
	author = {Organization of Scientific Area Committees (OSAC) for Forensic Science},
	year = {2022},
	file = {PDF:C\:\\Users\\jstacey\\Zotero\\storage\\QRM8U3EY\\Best Practice Recommendations for Evaluative Forensic DNA Testimony.pdf:application/pdf},
}

@article{biedermann_evaluation_2016,
	title = {Evaluation of {Forensic} {DNA} {Traces} {When} {Propositions} of {Interest} {Relate} to {Activities}: {Analysis} and {Discussion} of {Recurrent} {Concerns}},
	volume = {7},
	issn = {1664-8021},
	shorttitle = {Evaluation of {Forensic} {DNA} {Traces} {When} {Propositions} of {Interest} {Relate} to {Activities}},
	url = {https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2016.00215/full},
	doi = {10.3389/fgene.2016.00215},
	abstract = {{\textless}p{\textgreater}When forensic scientists evaluate and report on the probative strength of single DNA traces, they commonly rely on only one number, expressing the rarity of the DNA profile in the population of interest. This is so because the focus is on propositions regarding the source of the recovered trace material, such as “the person of interest is the source of the crime stain.” In particular, when the alternative proposition is “an unknown person is the source of the crime stain,” one is directed to think about the rarity of the profile. However, in the era of DNA profiling technology capable of producing results from small quantities of trace material (i.e., non-visible staining) that is subject to easy and ubiquitous modes of transfer, the issue of source is becoming less central, to the point that it is often not contested. There is now a shift from the question “whose DNA is this?” to the question “how did it get there?” As a consequence, recipients of expert information are now very much in need of assistance with the evaluation of the meaning and probative strength of DNA profiling results when the competing propositions of interest refer to different activities. This need is widely demonstrated in day-to-day forensic practice and is also voiced in specialized literature. Yet many forensic scientists remain reluctant to assess their results given propositions that relate to different activities. Some scientists consider evaluations beyond the issue of source as being overly speculative, because of the lack of relevant data and knowledge regarding phenomena and mechanisms of transfer, persistence and background of DNA. Similarly, encouragements to deal with these activity issues, expressed in a recently released European guideline on evaluative reporting (Willis et al., {\textless}xref ref-type="bibr" rid="B43"{\textgreater}2015{\textless}/xref{\textgreater}), which highlights the need for rethinking current practice, are sometimes viewed skeptically or are not considered feasible. In this discussion paper, we select and discuss recurrent skeptical views brought to our attention, as well as some of the alternative solutions that have been suggested. We will argue that the way forward is to address now, rather than later, the challenges associated with the evaluation of DNA results (from small quantities of trace material) in light of different activities to prevent them being misrepresented in court.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-02-21},
	journal = {Frontiers in Genetics},
	author = {Biedermann, Alex and Champod, Christophe and Jackson, Graham and Gill, Peter and Taylor, Duncan and Butler, John and Morling, Niels and Hicks, Tacha and Vuille, Joelle and Taroni, Franco},
	month = dec,
	year = {2016},
	note = {Publisher: Frontiers},
	keywords = {interpretation, Forensic DNA, Hierarchy of propositions, Probability assignment, Probative value},
	file = {Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\48Y6WI8B\\Biedermann et al. - 2016 - Evaluation of Forensic DNA Traces When Propositions of Interest Relate to Activities Analysis and D.pdf:application/pdf},
}

@article{biedermann_importance_2016,
	title = {The {Importance} of {Critically} {Examining} the {Level} of {Propositions} {When} {Evaluating} {Forensic} {DNA} {Results}},
	volume = {7},
	issn = {1664-8021},
	url = {https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2016.00008/full},
	doi = {10.3389/fgene.2016.00008},
	abstract = {IntroductionThe proposal of a discussion about the use of software to help assign likelihood ratios for forensic DNA profiling results, and the use of their output in the legal process, is both timely and important (see also related contributions elsewhere in Frontiers, e.g., Biedermann et al., 2014). Ever since their introduction in forensic science, DNA profiling analyses have been accompanied with the results of calculations of various sorts. Their scope is well illustrated and documented in several reference monographs (e.g., Balding and Steele, 2015; Buckleton et al., 2005; Evett and Weir, 1998). This solid body of scholarly research and established practice has contributed to the widely held view among scientists and recipients of expert information that eliciting the probative strength of forensic DNA profiling results is per se a numerical task. In this commentary, we intend – in a first part – to make the point that although calculations are, by virtue, an integral part of the quantification of probative strength, it is equally important at the outset to be clear about the question “Why are we doing a calculation?” (Buckleton et al., 2005, at p. 151). We will argue that this is not a question that statistics can answer. Stated otherwise, we will contend that, as much it is important to be clear in any instance about what a particular computation exactly purports to do, it is essential to define the questions that are of interest in a particular case at hand. In a second part, we will emphasize on the extent to which, why and how a recently issued guidelines (e.g., ENFSI, 2015) encourage such thinking about cases prior to conducting calculations, if any.Questioning default calculationsExperience demonstrates that many scientists working in operational laboratories decide on the use of particular computational procedures [Footnote: The scope of these procedures is large and includes topics such as complex modelling of products (e.g., stutters) of the PCR amplification of STRs (e.g., Bright et al., 2014; Gittelson et al., 2014) and the study of the sensitivity of expressions of probative value due to the use of particular statistical techniques (e.g., MCMC techniques, see for example Bright et al., 2015).] – often provided by ready-to-use software packages – based on the mere availability of those procedures at their workplace. This amounts to a convenience choice, but what is more is that proceeding in this way is considered the best one could do. This view may be reinforced if the software is based on Bayesian principles, because procedures that belong to this class of inferential methods are referred to as the most inferentially sound. But the sole fact that a procedure relies on Bayesian principles does not make it per se pertinent for the case at hand. As noted by Lindley (2004, p. 74), “[t]he main danger is that they [Bayesian methods; added by the authors] will be used automatically. (…) You must think about the real quantities involved, like temperature or blood pressure, and not about symbols that represent them. This distinction between the thinking you and the unthinking, calculating personal computer is essential.” This danger also exists in the context of interpreting and reporting forensic DNA results. Indeed, most of the commonly available computational procedures lead to expressions of probative strength to help discriminate between so-called sub-source level propositions (e.g., ‘the person of interest (POI) is the source of the recovered DNA’ versus ‘an unknown person is the source of the recovered DNA’). But, in many practical cases, the real question goes beyond this level, e.g. how the detected DNA got where it was found (Evett et al., 2002; Taroni et al., 2013), that is so-called activity level propositions. Cases of alleged rape where the competing versions only differ with respect to the activities that led to the trace illustrate this. This is of course not a critique of models being Bayesian in nature, but of the kind of questions to which some of these models are tailored.Sceptics may invoke that none of the above problems are novel. But why then practice by and large remains unchanged? While some scientists openly acknowledge that expressions of probative strength of DNA considering sub-source level propositions may indeed be insufficient for the needs, some hold that it is for the Court to decide on that matter. We perfectly agree with this stance, of course, because whatever the level of the propositions, it is for the Court to decide on the probability of the propositions. Notwithstanding, scientists can add considerable value by assessing their results given activity level propositions. Yet others contend that one can leave this debate until the Courtroom. However, this may raise issues from a quality management point of view, and render the situation very uncomfortable for the witness, because of the inevitable difficulty of the task. The challenge is real for a variety of case scenarios, in particular where only low quantities of DNA are detected and/or when POIs do not deny that the recovered DNA is theirs. We seriously doubt that members of the judiciary are able to properly appreciate the extent to which one can expect to obtain a low quantity of DNA, recovered at a certain position on the crime scene, the victim or a POI, given one activity as compared to another activity. We would not recommend either doing this evaluation on the stand. This is because such assessments are very challenging even for experts, and require scientific knowledge about many factors, such as transfer, persistence and the capacity of a given donor to shed detectable quantities of DNA.[footnote: On this topic, see for example http://www.telegraph.co.uk/news/science/9115916/The-case-against-DNA.html.] Let us emphasise again that the question of whether the detected DNA is that of the POI may be entirely uncontested (and thus there would be no need for a likelihood ratio given sub-source propositions as there is no uncertainty about sub-source). What is really of interest is to assess the probability of observing such a result for a DNA trace, that is a trace found in a particular position, in a given quantity and leading to a profile of the observed quality given the alleged activities and given relevant information such as the time lapse between collection of trace material and the commission of the crime, environmental factors to which the trace was exposed (e.g., temperature, humidity) etc. Such assessments are highly case dependent, which calls for the generation of more research with experiments under controlled conditions, that can help build a community-wide knowledge base (Evett, 2015).[footnote: For an example in other transfer traces, see Buckleton et al. (1989).] To further emphasize the need for considering observations given activity level propositions, note again that the result which is to be assessed is not only the rarity of genetic features, but also extends to the very fact of finding, at a given position, a detectable quantity of DNA (Evett and Weir, 1998), which may be nil. Sub-source level propositions cannot deal with results that did not yield a DNA profile.The mismatch between default evaluations given sub-source level propositions and the decision makers’ interest in activity level propositions is a cause of concern because the strength of the observations in the former case can be radically different from that of the latter, so that inappropriate conclusions can result if the two are taken to be equivalent. We have seen this happen in cases where scientists report likelihood ratios in the order of \&gt;1020 with propositions at sub-source level when in fact the real issue was one of activities and where the strength of the findings, given the conditioning information of the case at hand, was way more moderate.[footnote: Another issue, not pursued here, is whether likelihood ratios exceeding one over the earth population, and multiples of that, are reasonable. There is much argument to say they are not (e.g., Hopwood et al., 2012; Thompson et al., 2003).] Current recommendationsThe above discussion is not intended to suggest that evaluation given (sub)-source level propositions is useless or detrimental in principle.[footnote: In fact, the strength of the DNA correspondence is so high that this will lead to situations where the source of the DNA will be admitted (leaving no uncertainty on the source of the DNA). This, then moves the issue to the activities.] The point we seek to make is that it is crucial to assess the needs of the recipient of expert information prior to choosing a computational procedure. This seems like an obvious and moderate requirement, yet experience shows that often it is given little attention in practice. Recent works by forensic scientists from across Europe, published in the form of a guideline (ENFSI, 2015), seek both to strengthen awareness of this issue and help scientists and recipients of expert information proceed in a more sensible way. For example, in its Guidance Note 2 on propositions, the document specifies: “Source level propositions are adequate in cases where there is no risk that the court will misinterpret them in the context of the alleged activities in the case” (ENFSI, 2015, p. 12). To illustrate this idea, the following example is given: “A large fresh bloodstain is recovered at the point of entry at a burglary scene and delivered to the laboratory for DNA analysis. Combination of a presumptive test and appearance allows the scientist to safely assume that the stain is blood. A suspect says that he has never been in the premises. The set of propositions can be (1) the bloodstain came from the defendant and (2) the bloodstain came from another unknown individual” (ENFSI, 2015, p. 12). In this example, source level propositions are not problematic because no expert knowledge is required regarding phenomena such as transfer and persistence, as well as background levels of DNA. Such factors do not impact, in this kind of circumstances, on the understanding of scientific findings relative to the alleged activities. In particular, it is not doubted that the bloodstain results from the act of breaking in. This example also illustrates that there is more to the collected trace than the DNA profile: there are aspects such as the freshness of the stain, the quantity of material and the position where the trace was found. In turn, it is clear that specialised knowledge regarding transfer, persistence and background would matter in the above scenario if DNA had been detected in low quantities, rather than from a rich bloodstain. The above understanding has far reaching implications: the level of propositions depends on the factors and observations on which forensic scientists have expert knowledge. It is their duty to evaluate all their results so that the Court is not deprived of information that is necessary for a balanced view. For example, the ENFSI guideline explicitly advises against the changing of propositions from activity to (sub-) source level when relevant expert knowledge is not available: “In fact, the choice between (sub-) source and activity should not be influenced by the availability of data or expert knowledge but solely from the consideration of factors such as transfer, persistence and background levels that could crucially affect the strength of the findings within the context of the case circumstances.”\&\#160; (ENFSI, 2015, p. 13)We acknowledge, from personal experience, that the implementation of the above perspective is challenging. It may be even more so in systems exposed to commercialisation where forensic providers that conduct DNA profiling analyses operate more and more separated from those entities that collected trace material at the crime scene (Jackson, 2013). Further obstacles may be operational constraints such as time and costs, because evaluation given activity level propositions does not rely on default computations, but generally requires a case-based approach. Regarding the latter point, some scientists deplore a lack of formulaic developments for evaluation given propositions at higher hierarchical levels. But this critique does fall short of the current state of developments. Formal likelihood ratio approaches exist (e.g., Evett, 1984; Evett and Weir, 1998), used also for other transfer materials (e.g., glass; Curran et al., 2001), and there are reports that demonstrate the relevance and practical feasibility (e.g., McKenna, 2013). Yet other developments allow one to account for uncertainty about the relevance of the recovered material and the possibility that material was left for innocent reasons (e.g., Evett, 1993; Evett et al., 2002).The role of statistics in evaluating DNA profiling evidence has always been important, but we now must realise that, increasingly often, the traditional perspective of sub-source level propositions, and the main focus on the rarity of the corresponding features (i.e., the so-called conditional genotype probability), may represent only a first step of the evaluative process. This does not make these evaluation approaches wrong, only less comprehensive. The fact is that the extrinsic characteristics of the trace material (i.e., low quantities of DNA) and the propositions of interest have changed, and it is important to realise that this represents the relevant starting point. This recognition of the needs cannot be answered by statistics, only the evaluative procedures that need to be built once the needs are properly elicited. The importance of statistics in this endeavour remains unaffected, and stands as noted by Lindley (2000, p. 38): “(…) the first task of a statistician is to develop a (probability) model to embrace the client’s interest’s and uncertainties. It will include the data and any parameters that are judged necessary. Once accomplished, the mechanics of the calculus take over and the required inference is made.”},
	language = {English},
	urldate = {2025-02-21},
	journal = {Frontiers in Genetics},
	author = {Biedermann, Alex and Hicks, Tacha},
	month = feb,
	year = {2016},
	note = {Publisher: Frontiers},
	keywords = {European guidelines, Evaluative Reporting, Level of propositions},
	file = {Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\UE9IV77P\\Biedermann and Hicks - 2016 - The Importance of Critically Examining the Level of Propositions When Evaluating Forensic DNA Result.pdf:application/pdf},
}

@misc{european_network_of_forensic_science_institutes_enfsi_2022,
	title = {{ENFSI} {Best} {Practice} {Manual} for {Human} {Forensic} {Biology} and {DNA} {Profiling}},
	url = {https://enfsi.eu/about-enfsi/structure/working-groups/documents-page/documents/best-practice-manuals/},
	language = {en-GB},
	urldate = {2025-03-04},
	publisher = {European Network of Forensic Science Institutes},
	author = {European Network of Forensic Science Institutes},
	month = dec,
	year = {2022},
	file = {Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\UAAKZVPY\\best-practice-manuals.html:text/html},
}

@incollection{rioul_this_2021,
	address = {Cham},
	title = {This is {IT}: {A} {Primer} on {Shannon}'s {Entropy} and {Information}},
	isbn = {978-3-030-81480-9},
	url = {https://doi.org/10.1007/978-3-030-81480-9_2},
	abstract = {What is Shannon's information theory (IT)? Despite its continued impact on our digital society, Claude Shannon's life and work is still unknown to numerous people. In this tutorial, we review many aspects of the concept of entropy and information from a historical and mathematical point of view. The text is structured into small, mostly independent sections, each covering a particular topic. For simplicity we restrict our attention to one-dimensional variables and use logarithm and exponential notations log and exp without specifying the base. We culminate with a simple exposition of a recent proof (2017) of the entropy power inequality (EPI), one of the most fascinating inequalities in the theory.},
	booktitle = {Information {Theory}: {Poincaré} {Seminar} 2018},
	publisher = {Springer International Publishing},
	author = {Rioul, Olivier},
	editor = {Duplantier, Bertrand and Rivasseau, Vincent},
	year = {2021},
	doi = {10.1007/978-3-030-81480-9_2},
	pages = {49--86},
}

@misc{boon_subjectivity_2007,
	title = {Subjectivity},
	isbn = {978-1-4051-6551-8},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781405165518.wbeoss297},
	abstract = {In sociology, subjectivity is often positioned as the opposite of objectivity, with objectivity being the ideal to which all empirical sociology should aspire. When Auguste Comte coined the phrase sociology, he had in mind the objective study of human behavior according to rational principles. A pertinent example of objective sociological analysis is that of Émile Durkheim, who argued that sociologists should analyze the internal (and impersonal) causes of social phenomena through the observation of concrete facts. Another notable example is, of course, the scholarship of Karl Marx, who also believed in a form of social scientism that went beyond the surface of social life through the analysis of concrete social facts. Durkheim distinguished this analysis from philosophical introspection and generalizations that would be unduly affected by subjective influences such as beliefs and values (Giddens 1993; Crow 2005).},
	language = {en},
	urldate = {2025-03-14},
	journal = {The Blackwell Encyclopedia of Sociology},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Boon, Vivienne},
	year = {2007},
	doi = {10.1002/9781405165518.wbeoss297},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781405165518.wbeoss297},
	keywords = {Philosophy, self, Sociology of Knowledge, subjective},
	file = {Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\B2W7Z86M\\9781405165518.html:text/html},
}

@article{bali_communicating_2020,
	title = {Communicating forensic science opinion: {An} examination of expert reporting practices},
	volume = {60},
	issn = {1355-0306},
	shorttitle = {Communicating forensic science opinion},
	url = {https://www.sciencedirect.com/science/article/pii/S1355030619302102},
	doi = {10.1016/j.scijus.2019.12.005},
	abstract = {Forensic scientists endeavour to explain complex scientific principles to legal decision-makers with limited scientific training (e.g., police, lawyers, judges, and jurors). Much of the time this communication is limited to written opinions in expert reports. Notwithstanding considerable scientific research and debate about the best way to communicate forensic science opinions, it is unclear how much of the advice has translated into forensic science practice. In conducting this descriptive study, we examined the reporting practices adopted by forensic scientists across a range of forensic science disciplines. Specifically, we used a quantitative content analysis approach to identify the conclusion types and additional information submitted by forensic scientists in proficiency tests during 2016 (“What would be the wording of the Conclusions in your report?”). Our analysis of 500 randomly selected responses in eight disciplines indicated that the conclusion type which has received the most criticism in recent years (categorical statements) remains the preferred means of expression in a clear majority of responses. We also found that the provision of additional information often considered necessary for rational evaluation of the evidence (e.g., information about reliability and validity) was rarely reported. These results suggest limited engagement with recent recommendations and are concerning given the gravity of the legal decisions that hinge on accurate and transparent forensic science communication.},
	number = {3},
	urldate = {2025-03-24},
	journal = {Science \& Justice},
	author = {Bali, Agnes S. and Edmond, Gary and Ballantyne, Kaye N. and Kemp, Richard I. and Martire, Kristy A.},
	month = may,
	year = {2020},
	keywords = {Forensic science, Communicating opinion, Expert evidence, Expert reports, Uncertainty, Proficiency testing},
	pages = {216--224},
	file = {ScienceDirect Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\VE7M66VM\\S1355030619302102.html:text/html;Submitted Version:C\:\\Users\\jstacey\\Zotero\\storage\\LFIMJDJB\\Bali et al. - 2020 - Communicating forensic science opinion An examination of expert reporting practices.pdf:application/pdf},
}

@article{airlie_contemporary_2021,
	title = {Contemporary issues in forensic science—{Worldwide} survey results},
	volume = {320},
	issn = {0379-0738},
	url = {https://www.sciencedirect.com/science/article/pii/S0379073821000244},
	doi = {10.1016/j.forsciint.2021.110704},
	abstract = {Forensic science continues to be openly challenged and criticized. The aim of this study was to gain an understanding of forensic workplaces and the perceived current and potential future issues from forensic scientists via a detailed survey. An online survey was designed and disseminated to forensic practitioners and researchers worldwide. 544 participants from more than 20 countries took part in the survey. Participants personally rated ten forensic disciplines with subjective methodologies, responded on a five-point Likert scale to 22 statements that addressed subjectivity and objectivity, validation and proficiency testing and error and bias and answered demographic questions relating to their workplace type, level of experience and qualifications. Participants also commented freely on forensic issues specifically important to them. The purpose of this paper is to provide the survey results and consensuses captured on several key issues. Overall participants believed forensic science must be valid and reliable and supported development of objective methodologies, validation and further investigation into the application statistics, use of error rates and implications of cognitive bias. Participants raised consensus concerns with the provision of expert evidence and other broader issues. This information and understanding from the forensic front line are essential for forensic science moving forward to best address current challenges and criticisms not only of forensic evidence for the court but for applications of forensic science beyond the courtroom.},
	urldate = {2025-03-24},
	journal = {Forensic Science International},
	author = {Airlie, Melissa and Robertson, James and Krosch, Matt N. and Brooks, Elizabeth},
	month = mar,
	year = {2021},
	keywords = {Evidence, Collaboration, Holistic, Intelligence, Objective, Subjective, Validation},
	pages = {110704},
	file = {ScienceDirect Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\YDJCE58V\\S0379073821000244.html:text/html},
}

@article{roux_forensic_2018,
	title = {Forensic science 2020 – the end of the crossroads?},
	volume = {50},
	issn = {0045-0618},
	url = {https://doi.org/10.1080/00450618.2018.1485738},
	doi = {10.1080/00450618.2018.1485738},
	abstract = {Forensic science has been at the crossroads for over a decade. While this situation is a fertile ground for discussion, security problem solving and the sound administration of justice cannot be put on hold until solutions pleasing everyone emerge. In all practical reality, forensic science will continue to be applied because it is simply the most reliable way to reconstruct the past through the exploitation of relics of criminal activities and by logical treatment of the collected information. In this paper, it is argued that instead of exclusively focusing on error management and processes, we should also question the very ontological nature of forensic science. Not only should the dominant conception of forensic sciences as a patchwork of disciplines assisting the criminal justice system be challenged, but forensic science’s own fundamental principles should also be better enunciated and promoted so they can be more broadly accepted and understood. Such changes invite operations, education and research to become more collective and interdisciplinary. This is necessary to fully exploit the investigative, epidemiological, court and social functions of forensic science. We ought to ask the question: will forensic science reach the end of the crossroads soon?},
	number = {6},
	urldate = {2025-03-24},
	journal = {Australian Journal of Forensic Sciences},
	author = {Roux, Claude and , Olivier, Ribaux and and Crispino, Frank},
	month = nov,
	year = {2018},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00450618.2018.1485738},
	pages = {607--618},
	file = {Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\QCCBN4I3\\Roux et al. - 2018 - Forensic science 2020 – the end of the crossroads.pdf:application/pdf},
}

@article{winburn_response_2021,
	title = {Response to letter to the editor of {FSI}: {Synergy} regarding {Objectivity} is amyth that harms the practice and diversity of forensic science},
	volume = {3},
	issn = {2589-871X},
	shorttitle = {Response to letter to the editor of {FSI}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8710839/},
	doi = {10.1016/j.fsisyn.2021.100212},
	urldate = {2025-03-24},
	journal = {Forensic Science International: Synergy},
	author = {Winburn, Allysha Powanda and Clemmons, Chaunesey M.J.},
	month = nov,
	year = {2021},
	pmid = {34988416},
	pmcid = {PMC8710839},
	pages = {100212},
	file = {PubMed Central Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\AN8NB3CQ\\Winburn and Clemmons - 2021 - Response to letter to the editor of FSI Synergy regarding Objectivity is amyth that harms the pract.pdf:application/pdf},
}

@article{budowle_perspective_2009,
	title = {A {Perspective} on {Errors}, {Bias}, and {Interpretation} in the {Forensic} {Sciences} and {Direction} for {Continuing} {Advancement}},
	volume = {54},
	copyright = {Journal compilation © 2009 American Academy of Forensic Sciences. No claim to original U.S. government works},
	issn = {1556-4029},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1556-4029.2009.01081.x},
	doi = {10.1111/j.1556-4029.2009.01081.x},
	abstract = {Abstract: The forensic sciences are under review more so than ever before. Such review is necessary and healthy and should be a continuous process. It identifies areas for improvement in quality practices and services. The issues surrounding error, i.e., measurement error, human error, contextual bias, and confirmatory bias, and interpretation are discussed. Infrastructure is already in place to support reliability. However, more definition and clarity of terms and interpretation would facilitate communication and understanding. Material improvement across the disciplines should be sought through national programs in education and training, focused on science, the scientific method, statistics, and ethics. To provide direction for advancing the forensic sciences a list of recommendations ranging from further documentation to new research and validation to education and to accreditation is provided for consideration. The list is a starting point for discussion that could foster further thought and input in developing an overarching strategic plan for enhancing the forensic sciences.},
	language = {en},
	number = {4},
	urldate = {2025-03-24},
	journal = {Journal of Forensic Sciences},
	author = {Budowle, Bruce and Bottrell, Maureen C. and Bunch, Stephen G. and Fram, Robert and Harrison, Diana and Meagher, Stephen and Oien, Cary T. and Peterson, Peter E. and Seiger, Danielle P. and Smith, Michael B. and Smrz, Melissa A. and Soltis, Greg L. and Stacey, Robert B.},
	year = {2009},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1556-4029.2009.01081.x},
	keywords = {forensic science, interpretation, training, confirmation bias, context bias, education, error, ethics},
	pages = {798--809},
	file = {Full Text PDF:C\:\\Users\\jstacey\\Zotero\\storage\\FIWIFJTL\\Budowle et al. - 2009 - A Perspective on Errors, Bias, and Interpretation in the Forensic Sciences and Direction for Continu.pdf:application/pdf;Snapshot:C\:\\Users\\jstacey\\Zotero\\storage\\HVI456KT\\j.1556-4029.2009.01081.html:text/html},
}

@article{bird_formulation_2025,
	title = {Formulation of propositions for forensic handwriting examinations},
	volume = {7},
	copyright = {© 2024 Wiley Periodicals LLC.},
	issn = {2573-9468},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wfs2.1532},
	doi = {10.1002/wfs2.1532},
	abstract = {The forensic examiner's role is to assist the court by explaining the significance of their findings within the context of the case. In “evaluative” reporting this is achieved by considering the probability of observing the evidence given at least two competing propositions. This article outlines the requirements of propositions within an evaluative framework, common sets of propositions used in handwriting examinations and their associated basic expectations. As the forensic handwriting examiner may be asked to undertake different types of comparisons depending on the material available and the case circumstances, it is critical that examiners carefully consider the most appropriate propositions against which the findings are to be evaluated. Further, as the propositions are derived from case information, and may require assumptions to be drawn, the necessary disclosure within reports is discussed. Although the examples given are specific to handwriting examinations, the concepts discussed in this article are applicable to other forensic disciplines utilizing an evaluative approach to evidence interpretation. This article is categorized under: Forensic Chemistry and Trace Evidence {\textgreater} Presentation and Evaluation of Forensic Science Output Jurisprudence and Regulatory Oversight {\textgreater} Expert Evidence and Narrative},
	language = {en},
	number = {1},
	urldate = {2025-03-24},
	journal = {WIREs Forensic Science},
	author = {Bird, Carolyne and Ballantyne, Kaye and Jones, Kylie},
	year = {2025},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wfs2.1532},
	keywords = {evaluative reporting, propositions, forensic handwriting examination},
	pages = {e1532},
}

@article{gosch_dna_2019-1,
	title = {On {DNA} transfer: {The} lack and difficulty of systematic research and how to do it better},
	volume = {40},
	issn = {1872-4973, 1878-0326},
	shorttitle = {On {DNA} transfer},
	url = {https://www.fsigenetics.com/article/S1872-4973(18)30551-9/abstract},
	doi = {10.1016/j.fsigen.2019.01.012},
	language = {English},
	urldate = {2025-03-25},
	journal = {Forensic Science International: Genetics},
	author = {Gosch, Annica and Courts, Cornelius},
	month = may,
	year = {2019},
	pmid = {30731249},
	note = {Publisher: Elsevier},
	keywords = {Forensic genetics, DNA transfer, Trace DNA, Touch DNA},
	pages = {24--36},
}

@article{szkuta_dna_2015,
	title = {{DNA} transfer by examination tools – a risk for forensic casework?},
	volume = {16},
	issn = {1872-4973, 1878-0326},
	url = {https://www.fsigenetics.com/article/S1872-4973(15)00033-2/abstract},
	doi = {10.1016/j.fsigen.2015.02.004},
	language = {English},
	urldate = {2025-03-25},
	journal = {Forensic Science International: Genetics},
	author = {Szkuta, Bianca and Harvey, Michelle L. and Ballantyne, Kaye N. and Oorschot, Roland A. H. van},
	month = may,
	year = {2015},
	pmid = {25735003},
	note = {Publisher: Elsevier},
	keywords = {Contamination, DNA, Examination tools, Trace, Transfer},
	pages = {246--254},
}

@article{pickrahn_contamination_2017,
	title = {Contamination incidents in the pre-analytical phase of forensic {DNA} analysis in {Austria}—{Statistics} of 17 years},
	volume = {31},
	issn = {1872-4973, 1878-0326},
	url = {https://www.fsigenetics.com/article/S1872-4973(17)30154-0/abstract},
	doi = {10.1016/j.fsigen.2017.07.012},
	language = {English},
	urldate = {2025-03-25},
	journal = {Forensic Science International: Genetics},
	author = {Pickrahn, Ines and Kreindl, Gabriele and Müller, Eva and Dunkelmann, Bettina and Zahrer, Waltraud and Cemper-Kiesslich, Jan and Neuhuber, F.},
	month = nov,
	year = {2017},
	pmid = {28843849},
	note = {Publisher: Elsevier},
	keywords = {Contamination, Forensic DNA analysis, Police Elimination Database (PED), Profile comparison},
	pages = {12--18},
}
