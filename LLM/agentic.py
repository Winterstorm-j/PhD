import json
import time

# --- Agent & Tool Definitions ---

class RAG_LLM_Agent:
    """
    A simulated RAG-enabled LLM agent.
    It takes a user query, "retrieves" relevant data from a mock knowledge base,
    and "generates" a structured JSON output with parameters for a build script.
    """
    def __init__(self, knowledge_base):
        self.knowledge_base = knowledge_base
        print("🤖 RAG LLM Agent initialized.")

    def process_query(self, query: str) -> dict:
        print(f"\n🧠 RAG LLM Agent received query: '{query}'")
        time.sleep(1) # Simulate processing time

        # 1. Retrieval Phase (Simulated)
        print("🔍 Retrieving relevant information from knowledge base...")
        retrieved_data = self.knowledge_base.get("python_docker_template")
        if not retrieved_data:
            raise ValueError("Could not find relevant information in the knowledge base.")
        print("✅ Retrieval complete.")
        time.sleep(1)

        # 2. Generation Phase (Simulated)
        # The LLM uses the retrieved template to generate structured parameters.
        print("✨ Generating structured parameters from retrieved data...")
        build_parameters = {
            "base_image": retrieved_data["latest_python_image"],
            "workdir": "/usr/src/app",
            "dependencies": ["flask", "gunicorn"],
            "expose_port": 5000,
            "start_command": '["gunicorn", "--bind", "0.0.0.0:5000", "app:app"]'
        }
        print("✅ Parameter generation complete.")
        time.sleep(1)
        return build_parameters


class Build_Agent:
    """
    An agent responsible for executing build scripts (tools).
    It takes structured parameters and passes them to the correct tool.
    """
    def __init__(self):
        print("🛠️  Build Agent initialized.")

    def execute_build(self, build_params: dict) -> str:
        print(f"\n👷 Build Agent received parameters: {json.dumps(build_params, indent=2)}")
        time.sleep(1)
        print("🔩 Calling the Dockerfile build script tool...")
        dockerfile_content = self._run_docker_build_script(**build_params)
        print("✅ Build script executed successfully.")
        return dockerfile_content

    def _run_docker_build_script(self, base_image: str, workdir: str, dependencies: list, expose_port: int, start_command: str) -> str:
        """
        This is the "tool" that generates the Dockerfile content.
        It takes specific, structured parameters.
        """
        # Create a string for the RUN command to install dependencies
        pip_install_command = f"pip install --no-cache-dir {' '.join(dependencies)}"

        # Assemble the Dockerfile using an f-string template
        dockerfile = f"""
# ----------------------------------------------------
# Dockerfile generated by Agentic AI Script
# ----------------------------------------------------

# 1. Set the base image
FROM {base_image}

# 2. Set the working directory
WORKDIR {workdir}

# 3. Copy application files
COPY . .

# 4. Install dependencies
RUN {pip_install_command}

# 5. Expose the application port
EXPOSE {expose_port}

# 6. Define the container's start command
CMD {start_command}
# ----------------------------------------------------
"""
        time.sleep(1.5) # Simulate build time
        return dockerfile.strip()


# --- Main Orchestration Logic ---

def main():
    """
    The main function that orchestrates the agentic workflow.
    """
    # 1. Setup: Initialize agents and the mock knowledge base
    mock_knowledge_base = {
        "python_docker_template": {
            "latest_python_image": "python:3.11-slim",
            "description": "A standard template for a Python web application using Flask or similar frameworks."
        }
    }

    rag_agent = RAG_LLM_Agent(knowledge_base=mock_knowledge_base)
    build_agent = Build_Agent()

    # 2. User Interaction
    user_question = "I need to build a Docker container for my Python Flask web app. Can you create the Dockerfile for me?"

    print("\n=================================================")
    print(f"👤 User: {user_question}")
    print("=================================================\n")

    # 3. Agentic Workflow Execution
    try:
        # Step 1: The user's unstructured query goes to the RAG LLM agent.
        build_parameters = rag_agent.process_query(user_question)

        # Step 2: The structured output (parameters) from the LLM is passed to the Build Agent.
        final_output = build_agent.execute_build(build_parameters)

        # 4. Final Output to User
        print("\n=================================================")
        print("✅ Workflow complete! Here is the generated output:")
        print("=================================================\n")
        print(final_output)

    except Exception as e:
        print(f"\n💥 An error occurred during the workflow: {e}")


if __name__ == "__main__":
    main()
    
    '''How the Script Works
Initialization:

We create a mock_knowledge_base which simulates the "Retrieval" part of RAG. In a real system, this would be a vector database with technical documentation.

The RAG_LLM_Agent and Build_Agent are initialized.

User Query:

The user asks a high-level question: "I need to build a Docker container for my Python Flask web app...". This query lacks the specific details required for a script.

RAG LLM Agent's Turn:

Retrieval: The agent receives the query and (in this simulation) finds the python_docker_template in its knowledge base.

Generation: It then uses this information to generate a structured dict of build_parameters. This is the crucial step where the LLM translates the user's natural language request into machine-readable parameters (base_image, dependencies, etc.).

Build Agent's Turn:

The Build_Agent receives the structured build_parameters from the RAG agent. Its job is not to understand natural language but to execute tasks based on specific inputs.

It calls its internal tool, _run_docker_build_script, passing the parameters as function arguments.

Tool Execution:

The _run_docker_build_script function takes the precise parameters and uses them to populate a Dockerfile template. It doesn't need any AI; it's a standard, deterministic function.

Final Output:

The generated Dockerfile string is returned up the chain and finally presented to the user as the solution to their initial request.'''